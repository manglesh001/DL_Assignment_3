{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11871625,"sourceType":"datasetVersion","datasetId":7460569},{"sourceId":11885743,"sourceType":"datasetVersion","datasetId":7470370}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb\nimport wandb\n!wandb login 58a0b576fd5221cd0d63b154deaabbe535e853c6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:08:46.053408Z","iopub.execute_input":"2025-05-20T15:08:46.053642Z","iopub.status.idle":"2025-05-20T15:08:57.085323Z","shell.execute_reply.started":"2025-05-20T15:08:46.053618Z","shell.execute_reply":"2025-05-20T15:08:57.084545Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =======================\n# Imports and Sweep Config\n# =======================\n# =======================\n# Imports and Sweep Config\n# =======================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport wandb\nimport os\nimport math\n\nsweep_config = {\n    'method': 'bayes',\n    'metric': {\n        'name': 'val_loss',\n        'goal': 'minimize'\n    },\n    'parameters': {\n        'embedding_dim': {'values': [32,64,128,256]},\n        'hidden_dim': {'values': [32,64,128,256]},\n        'enc_layers': {'values': [1,2,3]},\n        'dec_layers': {'values': [1,2,3]},\n        'cell_type': {'values': ['GRU', 'LSTM', 'RNN']},\n        'dropout': {'values': [0.2,0.3,0.5]},\n        'epochs': {'values': [10,15]},\n        'beam_size': {'values': [1,3,5]},\n        'attention_type': {'values': ['dot', 'general', 'concat']},\n        'batch_size': {'values': [64,128,256]},\n        'learning_rate': {'values': [0.001,0.0005,0.0001]}\n    }\n}\n\n# =======================\n# Default Config\n# =======================\ndefault_config = {\n    'embedding_dim': 32,\n    'hidden_dim': 64,\n    'enc_layers': 1,\n    'dec_layers': 1,\n    'cell_type': 'LSTM',\n    'dropout': 0.2,\n    'epochs': 10,\n    'beam_size': 1,\n    'attention_type': 'general',\n    'batch_size': 64,\n    'learning_rate': 0.001\n}\n\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.char2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2char = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for char in text:\n                if char not in self.char2idx:\n                    self.char2idx[char] = self.size\n                    self.idx2char[self.size] = char\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.char2idx[c] for c in text]\n\n    def decode(self, idxs):\n        return ''.join([self.idx2char[i] for i in idxs if i > 2])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab, out_vocab):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        inp_vocab.build([p[0] for p in self.pairs])\n        out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        y = [self.out_vocab.char2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.char2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y)\n\ndef collate_fn(batch):\n    x_batch, y_batch = zip(*batch)\n    x_lens = [len(x) for x in x_batch]\n    y_lens = [len(y) for y in y_batch]\n    x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n    y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n    return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens)\n\n# =======================\n# Attention Mechanism\n# =======================\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_type='general'):\n        super().__init__()\n        self.attention_type = attention_type\n        \n        if attention_type == 'general':\n            self.attn = nn.Linear(hidden_dim, hidden_dim)\n        elif attention_type == 'concat':\n            self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n            self.v = nn.Linear(hidden_dim, 1, bias=False)\n            \n    def forward(self, hidden, encoder_outputs, mask=None):\n        # hidden: [batch_size, hidden_dim]\n        # encoder_outputs: [batch_size, src_len, hidden_dim]\n        \n        batch_size, src_len, hidden_dim = encoder_outputs.shape\n        \n        # For dot and general attention\n        if self.attention_type == 'dot':\n            # Calculate dot product between hidden and encoder_outputs\n            energy = torch.bmm(encoder_outputs, hidden.unsqueeze(2)).squeeze(2)\n            # energy: [batch_size, src_len]\n            \n        elif self.attention_type == 'general':\n            # Calculate general attention\n            energy = torch.bmm(encoder_outputs, self.attn(hidden).unsqueeze(2)).squeeze(2)\n            # energy: [batch_size, src_len]\n            \n        elif self.attention_type == 'concat':\n            # Repeat hidden across source length\n            hidden_expanded = hidden.unsqueeze(1).repeat(1, src_len, 1)\n            # Concatenate hidden and encoder_outputs\n            energy = self.v(torch.tanh(self.attn(torch.cat((hidden_expanded, encoder_outputs), dim=2)))).squeeze(2)\n            # energy: [batch_size, src_len]\n        \n        # Apply mask if provided (for padding)\n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, -1e10)\n        \n        # Apply softmax to get attention weights\n        attention_weights = F.softmax(energy, dim=1)\n        # attention_weights: [batch_size, src_len]\n        \n        # Apply attention weights to encoder outputs\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n        # context: [batch_size, hidden_dim]\n        \n        return context, attention_weights\n\n# =======================\n# Encoder and Decoder\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.cell_type = cell_type\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, bidirectional=False, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        \n        return outputs, hidden\n\nclass AttentionDecoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout, attention_type):\n        super().__init__()\n        self.cell_type = cell_type\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        \n        # Input to RNN will be embedding + context vector\n        self.rnn = rnn_class(emb_dim + hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        \n        self.attention = Attention(hidden_dim, attention_type)\n        \n        # Output layer combines hidden state and context vector\n        self.out = nn.Linear(hidden_dim * 2, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n        # input_token: [batch_size]\n        # hidden: tuple of [num_layers, batch_size, hidden_dim] for LSTM or [num_layers, batch_size, hidden_dim] for GRU/RNN\n        # encoder_outputs: [batch_size, src_len, hidden_dim]\n        \n        # Get the last layer's hidden state for attention\n        if self.cell_type == \"LSTM\":\n            attn_hidden = hidden[0][-1]  # Last layer's hidden state\n        else:\n            attn_hidden = hidden[-1]  # Last layer's hidden state\n        \n        # Calculate attention\n        context, attention_weights = self.attention(attn_hidden, encoder_outputs, mask)\n        \n        # Embed input token\n        embedded = self.embedding(input_token)  # [batch_size, emb_dim]\n        \n        # Concatenate embedding and context vector\n        rnn_input = torch.cat((embedded, context), dim=1).unsqueeze(1)  # [batch_size, 1, emb_dim + hidden_dim]\n        \n        # Pass through RNN\n        output, hidden = self.rnn(rnn_input, hidden)\n        \n        # Get the output from the last layer\n        if self.cell_type == \"LSTM\":\n            output_hidden = hidden[0][-1]  # Last layer's hidden state\n        else:\n            output_hidden = hidden[-1]  # Last layer's hidden state\n        \n        # Concatenate output and context for prediction\n        output = torch.cat((output_hidden, context), dim=1)\n        \n        # Apply dropout and predict\n        output = self.dropout(output)\n        prediction = self.out(output)\n        \n        return prediction, hidden, attention_weights\n\n# =======================\n# Seq2Seq Model with Beam Search\n# =======================\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, enc_layers, dec_layers, cell_type, device, beam_size=1):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.cell_type = cell_type\n        self.enc_layers = enc_layers\n        self.dec_layers = dec_layers\n        self.device = device\n        self.beam_size = beam_size\n\n    def create_mask(self, src, src_lens):\n        # Create mask for attention (1 for valid positions, 0 for padding)\n        batch_size = src.size(0)\n        max_len = src.size(1)\n        mask = torch.zeros(batch_size, max_len, device=self.device)\n        for i, length in enumerate(src_lens):\n            mask[i, :length] = 1\n        return mask\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        src_data, src_lens = src\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.out.out_features\n        \n        # Store outputs, attention weights\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n        attentions = torch.zeros(batch_size, trg_len, src_data.size(1)).to(self.device)\n        \n        # Encode\n        encoder_outputs, enc_hidden = self.encoder(src_data, src_lens)\n        \n        # Create mask for attention\n        mask = self.create_mask(src_data, src_lens)\n        \n        # Match encoder and decoder layers\n        if self.cell_type == \"LSTM\":\n            h, c = enc_hidden\n            h = self._match_layers(h)\n            c = self._match_layers(c)\n            dec_hidden = (h, c)\n        else:\n            dec_hidden = self._match_layers(enc_hidden)\n        \n        # First input to the decoder is the <sos> token\n        input_token = trg[:, 0]\n        \n        for t in range(1, trg_len):\n            output, dec_hidden, attn_weights = self.decoder(\n                input_token, dec_hidden, encoder_outputs, mask\n            )\n            \n            outputs[:, t] = output\n            attentions[:, t] = attn_weights\n            \n            # Teacher forcing\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if teacher_force else top1\n            \n        return outputs, attentions\n\n    def _match_layers(self, hidden):\n        # Match encoder and decoder layers\n        if self.enc_layers == self.dec_layers:\n            return hidden\n        elif self.enc_layers > self.dec_layers:\n            return hidden[:self.dec_layers]\n        else:\n            pad = hidden.new_zeros((self.dec_layers - self.enc_layers, *hidden.shape[1:]))\n            return torch.cat([hidden, pad], dim=0)\n    \n    def beam_search(self, src, max_len=50, sos_idx=1, eos_idx=2):\n        src_data, src_lens = src\n        batch_size = src_data.size(0)\n        assert batch_size == 1, \"Beam search only supports batch size of 1 for now\"\n\n        # Encode\n        encoder_outputs, enc_hidden = self.encoder(src_data, src_lens)\n        mask = self.create_mask(src_data, src_lens)\n\n        # Prepare initial hidden state\n        if self.cell_type == \"LSTM\":\n            h, c = enc_hidden\n            h = self._match_layers(h)\n            c = self._match_layers(c)\n            dec_hidden = (h, c)\n        else:\n            dec_hidden = self._match_layers(enc_hidden)\n\n        # Initialize beams with [score, sequence, hidden_state]\n        beams = [{\n            \"score\": 0.0,\n            \"seq\": [sos_idx],\n            \"hidden\": dec_hidden\n        }]\n\n        for _ in range(max_len):\n            new_beams = []\n            for beam in beams:\n                seq = beam[\"seq\"]\n                if seq[-1] == eos_idx:\n                    new_beams.append(beam)\n                    continue\n\n                input_token = torch.tensor([seq[-1]], device=self.device)\n                dec_hidden = beam[\"hidden\"]\n                output, new_hidden, _ = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n\n                log_probs = F.log_softmax(output, dim=1).squeeze(0)\n                topk_log_probs, topk_indices = torch.topk(log_probs, self.beam_size)\n\n                for log_prob, idx in zip(topk_log_probs, topk_indices):\n                    new_beams.append({\n                        \"score\": beam[\"score\"] + log_prob.item(),\n                        \"seq\": beam[\"seq\"] + [idx.item()],\n                        \"hidden\": new_hidden\n                    })\n\n            # Keep top `beam_size` beams\n            beams = sorted(new_beams, key=lambda x: x[\"score\"], reverse=True)[:self.beam_size]\n\n            # Early stopping if all beams end with <eos>\n            if all(beam[\"seq\"][-1] == eos_idx for beam in beams):\n                break\n\n        # Choose best beam\n        best_beam = max(beams, key=lambda x: x[\"score\"])\n        return best_beam[\"seq\"]\n\n\n\n\n\n# =======================\n# Train & Eval\n# =======================\ndef train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss, total_correct, total_count = 0, 0, 0\n    for src, trg, src_lens, _ in loader:\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output, _ = model((src, src_lens), trg)\n        output_dim = output.shape[-1]\n        loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n        pred = output.argmax(2)\n        correct = ((pred[:, 1:] == trg[:, 1:]) & (trg[:, 1:] != 0)).sum().item()\n        total_correct += correct\n        total_count += (trg[:, 1:] != 0).sum().item()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    acc = 100.0 * total_correct / total_count\n    print(f\"Train Loss: {total_loss / len(loader):.4f}, Acc: {acc:.2f}%\")\n    return total_loss / len(loader), acc\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss, total_correct, total_count = 0, 0, 0\n    with torch.no_grad():\n        for src, trg, src_lens, _ in loader:\n            src, trg = src.to(device), trg.to(device)\n            output, _ = model((src, src_lens), trg, teacher_forcing_ratio=0)\n            output_dim = output.shape[-1]\n            loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n            pred = output.argmax(2)\n            correct = ((pred[:, 1:] == trg[:, 1:]) & (trg[:, 1:] != 0)).sum().item()\n            total_correct += correct\n            total_count += (trg[:, 1:] != 0).sum().item()\n            total_loss += loss.item()\n    acc = 100.0 * total_correct / total_count\n    print(f\"Val Loss: {total_loss / len(loader):.4f}, Acc: {acc:.2f}%\")\n    return total_loss / len(loader), acc\n\n# =======================\n# Main\n# =======================\ndef main():\n    wandb.init(config=default_config, project=\"dakshina-transliteration\")\n    config = wandb.config\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    inp_vocab, out_vocab = Vocab(), Vocab()\n    train_path = \"/kaggle/input/devnagiridata/hi.translit.sampled.train.tsv\"\n    dev_path = \"/kaggle/input/devnagiridata/hi.translit.sampled.dev.tsv\"\n    \n    train_data = TransliterationDataset(train_path, inp_vocab, out_vocab)\n    dev_data = TransliterationDataset(dev_path, inp_vocab, out_vocab)\n    \n    # Use config.batch_size for DataLoader\n    train_loader = DataLoader(train_data, batch_size=config.batch_size, \n                             shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=config.batch_size,\n                           shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(inp_vocab.size, config.embedding_dim, config.hidden_dim,\n                     config.enc_layers, config.cell_type, config.dropout)\n    decoder = AttentionDecoder(out_vocab.size, config.embedding_dim, config.hidden_dim,\n                              config.dec_layers, config.cell_type, config.dropout,\n                              config.attention_type)\n    \n    model = Seq2Seq(encoder, decoder, config.enc_layers, config.dec_layers,\n                   config.cell_type, device, beam_size=config.beam_size).to(device)\n    \n    # Use config.learning_rate for optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    for epoch in range(config.epochs):\n        print(f\"Epoch {epoch+1}/{config.epochs}\")\n        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n        wandb.log({\n            \"train_loss\": train_loss, \n            \"train_acc\": train_acc, \n            \"val_loss\": val_loss, \n            \"val_acc\": val_acc, \n            \"epoch\": epoch+1\n        })\n\n# =======================\nif __name__ == '__main__':\n    sweep_id = wandb.sweep(sweep_config, project=\"dakshina-transliteration-attention\")\n    wandb.agent(sweep_id, function=main, count=30)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:16:46.826237Z","iopub.execute_input":"2025-05-19T14:16:46.827014Z","iopub.status.idle":"2025-05-19T16:34:23.246442Z","shell.execute_reply.started":"2025-05-19T14:16:46.826985Z","shell.execute_reply":"2025-05-19T16:34:23.245752Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: 64q660zv\nSweep URL: https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kk1paf4n with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: general\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanglesh_dlass3\u001b[0m (\u001b[33mmanglesh_dl_ass3\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_141703-kk1paf4n</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/kk1paf4n' target=\"_blank\">gentle-sweep-1</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/kk1paf4n' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/kk1paf4n</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 2.5110, Acc: 29.00%\nVal Loss: 1.9818, Acc: 41.74%\nEpoch 2/10\nTrain Loss: 1.7008, Acc: 50.11%\nVal Loss: 1.3793, Acc: 56.66%\nEpoch 3/10\nTrain Loss: 1.3229, Acc: 61.12%\nVal Loss: 1.1319, Acc: 65.53%\nEpoch 4/10\nTrain Loss: 1.1588, Acc: 66.26%\nVal Loss: 1.0422, Acc: 68.36%\nEpoch 5/10\nTrain Loss: 1.0605, Acc: 69.47%\nVal Loss: 1.0014, Acc: 70.01%\nEpoch 6/10\nTrain Loss: 1.0092, Acc: 71.01%\nVal Loss: 0.9782, Acc: 70.04%\nEpoch 7/10\nTrain Loss: 0.9667, Acc: 72.27%\nVal Loss: 0.9751, Acc: 70.81%\nEpoch 8/10\nTrain Loss: 0.9433, Acc: 72.94%\nVal Loss: 0.9551, Acc: 71.02%\nEpoch 9/10\nTrain Loss: 0.9219, Acc: 73.60%\nVal Loss: 0.9505, Acc: 71.29%\nEpoch 10/10\nTrain Loss: 0.9076, Acc: 74.02%\nVal Loss: 0.9340, Acc: 71.47%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇█████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▇▇██████</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>74.02213</td></tr><tr><td>train_loss</td><td>0.90762</td></tr><tr><td>val_acc</td><td>71.46784</td></tr><tr><td>val_loss</td><td>0.93404</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">gentle-sweep-1</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/kk1paf4n' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/kk1paf4n</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_141703-kk1paf4n/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z7kry7ze with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: dot\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_141850-z7kry7ze</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/z7kry7ze' target=\"_blank\">vocal-sweep-2</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/z7kry7ze' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/z7kry7ze</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 2.8594, Acc: 20.93%\nVal Loss: 2.5821, Acc: 25.49%\nEpoch 2/10\nTrain Loss: 2.4010, Acc: 28.89%\nVal Loss: 2.2919, Acc: 29.30%\nEpoch 3/10\nTrain Loss: 2.0285, Acc: 38.11%\nVal Loss: 1.9301, Acc: 38.68%\nEpoch 4/10\nTrain Loss: 1.6875, Acc: 49.23%\nVal Loss: 1.6945, Acc: 47.08%\nEpoch 5/10\nTrain Loss: 1.4732, Acc: 56.66%\nVal Loss: 1.5609, Acc: 51.94%\nEpoch 6/10\nTrain Loss: 1.3461, Acc: 60.52%\nVal Loss: 1.4823, Acc: 54.67%\nEpoch 7/10\nTrain Loss: 1.2665, Acc: 62.74%\nVal Loss: 1.4252, Acc: 56.46%\nEpoch 8/10\nTrain Loss: 1.2155, Acc: 64.04%\nVal Loss: 1.3827, Acc: 57.94%\nEpoch 9/10\nTrain Loss: 1.1562, Acc: 65.99%\nVal Loss: 1.3538, Acc: 59.08%\nEpoch 10/10\nTrain Loss: 1.1214, Acc: 67.00%\nVal Loss: 1.3155, Acc: 60.74%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▂▄▅▆▇▇███</td></tr><tr><td>train_loss</td><td>█▆▅▃▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▄▅▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▆▄▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>66.99628</td></tr><tr><td>train_loss</td><td>1.12138</td></tr><tr><td>val_acc</td><td>60.74188</td></tr><tr><td>val_loss</td><td>1.31552</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vocal-sweep-2</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/z7kry7ze' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/z7kry7ze</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_141850-z7kry7ze/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zd0d9rsu with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: dot\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_142026-zd0d9rsu</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/zd0d9rsu' target=\"_blank\">bumbling-sweep-3</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/zd0d9rsu' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/zd0d9rsu</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.5174, Acc: 25.81%\nVal Loss: 2.1532, Acc: 33.30%\nEpoch 2/15\nTrain Loss: 1.9083, Acc: 40.98%\nVal Loss: 1.7132, Acc: 45.88%\nEpoch 3/15\nTrain Loss: 1.5536, Acc: 51.78%\nVal Loss: 1.4826, Acc: 53.44%\nEpoch 4/15\nTrain Loss: 1.3771, Acc: 57.32%\nVal Loss: 1.3707, Acc: 57.18%\nEpoch 5/15\nTrain Loss: 1.2759, Acc: 60.66%\nVal Loss: 1.2924, Acc: 59.76%\nEpoch 6/15\nTrain Loss: 1.2133, Acc: 62.79%\nVal Loss: 1.2253, Acc: 61.99%\nEpoch 7/15\nTrain Loss: 1.1586, Acc: 64.62%\nVal Loss: 1.1762, Acc: 64.34%\nEpoch 8/15\nTrain Loss: 1.1129, Acc: 66.35%\nVal Loss: 1.1528, Acc: 64.93%\nEpoch 9/15\nTrain Loss: 1.0816, Acc: 67.48%\nVal Loss: 1.1283, Acc: 66.03%\nEpoch 10/15\nTrain Loss: 1.0536, Acc: 68.38%\nVal Loss: 1.1321, Acc: 66.47%\nEpoch 11/15\nTrain Loss: 1.0402, Acc: 68.86%\nVal Loss: 1.1032, Acc: 66.82%\nEpoch 12/15\nTrain Loss: 1.0175, Acc: 69.68%\nVal Loss: 1.0828, Acc: 67.74%\nEpoch 13/15\nTrain Loss: 0.9974, Acc: 70.33%\nVal Loss: 1.0473, Acc: 67.78%\nEpoch 14/15\nTrain Loss: 0.9874, Acc: 70.58%\nVal Loss: 1.0524, Acc: 68.34%\nEpoch 15/15\nTrain Loss: 0.9642, Acc: 71.38%\nVal Loss: 1.0491, Acc: 68.62%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▃▅▆▆▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▅▆▆▇▇▇▇██████</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>71.37816</td></tr><tr><td>train_loss</td><td>0.9642</td></tr><tr><td>val_acc</td><td>68.62359</td></tr><tr><td>val_loss</td><td>1.04909</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bumbling-sweep-3</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/zd0d9rsu' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/zd0d9rsu</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_142026-zd0d9rsu/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yhqr0gl6 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: general\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_142257-yhqr0gl6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/yhqr0gl6' target=\"_blank\">peach-sweep-4</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/yhqr0gl6' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/yhqr0gl6</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.4411, Acc: 58.91%\nVal Loss: 1.0458, Acc: 69.22%\nEpoch 2/15\nTrain Loss: 0.8480, Acc: 75.06%\nVal Loss: 0.9443, Acc: 71.39%\nEpoch 3/15\nTrain Loss: 0.7820, Acc: 76.61%\nVal Loss: 0.9259, Acc: 71.28%\nEpoch 4/15\nTrain Loss: 0.7430, Acc: 77.54%\nVal Loss: 0.9123, Acc: 72.36%\nEpoch 5/15\nTrain Loss: 0.6997, Acc: 78.80%\nVal Loss: 0.8711, Acc: 73.25%\nEpoch 6/15\nTrain Loss: 0.7317, Acc: 77.84%\nVal Loss: 0.8825, Acc: 73.04%\nEpoch 7/15\nTrain Loss: 0.6677, Acc: 79.61%\nVal Loss: 0.8589, Acc: 73.20%\nEpoch 8/15\nTrain Loss: 0.6786, Acc: 79.02%\nVal Loss: 0.8512, Acc: 73.68%\nEpoch 9/15\nTrain Loss: 0.7135, Acc: 78.02%\nVal Loss: 0.8835, Acc: 72.50%\nEpoch 10/15\nTrain Loss: 0.6985, Acc: 78.56%\nVal Loss: 0.8656, Acc: 73.36%\nEpoch 11/15\nTrain Loss: 0.6477, Acc: 80.06%\nVal Loss: 0.8398, Acc: 74.10%\nEpoch 12/15\nTrain Loss: 0.6419, Acc: 80.11%\nVal Loss: 0.8249, Acc: 74.11%\nEpoch 13/15\nTrain Loss: 0.6534, Acc: 79.87%\nVal Loss: 0.8633, Acc: 73.83%\nEpoch 14/15\nTrain Loss: 0.6161, Acc: 81.04%\nVal Loss: 0.8324, Acc: 73.55%\nEpoch 15/15\nTrain Loss: 0.6433, Acc: 80.09%\nVal Loss: 0.8361, Acc: 73.68%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇█▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▄▅▇▆▇▇▆▇███▇▇</td></tr><tr><td>val_loss</td><td>█▅▄▄▂▃▂▂▃▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>80.08619</td></tr><tr><td>train_loss</td><td>0.64332</td></tr><tr><td>val_acc</td><td>73.67553</td></tr><tr><td>val_loss</td><td>0.83605</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">peach-sweep-4</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/yhqr0gl6' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/yhqr0gl6</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_142257-yhqr0gl6/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w75cha9e with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_142639-w75cha9e</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/w75cha9e' target=\"_blank\">cosmic-sweep-5</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/w75cha9e' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/w75cha9e</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.4732, Acc: 55.43%\nVal Loss: 1.0160, Acc: 69.63%\nEpoch 2/15\nTrain Loss: 0.8437, Acc: 74.68%\nVal Loss: 0.9017, Acc: 71.98%\nEpoch 3/15\nTrain Loss: 0.7513, Acc: 77.02%\nVal Loss: 0.8873, Acc: 72.51%\nEpoch 4/15\nTrain Loss: 0.6997, Acc: 78.38%\nVal Loss: 0.8793, Acc: 73.36%\nEpoch 5/15\nTrain Loss: 0.6681, Acc: 79.17%\nVal Loss: 0.8600, Acc: 73.56%\nEpoch 6/15\nTrain Loss: 0.6406, Acc: 79.86%\nVal Loss: 0.8276, Acc: 74.35%\nEpoch 7/15\nTrain Loss: 0.6221, Acc: 80.42%\nVal Loss: 0.8281, Acc: 74.25%\nEpoch 8/15\nTrain Loss: 0.6135, Acc: 80.58%\nVal Loss: 0.8139, Acc: 74.38%\nEpoch 9/15\nTrain Loss: 0.5996, Acc: 81.07%\nVal Loss: 0.7871, Acc: 74.82%\nEpoch 10/15\nTrain Loss: 0.5896, Acc: 81.21%\nVal Loss: 0.8049, Acc: 75.28%\nEpoch 11/15\nTrain Loss: 0.5782, Acc: 81.61%\nVal Loss: 0.8288, Acc: 75.32%\nEpoch 12/15\nTrain Loss: 0.5746, Acc: 81.61%\nVal Loss: 0.8136, Acc: 75.42%\nEpoch 13/15\nTrain Loss: 0.5725, Acc: 81.62%\nVal Loss: 0.8254, Acc: 75.34%\nEpoch 14/15\nTrain Loss: 0.5710, Acc: 81.58%\nVal Loss: 0.7887, Acc: 75.52%\nEpoch 15/15\nTrain Loss: 0.5529, Acc: 82.35%\nVal Loss: 0.7825, Acc: 75.71%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▄▅▆▆▆▆▇██████</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▂▂▂▁▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>82.3508</td></tr><tr><td>train_loss</td><td>0.55287</td></tr><tr><td>val_acc</td><td>75.70672</td></tr><tr><td>val_loss</td><td>0.78246</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cosmic-sweep-5</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/w75cha9e' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/w75cha9e</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_142639-w75cha9e/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n67ny8ll with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: general\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_143157-n67ny8ll</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/n67ny8ll' target=\"_blank\">vital-sweep-6</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/n67ny8ll' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/n67ny8ll</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.5483, Acc: 54.28%\nVal Loss: 1.3092, Acc: 59.25%\nEpoch 2/15\nTrain Loss: 1.3283, Acc: 60.53%\nVal Loss: 1.3366, Acc: 58.21%\nEpoch 3/15\nTrain Loss: 1.2633, Acc: 62.46%\nVal Loss: 1.1851, Acc: 62.79%\nEpoch 4/15\nTrain Loss: 1.0925, Acc: 67.59%\nVal Loss: 1.1708, Acc: 63.94%\nEpoch 5/15\nTrain Loss: 1.2435, Acc: 63.32%\nVal Loss: 1.1732, Acc: 63.95%\nEpoch 6/15\nTrain Loss: 1.2622, Acc: 62.71%\nVal Loss: 1.2109, Acc: 63.22%\nEpoch 7/15\nTrain Loss: 1.1317, Acc: 66.46%\nVal Loss: 1.1001, Acc: 66.65%\nEpoch 8/15\nTrain Loss: 1.0772, Acc: 68.46%\nVal Loss: 1.2319, Acc: 62.06%\nEpoch 9/15\nTrain Loss: 1.1424, Acc: 66.46%\nVal Loss: 1.1441, Acc: 64.76%\nEpoch 10/15\nTrain Loss: 1.1595, Acc: 66.05%\nVal Loss: 1.3471, Acc: 60.34%\nEpoch 11/15\nTrain Loss: 1.2318, Acc: 64.25%\nVal Loss: 1.1066, Acc: 66.90%\nEpoch 12/15\nTrain Loss: 1.0795, Acc: 68.77%\nVal Loss: 1.0838, Acc: 67.83%\nEpoch 13/15\nTrain Loss: 1.1133, Acc: 67.77%\nVal Loss: 1.1428, Acc: 65.92%\nEpoch 14/15\nTrain Loss: 1.1924, Acc: 65.39%\nVal Loss: 1.2334, Acc: 63.05%\nEpoch 15/15\nTrain Loss: 1.1052, Acc: 68.02%\nVal Loss: 1.0701, Acc: 67.62%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▄▅▇▅▅▇█▇▇▆██▆█</td></tr><tr><td>train_loss</td><td>█▅▄▁▃▄▂▁▂▂▃▁▂▃▁</td></tr><tr><td>val_acc</td><td>▂▁▄▅▅▅▇▄▆▃▇█▇▅█</td></tr><tr><td>val_loss</td><td>▇█▄▄▄▅▂▅▃█▂▁▃▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>68.01712</td></tr><tr><td>train_loss</td><td>1.10516</td></tr><tr><td>val_acc</td><td>67.62246</td></tr><tr><td>val_loss</td><td>1.07009</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vital-sweep-6</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/n67ny8ll' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/n67ny8ll</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_143157-n67ny8ll/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nu7qlc0k with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_143919-nu7qlc0k</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/nu7qlc0k' target=\"_blank\">hopeful-sweep-7</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/nu7qlc0k' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/nu7qlc0k</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 1.8805, Acc: 44.40%\nVal Loss: 1.1568, Acc: 66.07%\nEpoch 2/10\nTrain Loss: 0.8845, Acc: 73.82%\nVal Loss: 0.9274, Acc: 71.81%\nEpoch 3/10\nTrain Loss: 0.7638, Acc: 77.16%\nVal Loss: 0.8985, Acc: 72.59%\nEpoch 4/10\nTrain Loss: 0.7122, Acc: 78.52%\nVal Loss: 0.8970, Acc: 73.11%\nEpoch 5/10\nTrain Loss: 0.6927, Acc: 78.81%\nVal Loss: 0.8723, Acc: 73.46%\nEpoch 6/10\nTrain Loss: 0.6513, Acc: 80.06%\nVal Loss: 0.8552, Acc: 73.83%\nEpoch 7/10\nTrain Loss: 0.6379, Acc: 80.32%\nVal Loss: 0.8488, Acc: 73.69%\nEpoch 8/10\nTrain Loss: 0.6217, Acc: 80.69%\nVal Loss: 0.8550, Acc: 73.93%\nEpoch 9/10\nTrain Loss: 0.6201, Acc: 80.60%\nVal Loss: 0.8358, Acc: 74.37%\nEpoch 10/10\nTrain Loss: 0.6000, Acc: 81.23%\nVal Loss: 0.8255, Acc: 74.43%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▆▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▃▃▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>81.23367</td></tr><tr><td>train_loss</td><td>0.60001</td></tr><tr><td>val_acc</td><td>74.43072</td></tr><tr><td>val_loss</td><td>0.8255</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">hopeful-sweep-7</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/nu7qlc0k' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/nu7qlc0k</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_143919-nu7qlc0k/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vrw1qiba with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_144236-vrw1qiba</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vrw1qiba' target=\"_blank\">fearless-sweep-8</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vrw1qiba' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vrw1qiba</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 1.8408, Acc: 42.82%\nVal Loss: 1.1136, Acc: 64.21%\nEpoch 2/10\nTrain Loss: 0.9016, Acc: 71.88%\nVal Loss: 0.8618, Acc: 72.08%\nEpoch 3/10\nTrain Loss: 0.7479, Acc: 76.40%\nVal Loss: 0.8066, Acc: 73.81%\nEpoch 4/10\nTrain Loss: 0.6717, Acc: 78.76%\nVal Loss: 0.8281, Acc: 74.23%\nEpoch 5/10\nTrain Loss: 0.6313, Acc: 80.08%\nVal Loss: 0.7937, Acc: 74.88%\nEpoch 6/10\nTrain Loss: 0.6036, Acc: 80.82%\nVal Loss: 0.8305, Acc: 75.22%\nEpoch 7/10\nTrain Loss: 0.5857, Acc: 81.32%\nVal Loss: 0.7871, Acc: 75.57%\nEpoch 8/10\nTrain Loss: 0.5755, Acc: 81.58%\nVal Loss: 0.7826, Acc: 75.82%\nEpoch 9/10\nTrain Loss: 0.5514, Acc: 82.36%\nVal Loss: 0.7783, Acc: 75.75%\nEpoch 10/10\nTrain Loss: 0.5404, Acc: 82.65%\nVal Loss: 0.7910, Acc: 75.84%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇██████</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>82.64877</td></tr><tr><td>train_loss</td><td>0.54038</td></tr><tr><td>val_acc</td><td>75.84271</td></tr><tr><td>val_loss</td><td>0.79096</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fearless-sweep-8</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vrw1qiba' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vrw1qiba</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_144236-vrw1qiba/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wm1p0ya7 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_144634-wm1p0ya7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/wm1p0ya7' target=\"_blank\">driven-sweep-9</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/wm1p0ya7' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/wm1p0ya7</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.5955, Acc: 26.04%\nVal Loss: 2.1989, Acc: 34.41%\nEpoch 2/15\nTrain Loss: 1.9017, Acc: 43.22%\nVal Loss: 1.5169, Acc: 53.87%\nEpoch 3/15\nTrain Loss: 1.3732, Acc: 59.15%\nVal Loss: 1.1632, Acc: 65.06%\nEpoch 4/15\nTrain Loss: 1.1540, Acc: 66.07%\nVal Loss: 1.0790, Acc: 67.20%\nEpoch 5/15\nTrain Loss: 1.0612, Acc: 68.94%\nVal Loss: 1.0271, Acc: 68.30%\nEpoch 6/15\nTrain Loss: 1.0144, Acc: 70.23%\nVal Loss: 1.0004, Acc: 69.46%\nEpoch 7/15\nTrain Loss: 0.9723, Acc: 71.48%\nVal Loss: 1.0115, Acc: 68.95%\nEpoch 8/15\nTrain Loss: 0.9468, Acc: 72.12%\nVal Loss: 0.9160, Acc: 71.81%\nEpoch 9/15\nTrain Loss: 0.9189, Acc: 72.93%\nVal Loss: 0.9196, Acc: 71.93%\nEpoch 10/15\nTrain Loss: 0.9006, Acc: 73.51%\nVal Loss: 0.9313, Acc: 71.81%\nEpoch 11/15\nTrain Loss: 0.8851, Acc: 73.91%\nVal Loss: 0.9158, Acc: 72.55%\nEpoch 12/15\nTrain Loss: 0.8813, Acc: 73.96%\nVal Loss: 0.9167, Acc: 72.11%\nEpoch 13/15\nTrain Loss: 0.8626, Acc: 74.51%\nVal Loss: 0.9207, Acc: 72.39%\nEpoch 14/15\nTrain Loss: 0.8585, Acc: 74.54%\nVal Loss: 0.9101, Acc: 72.66%\nEpoch 15/15\nTrain Loss: 0.8387, Acc: 75.26%\nVal Loss: 0.9085, Acc: 72.49%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▇▇▇▇▇████████</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>75.25569</td></tr><tr><td>train_loss</td><td>0.83872</td></tr><tr><td>val_acc</td><td>72.49212</td></tr><tr><td>val_loss</td><td>0.90851</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">driven-sweep-9</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/wm1p0ya7' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/wm1p0ya7</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_144634-wm1p0ya7/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3ecgr25p with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_144930-3ecgr25p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/3ecgr25p' target=\"_blank\">iconic-sweep-10</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/3ecgr25p' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/3ecgr25p</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 2.5609, Acc: 26.99%\nVal Loss: 2.1108, Acc: 33.60%\nEpoch 2/10\nTrain Loss: 2.0077, Acc: 35.21%\nVal Loss: 1.7490, Acc: 41.19%\nEpoch 3/10\nTrain Loss: 1.7446, Acc: 41.12%\nVal Loss: 1.5677, Acc: 45.49%\nEpoch 4/10\nTrain Loss: 1.5799, Acc: 46.27%\nVal Loss: 1.4801, Acc: 48.69%\nEpoch 5/10\nTrain Loss: 1.4246, Acc: 52.02%\nVal Loss: 1.2667, Acc: 56.98%\nEpoch 6/10\nTrain Loss: 1.1684, Acc: 62.44%\nVal Loss: 1.0664, Acc: 65.71%\nEpoch 7/10\nTrain Loss: 1.0131, Acc: 68.74%\nVal Loss: 0.9662, Acc: 69.63%\nEpoch 8/10\nTrain Loss: 0.9299, Acc: 71.46%\nVal Loss: 0.9219, Acc: 71.12%\nEpoch 9/10\nTrain Loss: 0.8688, Acc: 73.43%\nVal Loss: 0.9018, Acc: 71.51%\nEpoch 10/10\nTrain Loss: 0.8289, Acc: 74.61%\nVal Loss: 0.8714, Acc: 72.46%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▃▄▅▇▇███</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>74.60539</td></tr><tr><td>train_loss</td><td>0.8289</td></tr><tr><td>val_acc</td><td>72.46029</td></tr><tr><td>val_loss</td><td>0.87139</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">iconic-sweep-10</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/3ecgr25p' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/3ecgr25p</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_144930-3ecgr25p/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e1hiotdo with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_145327-e1hiotdo</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/e1hiotdo' target=\"_blank\">amber-sweep-11</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/e1hiotdo' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/e1hiotdo</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 1.9734, Acc: 39.25%\nVal Loss: 1.1697, Acc: 63.23%\nEpoch 2/10\nTrain Loss: 1.0246, Acc: 68.52%\nVal Loss: 0.9209, Acc: 71.17%\nEpoch 3/10\nTrain Loss: 0.8391, Acc: 74.36%\nVal Loss: 0.8998, Acc: 71.52%\nEpoch 4/10\nTrain Loss: 0.7618, Acc: 76.42%\nVal Loss: 0.8389, Acc: 73.46%\nEpoch 5/10\nTrain Loss: 0.7143, Acc: 77.67%\nVal Loss: 0.8374, Acc: 74.01%\nEpoch 6/10\nTrain Loss: 0.6773, Acc: 78.86%\nVal Loss: 0.8301, Acc: 74.18%\nEpoch 7/10\nTrain Loss: 0.6496, Acc: 79.65%\nVal Loss: 0.8580, Acc: 74.17%\nEpoch 8/10\nTrain Loss: 0.6303, Acc: 80.14%\nVal Loss: 0.8562, Acc: 74.25%\nEpoch 9/10\nTrain Loss: 0.6158, Acc: 80.57%\nVal Loss: 0.8267, Acc: 74.64%\nEpoch 10/10\nTrain Loss: 0.6044, Acc: 80.92%\nVal Loss: 0.7977, Acc: 75.33%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▆▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>80.91776</td></tr><tr><td>train_loss</td><td>0.60442</td></tr><tr><td>val_acc</td><td>75.33347</td></tr><tr><td>val_loss</td><td>0.7977</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">amber-sweep-11</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/e1hiotdo' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/e1hiotdo</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_145327-e1hiotdo/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gj8qzdck with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_145719-gj8qzdck</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/gj8qzdck' target=\"_blank\">swift-sweep-12</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/gj8qzdck' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/gj8qzdck</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.2572, Acc: 32.28%\nVal Loss: 1.7513, Acc: 41.84%\nEpoch 2/15\nTrain Loss: 1.4334, Acc: 55.80%\nVal Loss: 1.0554, Acc: 68.41%\nEpoch 3/15\nTrain Loss: 1.0429, Acc: 69.04%\nVal Loss: 0.9534, Acc: 71.40%\nEpoch 4/15\nTrain Loss: 0.9300, Acc: 72.32%\nVal Loss: 0.9132, Acc: 72.43%\nEpoch 5/15\nTrain Loss: 0.8649, Acc: 74.08%\nVal Loss: 0.9180, Acc: 72.31%\nEpoch 6/15\nTrain Loss: 0.8280, Acc: 74.99%\nVal Loss: 0.8964, Acc: 73.03%\nEpoch 7/15\nTrain Loss: 0.7859, Acc: 76.34%\nVal Loss: 0.8752, Acc: 73.79%\nEpoch 8/15\nTrain Loss: 0.7617, Acc: 77.00%\nVal Loss: 0.8549, Acc: 74.27%\nEpoch 9/15\nTrain Loss: 0.7369, Acc: 77.72%\nVal Loss: 0.8398, Acc: 74.51%\nEpoch 10/15\nTrain Loss: 0.7195, Acc: 78.18%\nVal Loss: 0.8544, Acc: 74.62%\nEpoch 11/15\nTrain Loss: 0.7056, Acc: 78.58%\nVal Loss: 0.8485, Acc: 74.58%\nEpoch 12/15\nTrain Loss: 0.6942, Acc: 78.81%\nVal Loss: 0.8479, Acc: 74.88%\nEpoch 13/15\nTrain Loss: 0.6825, Acc: 79.14%\nVal Loss: 0.8410, Acc: 75.09%\nEpoch 14/15\nTrain Loss: 0.6793, Acc: 79.06%\nVal Loss: 0.8267, Acc: 75.10%\nEpoch 15/15\nTrain Loss: 0.6655, Acc: 79.47%\nVal Loss: 0.8229, Acc: 75.24%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▇▇▇▇██████████</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>79.46679</td></tr><tr><td>train_loss</td><td>0.66553</td></tr><tr><td>val_acc</td><td>75.23799</td></tr><tr><td>val_loss</td><td>0.82287</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">swift-sweep-12</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/gj8qzdck' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/gj8qzdck</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_145719-gj8qzdck/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oskj9yx6 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_150703-oskj9yx6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/oskj9yx6' target=\"_blank\">lunar-sweep-13</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/oskj9yx6' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/oskj9yx6</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 1.4046, Acc: 56.68%\nVal Loss: 0.9350, Acc: 70.49%\nEpoch 2/10\nTrain Loss: 0.7508, Acc: 76.79%\nVal Loss: 0.8580, Acc: 73.27%\nEpoch 3/10\nTrain Loss: 0.6615, Acc: 79.14%\nVal Loss: 0.8463, Acc: 74.17%\nEpoch 4/10\nTrain Loss: 0.6086, Acc: 80.74%\nVal Loss: 0.8234, Acc: 75.17%\nEpoch 5/10\nTrain Loss: 0.5759, Acc: 81.67%\nVal Loss: 0.7827, Acc: 74.90%\nEpoch 6/10\nTrain Loss: 0.5585, Acc: 82.06%\nVal Loss: 0.8171, Acc: 75.12%\nEpoch 7/10\nTrain Loss: 0.5357, Acc: 82.79%\nVal Loss: 0.7760, Acc: 76.07%\nEpoch 8/10\nTrain Loss: 0.5161, Acc: 83.33%\nVal Loss: 0.8172, Acc: 75.91%\nEpoch 9/10\nTrain Loss: 0.5048, Acc: 83.57%\nVal Loss: 0.7907, Acc: 75.76%\nEpoch 10/10\nTrain Loss: 0.4957, Acc: 83.74%\nVal Loss: 0.8165, Acc: 75.94%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▄▃▁▃▁▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>83.73969</td></tr><tr><td>train_loss</td><td>0.49566</td></tr><tr><td>val_acc</td><td>75.94398</td></tr><tr><td>val_loss</td><td>0.81648</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lunar-sweep-13</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/oskj9yx6' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/oskj9yx6</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_150703-oskj9yx6/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oop6438f with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_151351-oop6438f</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/oop6438f' target=\"_blank\">confused-sweep-14</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/oop6438f' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/oop6438f</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.5673, Acc: 51.57%\nVal Loss: 0.9523, Acc: 70.36%\nEpoch 2/15\nTrain Loss: 0.8201, Acc: 74.98%\nVal Loss: 0.8907, Acc: 73.09%\nEpoch 3/15\nTrain Loss: 0.7193, Acc: 77.99%\nVal Loss: 0.8706, Acc: 73.72%\nEpoch 4/15\nTrain Loss: 0.6774, Acc: 79.13%\nVal Loss: 0.8422, Acc: 74.10%\nEpoch 5/15\nTrain Loss: 0.6409, Acc: 80.29%\nVal Loss: 0.8882, Acc: 73.98%\nEpoch 6/15\nTrain Loss: 0.6311, Acc: 80.24%\nVal Loss: 0.8514, Acc: 74.81%\nEpoch 7/15\nTrain Loss: 0.6095, Acc: 80.94%\nVal Loss: 0.8620, Acc: 74.52%\nEpoch 8/15\nTrain Loss: 0.5893, Acc: 81.41%\nVal Loss: 0.8133, Acc: 75.04%\nEpoch 9/15\nTrain Loss: 0.5806, Acc: 81.66%\nVal Loss: 0.8053, Acc: 75.37%\nEpoch 10/15\nTrain Loss: 0.5764, Acc: 81.65%\nVal Loss: 0.8388, Acc: 75.37%\nEpoch 11/15\nTrain Loss: 0.5600, Acc: 82.25%\nVal Loss: 0.7970, Acc: 76.08%\nEpoch 12/15\nTrain Loss: 0.5469, Acc: 82.58%\nVal Loss: 0.8427, Acc: 75.69%\nEpoch 13/15\nTrain Loss: 0.5532, Acc: 82.23%\nVal Loss: 0.8203, Acc: 76.15%\nEpoch 14/15\nTrain Loss: 0.5329, Acc: 82.88%\nVal Loss: 0.8377, Acc: 76.05%\nEpoch 15/15\nTrain Loss: 0.5302, Acc: 83.01%\nVal Loss: 0.8105, Acc: 76.03%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▆▅▆▆▇▇▇█▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▅▃▄▂▁▃▁▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>83.008</td></tr><tr><td>train_loss</td><td>0.53022</td></tr><tr><td>val_acc</td><td>76.03368</td></tr><tr><td>val_loss</td><td>0.8105</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">confused-sweep-14</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/oop6438f' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/oop6438f</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_151351-oop6438f/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vf7xpo41 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_151707-vf7xpo41</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vf7xpo41' target=\"_blank\">azure-sweep-15</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vf7xpo41' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vf7xpo41</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.7330, Acc: 46.50%\nVal Loss: 0.9966, Acc: 68.05%\nEpoch 2/15\nTrain Loss: 0.9578, Acc: 70.21%\nVal Loss: 0.9333, Acc: 71.18%\nEpoch 3/15\nTrain Loss: 0.8146, Acc: 74.97%\nVal Loss: 0.8729, Acc: 73.35%\nEpoch 4/15\nTrain Loss: 0.7570, Acc: 76.58%\nVal Loss: 0.8416, Acc: 73.85%\nEpoch 5/15\nTrain Loss: 0.7062, Acc: 78.17%\nVal Loss: 0.8570, Acc: 74.88%\nEpoch 6/15\nTrain Loss: 0.6742, Acc: 79.21%\nVal Loss: 0.8799, Acc: 74.75%\nEpoch 7/15\nTrain Loss: 0.6451, Acc: 80.02%\nVal Loss: 0.8132, Acc: 74.91%\nEpoch 8/15\nTrain Loss: 0.6207, Acc: 80.71%\nVal Loss: 0.8467, Acc: 75.56%\nEpoch 9/15\nTrain Loss: 0.6167, Acc: 80.77%\nVal Loss: 0.8610, Acc: 75.24%\nEpoch 10/15\nTrain Loss: 0.6051, Acc: 80.92%\nVal Loss: 0.8614, Acc: 75.21%\nEpoch 11/15\nTrain Loss: 0.5856, Acc: 81.57%\nVal Loss: 0.8465, Acc: 75.60%\nEpoch 12/15\nTrain Loss: 0.5792, Acc: 81.66%\nVal Loss: 0.8150, Acc: 75.65%\nEpoch 13/15\nTrain Loss: 0.5691, Acc: 82.00%\nVal Loss: 0.8408, Acc: 75.73%\nEpoch 14/15\nTrain Loss: 0.5706, Acc: 81.84%\nVal Loss: 0.8100, Acc: 75.62%\nEpoch 15/15\nTrain Loss: 0.5531, Acc: 82.46%\nVal Loss: 0.8200, Acc: 76.13%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▆▇▇▇█▇▇█████</td></tr><tr><td>val_loss</td><td>█▆▃▂▃▄▁▂▃▃▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>82.46392</td></tr><tr><td>train_loss</td><td>0.55308</td></tr><tr><td>val_acc</td><td>76.13206</td></tr><tr><td>val_loss</td><td>0.82</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">azure-sweep-15</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vf7xpo41' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vf7xpo41</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_151707-vf7xpo41/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 682bk2iw with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_152029-682bk2iw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/682bk2iw' target=\"_blank\">snowy-sweep-16</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/682bk2iw' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/682bk2iw</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 2.1580, Acc: 37.31%\nVal Loss: 1.2994, Acc: 60.84%\nEpoch 2/10\nTrain Loss: 1.3232, Acc: 61.32%\nVal Loss: 1.0723, Acc: 68.42%\nEpoch 3/10\nTrain Loss: 1.1875, Acc: 65.59%\nVal Loss: 1.0218, Acc: 69.09%\nEpoch 4/10\nTrain Loss: 1.1235, Acc: 67.59%\nVal Loss: 1.0007, Acc: 70.46%\nEpoch 5/10\nTrain Loss: 1.0876, Acc: 68.58%\nVal Loss: 0.9795, Acc: 71.07%\nEpoch 6/10\nTrain Loss: 1.0632, Acc: 69.42%\nVal Loss: 0.9620, Acc: 71.02%\nEpoch 7/10\nTrain Loss: 1.0424, Acc: 69.96%\nVal Loss: 0.9723, Acc: 70.74%\nEpoch 8/10\nTrain Loss: 1.0245, Acc: 70.54%\nVal Loss: 0.9795, Acc: 71.00%\nEpoch 9/10\nTrain Loss: 1.0126, Acc: 70.90%\nVal Loss: 0.9656, Acc: 71.60%\nEpoch 10/10\nTrain Loss: 1.0012, Acc: 71.22%\nVal Loss: 0.9512, Acc: 71.67%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▆▇██▇███</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>71.21538</td></tr><tr><td>train_loss</td><td>1.00123</td></tr><tr><td>val_acc</td><td>71.66749</td></tr><tr><td>val_loss</td><td>0.95121</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">snowy-sweep-16</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/682bk2iw' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/682bk2iw</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_152029-682bk2iw/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y0vvjpjv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_152603-y0vvjpjv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/y0vvjpjv' target=\"_blank\">vibrant-sweep-17</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/y0vvjpjv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/y0vvjpjv</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 2.1848, Acc: 36.30%\nVal Loss: 1.4162, Acc: 56.57%\nEpoch 2/10\nTrain Loss: 1.1347, Acc: 65.97%\nVal Loss: 0.9780, Acc: 69.51%\nEpoch 3/10\nTrain Loss: 0.9094, Acc: 72.94%\nVal Loss: 0.9145, Acc: 71.97%\nEpoch 4/10\nTrain Loss: 0.8226, Acc: 75.40%\nVal Loss: 0.8949, Acc: 72.81%\nEpoch 5/10\nTrain Loss: 0.7705, Acc: 76.84%\nVal Loss: 0.8672, Acc: 73.48%\nEpoch 6/10\nTrain Loss: 0.7365, Acc: 77.69%\nVal Loss: 0.8350, Acc: 73.43%\nEpoch 7/10\nTrain Loss: 0.7109, Acc: 78.32%\nVal Loss: 0.8462, Acc: 74.03%\nEpoch 8/10\nTrain Loss: 0.6881, Acc: 78.93%\nVal Loss: 0.8172, Acc: 74.46%\nEpoch 9/10\nTrain Loss: 0.6676, Acc: 79.38%\nVal Loss: 0.8003, Acc: 74.72%\nEpoch 10/10\nTrain Loss: 0.6536, Acc: 79.73%\nVal Loss: 0.7954, Acc: 74.49%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇██████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▇▇██████</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>79.72945</td></tr><tr><td>train_loss</td><td>0.65364</td></tr><tr><td>val_acc</td><td>74.49437</td></tr><tr><td>val_loss</td><td>0.79536</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vibrant-sweep-17</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/y0vvjpjv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/y0vvjpjv</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_152603-y0vvjpjv/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1sxvkv5x with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_152823-1sxvkv5x</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/1sxvkv5x' target=\"_blank\">pious-sweep-18</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/1sxvkv5x' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/1sxvkv5x</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 1.8565, Acc: 44.54%\nVal Loss: 1.0665, Acc: 67.06%\nEpoch 2/10\nTrain Loss: 0.9642, Acc: 70.94%\nVal Loss: 0.9299, Acc: 71.33%\nEpoch 3/10\nTrain Loss: 0.8129, Acc: 75.43%\nVal Loss: 0.9166, Acc: 72.70%\nEpoch 4/10\nTrain Loss: 0.7393, Acc: 77.52%\nVal Loss: 0.8785, Acc: 73.05%\nEpoch 5/10\nTrain Loss: 0.7016, Acc: 78.41%\nVal Loss: 0.8376, Acc: 73.92%\nEpoch 6/10\nTrain Loss: 0.6830, Acc: 78.70%\nVal Loss: 0.8279, Acc: 74.46%\nEpoch 7/10\nTrain Loss: 0.6344, Acc: 80.40%\nVal Loss: 0.8265, Acc: 75.28%\nEpoch 8/10\nTrain Loss: 0.6350, Acc: 80.02%\nVal Loss: 0.8087, Acc: 75.31%\nEpoch 9/10\nTrain Loss: 0.6265, Acc: 80.15%\nVal Loss: 0.8231, Acc: 74.95%\nEpoch 10/10\nTrain Loss: 0.5942, Acc: 81.39%\nVal Loss: 0.8046, Acc: 75.63%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▆▇▇██▇█</td></tr><tr><td>val_loss</td><td>█▄▄▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>81.39287</td></tr><tr><td>train_loss</td><td>0.5942</td></tr><tr><td>val_acc</td><td>75.63439</td></tr><tr><td>val_loss</td><td>0.80463</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">pious-sweep-18</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/1sxvkv5x' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/1sxvkv5x</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_152823-1sxvkv5x/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eqpz333v with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_153039-eqpz333v</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/eqpz333v' target=\"_blank\">rural-sweep-19</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/eqpz333v' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/eqpz333v</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.4409, Acc: 28.78%\nVal Loss: 2.0366, Acc: 35.57%\nEpoch 2/15\nTrain Loss: 1.7160, Acc: 47.78%\nVal Loss: 1.4151, Acc: 58.57%\nEpoch 3/15\nTrain Loss: 1.1653, Acc: 67.04%\nVal Loss: 1.1424, Acc: 67.59%\nEpoch 4/15\nTrain Loss: 0.9786, Acc: 72.42%\nVal Loss: 1.0481, Acc: 69.87%\nEpoch 5/15\nTrain Loss: 0.8922, Acc: 74.40%\nVal Loss: 0.9820, Acc: 71.76%\nEpoch 6/15\nTrain Loss: 0.8450, Acc: 75.52%\nVal Loss: 0.9760, Acc: 71.70%\nEpoch 7/15\nTrain Loss: 0.8054, Acc: 76.48%\nVal Loss: 1.0276, Acc: 70.94%\nEpoch 8/15\nTrain Loss: 0.7711, Acc: 77.39%\nVal Loss: 0.9448, Acc: 72.84%\nEpoch 9/15\nTrain Loss: 0.7504, Acc: 77.85%\nVal Loss: 0.9140, Acc: 73.20%\nEpoch 10/15\nTrain Loss: 0.7296, Acc: 78.43%\nVal Loss: 0.9243, Acc: 73.13%\nEpoch 11/15\nTrain Loss: 0.7190, Acc: 78.51%\nVal Loss: 0.9042, Acc: 73.53%\nEpoch 12/15\nTrain Loss: 0.7047, Acc: 78.81%\nVal Loss: 0.9551, Acc: 72.63%\nEpoch 13/15\nTrain Loss: 0.6898, Acc: 79.20%\nVal Loss: 0.9016, Acc: 73.65%\nEpoch 14/15\nTrain Loss: 0.6815, Acc: 79.40%\nVal Loss: 0.8858, Acc: 73.68%\nEpoch 15/15\nTrain Loss: 0.6680, Acc: 79.73%\nVal Loss: 0.9003, Acc: 73.87%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▇▇██▇████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>79.72835</td></tr><tr><td>train_loss</td><td>0.66803</td></tr><tr><td>val_acc</td><td>73.8665</td></tr><tr><td>val_loss</td><td>0.90026</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">rural-sweep-19</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/eqpz333v' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/eqpz333v</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_153039-eqpz333v/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eh9vz2pc with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_154009-eh9vz2pc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/eh9vz2pc' target=\"_blank\">sandy-sweep-20</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/eh9vz2pc' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/eh9vz2pc</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 1.9374, Acc: 41.61%\nVal Loss: 1.2123, Acc: 62.61%\nEpoch 2/10\nTrain Loss: 0.9624, Acc: 71.29%\nVal Loss: 0.9207, Acc: 71.33%\nEpoch 3/10\nTrain Loss: 0.8048, Acc: 75.78%\nVal Loss: 0.9002, Acc: 72.65%\nEpoch 4/10\nTrain Loss: 0.7378, Acc: 77.64%\nVal Loss: 0.8695, Acc: 73.26%\nEpoch 5/10\nTrain Loss: 0.6903, Acc: 78.77%\nVal Loss: 0.8785, Acc: 73.78%\nEpoch 6/10\nTrain Loss: 0.6619, Acc: 79.56%\nVal Loss: 0.8391, Acc: 74.14%\nEpoch 7/10\nTrain Loss: 0.6459, Acc: 80.00%\nVal Loss: 0.8238, Acc: 74.57%\nEpoch 8/10\nTrain Loss: 0.6217, Acc: 80.50%\nVal Loss: 0.7963, Acc: 75.13%\nEpoch 9/10\nTrain Loss: 0.6067, Acc: 80.91%\nVal Loss: 0.8051, Acc: 74.92%\nEpoch 10/10\nTrain Loss: 0.5939, Acc: 81.28%\nVal Loss: 0.8182, Acc: 75.26%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇██████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>81.2814</td></tr><tr><td>train_loss</td><td>0.5939</td></tr><tr><td>val_acc</td><td>75.25535</td></tr><tr><td>val_loss</td><td>0.81824</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sandy-sweep-20</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/eh9vz2pc' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/eh9vz2pc</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_154009-eh9vz2pc/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 05iwrav9 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_154224-05iwrav9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/05iwrav9' target=\"_blank\">stellar-sweep-21</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/05iwrav9' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/05iwrav9</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.2575, Acc: 33.58%\nVal Loss: 1.7159, Acc: 45.53%\nEpoch 2/15\nTrain Loss: 1.3144, Acc: 59.91%\nVal Loss: 1.0481, Acc: 68.16%\nEpoch 3/15\nTrain Loss: 0.9773, Acc: 70.71%\nVal Loss: 0.9763, Acc: 70.21%\nEpoch 4/15\nTrain Loss: 0.8661, Acc: 74.01%\nVal Loss: 0.9087, Acc: 71.59%\nEpoch 5/15\nTrain Loss: 0.8032, Acc: 75.73%\nVal Loss: 0.8968, Acc: 72.41%\nEpoch 6/15\nTrain Loss: 0.7693, Acc: 76.42%\nVal Loss: 0.8466, Acc: 73.13%\nEpoch 7/15\nTrain Loss: 0.7305, Acc: 77.61%\nVal Loss: 0.8583, Acc: 73.68%\nEpoch 8/15\nTrain Loss: 0.7100, Acc: 78.10%\nVal Loss: 0.8433, Acc: 74.05%\nEpoch 9/15\nTrain Loss: 0.6819, Acc: 79.05%\nVal Loss: 0.8332, Acc: 74.49%\nEpoch 10/15\nTrain Loss: 0.6672, Acc: 79.38%\nVal Loss: 0.8466, Acc: 74.78%\nEpoch 11/15\nTrain Loss: 0.6487, Acc: 79.97%\nVal Loss: 0.8449, Acc: 74.85%\nEpoch 12/15\nTrain Loss: 0.6439, Acc: 79.94%\nVal Loss: 0.8206, Acc: 74.83%\nEpoch 13/15\nTrain Loss: 0.6315, Acc: 80.19%\nVal Loss: 0.8718, Acc: 74.78%\nEpoch 14/15\nTrain Loss: 0.6157, Acc: 80.87%\nVal Loss: 0.7974, Acc: 75.26%\nEpoch 15/15\nTrain Loss: 0.6110, Acc: 80.89%\nVal Loss: 0.8392, Acc: 74.87%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▇▇▇▇█████████</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>80.89293</td></tr><tr><td>train_loss</td><td>0.61102</td></tr><tr><td>val_acc</td><td>74.87341</td></tr><tr><td>val_loss</td><td>0.83921</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">stellar-sweep-21</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/05iwrav9' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/05iwrav9</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_154224-05iwrav9/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ih4bxt3q with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_155219-ih4bxt3q</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/ih4bxt3q' target=\"_blank\">cosmic-sweep-22</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/ih4bxt3q' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/ih4bxt3q</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 2.5453, Acc: 27.60%\nVal Loss: 2.0865, Acc: 36.55%\nEpoch 2/10\nTrain Loss: 2.0107, Acc: 38.52%\nVal Loss: 1.6148, Acc: 48.87%\nEpoch 3/10\nTrain Loss: 1.6613, Acc: 48.91%\nVal Loss: 1.2463, Acc: 61.50%\nEpoch 4/10\nTrain Loss: 1.4642, Acc: 55.17%\nVal Loss: 1.1292, Acc: 64.97%\nEpoch 5/10\nTrain Loss: 1.3559, Acc: 58.80%\nVal Loss: 1.0711, Acc: 67.61%\nEpoch 6/10\nTrain Loss: 1.2791, Acc: 61.39%\nVal Loss: 1.0266, Acc: 68.66%\nEpoch 7/10\nTrain Loss: 1.2338, Acc: 62.81%\nVal Loss: 0.9922, Acc: 69.91%\nEpoch 8/10\nTrain Loss: 1.1852, Acc: 64.32%\nVal Loss: 1.0072, Acc: 70.10%\nEpoch 9/10\nTrain Loss: 1.1544, Acc: 65.40%\nVal Loss: 0.9742, Acc: 70.57%\nEpoch 10/10\nTrain Loss: 1.1238, Acc: 66.45%\nVal Loss: 0.9838, Acc: 70.52%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▃▅▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▇▇█████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>66.4453</td></tr><tr><td>train_loss</td><td>1.12379</td></tr><tr><td>val_acc</td><td>70.52458</td></tr><tr><td>val_loss</td><td>0.98381</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cosmic-sweep-22</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/ih4bxt3q' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/ih4bxt3q</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_155219-ih4bxt3q/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vihpbsf2 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_155601-vihpbsf2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vihpbsf2' target=\"_blank\">golden-sweep-23</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vihpbsf2' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vihpbsf2</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 1.5892, Acc: 51.98%\nVal Loss: 1.0002, Acc: 69.75%\nEpoch 2/10\nTrain Loss: 0.9114, Acc: 72.33%\nVal Loss: 0.8719, Acc: 72.41%\nEpoch 3/10\nTrain Loss: 0.8195, Acc: 75.04%\nVal Loss: 0.8747, Acc: 73.35%\nEpoch 4/10\nTrain Loss: 0.7654, Acc: 76.59%\nVal Loss: 0.8784, Acc: 72.95%\nEpoch 5/10\nTrain Loss: 0.7339, Acc: 77.55%\nVal Loss: 0.8646, Acc: 73.50%\nEpoch 6/10\nTrain Loss: 0.7078, Acc: 78.24%\nVal Loss: 0.8785, Acc: 73.62%\nEpoch 7/10\nTrain Loss: 0.6927, Acc: 78.52%\nVal Loss: 0.8622, Acc: 74.21%\nEpoch 8/10\nTrain Loss: 0.6837, Acc: 78.85%\nVal Loss: 0.8252, Acc: 74.74%\nEpoch 9/10\nTrain Loss: 0.6792, Acc: 78.83%\nVal Loss: 0.8278, Acc: 74.50%\nEpoch 10/10\nTrain Loss: 0.6512, Acc: 79.82%\nVal Loss: 0.8382, Acc: 74.60%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▅▆▆▇███</td></tr><tr><td>val_loss</td><td>█▃▃▃▃▃▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>79.82436</td></tr><tr><td>train_loss</td><td>0.65116</td></tr><tr><td>val_acc</td><td>74.60432</td></tr><tr><td>val_loss</td><td>0.83819</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">golden-sweep-23</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vihpbsf2' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/vihpbsf2</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_155601-vihpbsf2/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fq5wifyo with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_155853-fq5wifyo</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/fq5wifyo' target=\"_blank\">likely-sweep-24</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/fq5wifyo' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/fq5wifyo</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 1.0451, Acc: 68.22%\nVal Loss: 0.9252, Acc: 71.66%\nEpoch 2/10\nTrain Loss: 0.6480, Acc: 79.72%\nVal Loss: 0.8622, Acc: 73.75%\nEpoch 3/10\nTrain Loss: 0.5954, Acc: 81.10%\nVal Loss: 0.8244, Acc: 74.69%\nEpoch 4/10\nTrain Loss: 0.5648, Acc: 81.87%\nVal Loss: 0.7952, Acc: 74.87%\nEpoch 5/10\nTrain Loss: 0.5379, Acc: 82.78%\nVal Loss: 0.7798, Acc: 75.47%\nEpoch 6/10\nTrain Loss: 0.5222, Acc: 83.10%\nVal Loss: 0.8445, Acc: 75.85%\nEpoch 7/10\nTrain Loss: 0.5151, Acc: 83.24%\nVal Loss: 0.7786, Acc: 75.55%\nEpoch 8/10\nTrain Loss: 0.4989, Acc: 83.65%\nVal Loss: 0.8201, Acc: 75.71%\nEpoch 9/10\nTrain Loss: 0.4874, Acc: 84.05%\nVal Loss: 0.8110, Acc: 76.06%\nEpoch 10/10\nTrain Loss: 0.4824, Acc: 84.12%\nVal Loss: 0.7887, Acc: 75.52%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▆▇█▇▇█▇</td></tr><tr><td>val_loss</td><td>█▅▃▂▁▄▁▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>84.12375</td></tr><tr><td>train_loss</td><td>0.4824</td></tr><tr><td>val_acc</td><td>75.51575</td></tr><tr><td>val_loss</td><td>0.78872</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">likely-sweep-24</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/fq5wifyo' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/fq5wifyo</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_155853-fq5wifyo/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v6uqq6tb with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_160446-v6uqq6tb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/v6uqq6tb' target=\"_blank\">neat-sweep-25</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/v6uqq6tb' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/v6uqq6tb</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.2023, Acc: 63.48%\nVal Loss: 0.9374, Acc: 70.98%\nEpoch 2/15\nTrain Loss: 0.7656, Acc: 76.32%\nVal Loss: 0.8799, Acc: 73.31%\nEpoch 3/15\nTrain Loss: 0.6832, Acc: 78.70%\nVal Loss: 0.8715, Acc: 74.01%\nEpoch 4/15\nTrain Loss: 0.6542, Acc: 79.45%\nVal Loss: 0.8128, Acc: 74.49%\nEpoch 5/15\nTrain Loss: 0.6223, Acc: 80.43%\nVal Loss: 0.8497, Acc: 74.86%\nEpoch 6/15\nTrain Loss: 0.6021, Acc: 80.92%\nVal Loss: 0.8030, Acc: 75.01%\nEpoch 7/15\nTrain Loss: 0.5919, Acc: 81.22%\nVal Loss: 0.8172, Acc: 75.40%\nEpoch 8/15\nTrain Loss: 0.5799, Acc: 81.53%\nVal Loss: 0.8246, Acc: 75.43%\nEpoch 9/15\nTrain Loss: 0.5694, Acc: 81.84%\nVal Loss: 0.8179, Acc: 75.72%\nEpoch 10/15\nTrain Loss: 0.5642, Acc: 81.96%\nVal Loss: 0.8048, Acc: 75.19%\nEpoch 11/15\nTrain Loss: 0.5576, Acc: 82.16%\nVal Loss: 0.7998, Acc: 75.42%\nEpoch 12/15\nTrain Loss: 0.5532, Acc: 82.27%\nVal Loss: 0.8040, Acc: 75.53%\nEpoch 13/15\nTrain Loss: 0.5416, Acc: 82.71%\nVal Loss: 0.7887, Acc: 75.92%\nEpoch 14/15\nTrain Loss: 0.5461, Acc: 82.47%\nVal Loss: 0.7734, Acc: 76.12%\nEpoch 15/15\nTrain Loss: 0.5327, Acc: 82.86%\nVal Loss: 0.7953, Acc: 75.98%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▅▃▄▂▃▃▃▂▂▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>82.86094</td></tr><tr><td>train_loss</td><td>0.53274</td></tr><tr><td>val_acc</td><td>75.97581</td></tr><tr><td>val_loss</td><td>0.79531</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">neat-sweep-25</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/v6uqq6tb' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/v6uqq6tb</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_160446-v6uqq6tb/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uo6pujdn with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_161014-uo6pujdn</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/uo6pujdn' target=\"_blank\">decent-sweep-26</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/uo6pujdn' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/uo6pujdn</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.6826, Acc: 49.00%\nVal Loss: 0.9833, Acc: 68.59%\nEpoch 2/15\nTrain Loss: 0.8745, Acc: 72.92%\nVal Loss: 0.8375, Acc: 72.50%\nEpoch 3/15\nTrain Loss: 0.7482, Acc: 76.71%\nVal Loss: 0.8200, Acc: 74.29%\nEpoch 4/15\nTrain Loss: 0.6899, Acc: 78.46%\nVal Loss: 0.8262, Acc: 74.44%\nEpoch 5/15\nTrain Loss: 0.6451, Acc: 79.75%\nVal Loss: 0.8252, Acc: 74.69%\nEpoch 6/15\nTrain Loss: 0.6233, Acc: 80.42%\nVal Loss: 0.7637, Acc: 75.67%\nEpoch 7/15\nTrain Loss: 0.5933, Acc: 81.31%\nVal Loss: 0.8107, Acc: 75.33%\nEpoch 8/15\nTrain Loss: 0.5816, Acc: 81.61%\nVal Loss: 0.8100, Acc: 75.29%\nEpoch 9/15\nTrain Loss: 0.5680, Acc: 82.01%\nVal Loss: 0.7818, Acc: 75.72%\nEpoch 10/15\nTrain Loss: 0.5521, Acc: 82.50%\nVal Loss: 0.7805, Acc: 75.80%\nEpoch 11/15\nTrain Loss: 0.5380, Acc: 82.88%\nVal Loss: 0.8553, Acc: 75.81%\nEpoch 12/15\nTrain Loss: 0.5332, Acc: 82.96%\nVal Loss: 0.8068, Acc: 75.98%\nEpoch 13/15\nTrain Loss: 0.5382, Acc: 82.71%\nVal Loss: 0.7625, Acc: 75.95%\nEpoch 14/15\nTrain Loss: 0.5217, Acc: 83.21%\nVal Loss: 0.7591, Acc: 76.33%\nEpoch 15/15\nTrain Loss: 0.5134, Acc: 83.42%\nVal Loss: 0.7403, Acc: 76.56%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▂▃▃▂▂▄▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>83.41716</td></tr><tr><td>train_loss</td><td>0.51344</td></tr><tr><td>val_acc</td><td>76.55739</td></tr><tr><td>val_loss</td><td>0.7403</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">decent-sweep-26</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/uo6pujdn' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/uo6pujdn</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_161014-uo6pujdn/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 04smivhe with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_161330-04smivhe</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/04smivhe' target=\"_blank\">unique-sweep-27</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/04smivhe' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/04smivhe</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 2.5109, Acc: 28.89%\nVal Loss: 2.0238, Acc: 38.10%\nEpoch 2/10\nTrain Loss: 1.5624, Acc: 50.93%\nVal Loss: 1.1102, Acc: 65.64%\nEpoch 3/10\nTrain Loss: 1.0185, Acc: 68.82%\nVal Loss: 0.9203, Acc: 70.50%\nEpoch 4/10\nTrain Loss: 0.8715, Acc: 73.00%\nVal Loss: 0.8712, Acc: 72.10%\nEpoch 5/10\nTrain Loss: 0.7973, Acc: 75.35%\nVal Loss: 0.8537, Acc: 72.46%\nEpoch 6/10\nTrain Loss: 0.7430, Acc: 76.99%\nVal Loss: 0.8379, Acc: 73.87%\nEpoch 7/10\nTrain Loss: 0.7068, Acc: 78.18%\nVal Loss: 0.8035, Acc: 74.40%\nEpoch 8/10\nTrain Loss: 0.6847, Acc: 78.60%\nVal Loss: 0.8081, Acc: 74.29%\nEpoch 9/10\nTrain Loss: 0.6562, Acc: 79.48%\nVal Loss: 0.8043, Acc: 74.90%\nEpoch 10/10\nTrain Loss: 0.6404, Acc: 80.00%\nVal Loss: 0.8069, Acc: 75.12%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇█████</td></tr><tr><td>train_loss</td><td>█▄▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>80.0048</td></tr><tr><td>train_loss</td><td>0.64039</td></tr><tr><td>val_acc</td><td>75.11646</td></tr><tr><td>val_loss</td><td>0.80689</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">unique-sweep-27</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/04smivhe' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/04smivhe</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_161330-04smivhe/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z4a2zz82 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_161627-z4a2zz82</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/z4a2zz82' target=\"_blank\">lemon-sweep-28</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/z4a2zz82' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/z4a2zz82</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.3089, Acc: 60.64%\nVal Loss: 0.9456, Acc: 71.43%\nEpoch 2/15\nTrain Loss: 0.7817, Acc: 76.15%\nVal Loss: 0.8573, Acc: 73.20%\nEpoch 3/15\nTrain Loss: 0.7002, Acc: 78.28%\nVal Loss: 0.8484, Acc: 73.97%\nEpoch 4/15\nTrain Loss: 0.6577, Acc: 79.48%\nVal Loss: 0.8318, Acc: 74.52%\nEpoch 5/15\nTrain Loss: 0.6275, Acc: 80.33%\nVal Loss: 0.8067, Acc: 74.87%\nEpoch 6/15\nTrain Loss: 0.6082, Acc: 80.82%\nVal Loss: 0.7863, Acc: 75.19%\nEpoch 7/15\nTrain Loss: 0.5930, Acc: 81.26%\nVal Loss: 0.7977, Acc: 75.16%\nEpoch 8/15\nTrain Loss: 0.5812, Acc: 81.62%\nVal Loss: 0.7925, Acc: 75.02%\nEpoch 9/15\nTrain Loss: 0.5642, Acc: 82.12%\nVal Loss: 0.8068, Acc: 75.41%\nEpoch 10/15\nTrain Loss: 0.5582, Acc: 82.19%\nVal Loss: 0.8024, Acc: 75.83%\nEpoch 11/15\nTrain Loss: 0.5597, Acc: 82.06%\nVal Loss: 0.7603, Acc: 75.66%\nEpoch 12/15\nTrain Loss: 0.5426, Acc: 82.60%\nVal Loss: 0.8041, Acc: 75.50%\nEpoch 13/15\nTrain Loss: 0.5383, Acc: 82.76%\nVal Loss: 0.8117, Acc: 75.97%\nEpoch 14/15\nTrain Loss: 0.5326, Acc: 82.88%\nVal Loss: 0.8164, Acc: 75.93%\nEpoch 15/15\nTrain Loss: 0.5284, Acc: 82.96%\nVal Loss: 0.8089, Acc: 75.47%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▆▆▇▇▇▇██▇██▇</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▂▂▂▃▃▁▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>82.96027</td></tr><tr><td>train_loss</td><td>0.52844</td></tr><tr><td>val_acc</td><td>75.46946</td></tr><tr><td>val_loss</td><td>0.80888</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lemon-sweep-28</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/z4a2zz82' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/z4a2zz82</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_161627-z4a2zz82/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2yew837r with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_162445-2yew837r</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/2yew837r' target=\"_blank\">happy-sweep-29</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/2yew837r' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/2yew837r</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.1083, Acc: 67.11%\nVal Loss: 0.9629, Acc: 70.57%\nEpoch 2/15\nTrain Loss: 0.6854, Acc: 79.01%\nVal Loss: 0.8795, Acc: 72.39%\nEpoch 3/15\nTrain Loss: 0.6285, Acc: 80.40%\nVal Loss: 0.8690, Acc: 73.82%\nEpoch 4/15\nTrain Loss: 0.5906, Acc: 81.47%\nVal Loss: 0.8232, Acc: 74.25%\nEpoch 5/15\nTrain Loss: 0.5742, Acc: 81.76%\nVal Loss: 0.8182, Acc: 73.68%\nEpoch 6/15\nTrain Loss: 0.5536, Acc: 82.38%\nVal Loss: 0.8186, Acc: 74.91%\nEpoch 7/15\nTrain Loss: 0.5413, Acc: 82.65%\nVal Loss: 0.8081, Acc: 74.86%\nEpoch 8/15\nTrain Loss: 0.5237, Acc: 83.29%\nVal Loss: 0.8119, Acc: 74.97%\nEpoch 9/15\nTrain Loss: 0.5111, Acc: 83.60%\nVal Loss: 0.7972, Acc: 75.36%\nEpoch 10/15\nTrain Loss: 0.5088, Acc: 83.53%\nVal Loss: 0.7974, Acc: 75.48%\nEpoch 11/15\nTrain Loss: 0.5022, Acc: 83.76%\nVal Loss: 0.8097, Acc: 75.19%\nEpoch 12/15\nTrain Loss: 0.4964, Acc: 83.88%\nVal Loss: 0.8004, Acc: 75.36%\nEpoch 13/15\nTrain Loss: 0.4877, Acc: 84.08%\nVal Loss: 0.8045, Acc: 75.42%\nEpoch 14/15\nTrain Loss: 0.4861, Acc: 84.06%\nVal Loss: 0.8092, Acc: 75.04%\nEpoch 15/15\nTrain Loss: 0.4775, Acc: 84.41%\nVal Loss: 0.7950, Acc: 75.48%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▆▅▇▇▇█████▇█</td></tr><tr><td>val_loss</td><td>█▅▄▂▂▂▂▂▁▁▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>84.4082</td></tr><tr><td>train_loss</td><td>0.47749</td></tr><tr><td>val_acc</td><td>75.47814</td></tr><tr><td>val_loss</td><td>0.79498</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">happy-sweep-29</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/2yew837r' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/2yew837r</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_162445-2yew837r/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wbrsydns with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_type: concat\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_163228-wbrsydns</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/wbrsydns' target=\"_blank\">lunar-sweep-30</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/sweeps/64q660zv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/wbrsydns' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/wbrsydns</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 1.5142, Acc: 54.02%\nVal Loss: 1.0001, Acc: 68.74%\nEpoch 2/10\nTrain Loss: 0.8397, Acc: 74.15%\nVal Loss: 0.8635, Acc: 72.71%\nEpoch 3/10\nTrain Loss: 0.7281, Acc: 77.54%\nVal Loss: 0.8369, Acc: 73.72%\nEpoch 4/10\nTrain Loss: 0.6878, Acc: 78.64%\nVal Loss: 0.8221, Acc: 74.04%\nEpoch 5/10\nTrain Loss: 0.6428, Acc: 80.01%\nVal Loss: 0.8310, Acc: 74.59%\nEpoch 6/10\nTrain Loss: 0.6204, Acc: 80.64%\nVal Loss: 0.8199, Acc: 74.61%\nEpoch 7/10\nTrain Loss: 0.6094, Acc: 80.95%\nVal Loss: 0.8108, Acc: 74.48%\nEpoch 8/10\nTrain Loss: 0.5889, Acc: 81.50%\nVal Loss: 0.8158, Acc: 75.18%\nEpoch 9/10\nTrain Loss: 0.5622, Acc: 82.34%\nVal Loss: 0.8262, Acc: 75.21%\nEpoch 10/10\nTrain Loss: 0.5605, Acc: 82.30%\nVal Loss: 0.8074, Acc: 75.14%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>82.30252</td></tr><tr><td>train_loss</td><td>0.56055</td></tr><tr><td>val_acc</td><td>75.13961</td></tr><tr><td>val_loss</td><td>0.80738</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lunar-sweep-30</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/wbrsydns' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention/runs/wbrsydns</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration-attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_163228-wbrsydns/logs</code>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport random\nimport wandb\n\n# =======================\n# Best Configuration\n# =======================\nbest_config = {\n    'embedding_dim': 256,\n    'hidden_dim': 256,\n    'enc_layers': 2,\n    'dec_layers': 2,\n    'cell_type': 'LSTM',\n    'dropout': 0.5,\n    'epochs': 15,\n    'beam_size': 5,\n    'attention_type': 'concat',\n    'batch_size': 256,\n    'learning_rate': 0.001\n}\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.char2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2char = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for char in text:\n                if char not in self.char2idx:\n                    self.char2idx[char] = self.size\n                    self.idx2char[self.size] = char\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.char2idx[c] for c in text]\n\n    def decode(self, idxs):\n        return ''.join([self.idx2char[i] for i in idxs if i not in [0, 1, 2]])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab, out_vocab, is_test=False):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        if not is_test:\n            inp_vocab.build([p[0] for p in self.pairs])\n            out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        if self.is_test:\n            return torch.tensor(x), lat, dev\n        y = [self.out_vocab.char2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.char2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y), lat, dev\n\ndef collate_fn(batch):\n    if len(batch[0]) == 3:  # Test batch\n        x_batch, lat, dev = zip(*batch)\n        x_lens = [len(x) for x in x_batch]\n        x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n        return x_pad, lat, dev, torch.tensor(x_lens)\n    else:  # Train/val batch\n        x_batch, y_batch, lat, dev = zip(*batch)\n        x_lens = [len(x) for x in x_batch]\n        y_lens = [len(y) for y in y_batch]\n        x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n        y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n        return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens), lat, dev\n\n# =======================\n# Model Components\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        return outputs, hidden\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_type='general'):\n        super().__init__()\n        self.attention_type = attention_type\n        if attention_type == 'general':\n            self.attn = nn.Linear(hidden_dim, hidden_dim)\n        elif attention_type == 'concat':\n            self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n            self.v = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs, mask=None):\n        batch_size, src_len, hidden_dim = encoder_outputs.size()\n        \n        if self.attention_type == 'general':\n            energy = torch.bmm(encoder_outputs, self.attn(hidden).unsqueeze(2)).squeeze(2)\n        elif self.attention_type == 'concat':\n            hidden_expanded = hidden.unsqueeze(1).repeat(1, src_len, 1)\n            concat = torch.cat((hidden_expanded, encoder_outputs), dim=2)\n            energy = self.v(torch.tanh(self.attn(concat))).squeeze(2)\n        else:  # dot\n            energy = torch.bmm(encoder_outputs, hidden.unsqueeze(2)).squeeze(2)\n        \n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, -1e10)\n        attention_weights = F.softmax(energy, dim=1)\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n        return context, attention_weights\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout, attention_type):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim + hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.attention = Attention(hidden_dim, attention_type)\n        self.out = nn.Linear(hidden_dim * 2, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n        if isinstance(hidden, tuple):  # LSTM\n            attn_hidden = hidden[0][-1]\n        else:  # GRU/RNN\n            attn_hidden = hidden[-1]\n        \n        context, _ = self.attention(attn_hidden, encoder_outputs, mask)\n        embedded = self.embedding(input_token)\n        rnn_input = torch.cat((embedded, context), dim=1).unsqueeze(1)\n        output, hidden = self.rnn(rnn_input, hidden)\n        \n        if isinstance(hidden, tuple):\n            output_hidden = hidden[0][-1]\n        else:\n            output_hidden = hidden[-1]\n        \n        output = torch.cat((output_hidden, context), dim=1)\n        output = self.dropout(output)\n        prediction = self.out(output)\n        return prediction, hidden, None\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def create_mask(self, src_lens, max_len):\n        batch_size = len(src_lens)\n        mask = torch.zeros(batch_size, max_len, device=self.device)\n        for i, length in enumerate(src_lens):\n            mask[i, :length] = 1\n        return mask\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        src_data, src_lens = src\n        encoder_outputs, enc_hidden = self.encoder(src_data, src_lens)\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n        \n        src_len = encoder_outputs.size(1)\n        mask = self.create_mask(src_lens, src_len)\n\n        if isinstance(enc_hidden, tuple):\n            dec_hidden = enc_hidden\n        else:\n            dec_hidden = enc_hidden\n\n        input_token = trg[:, 0]\n        for t in range(1, trg_len):\n            output, dec_hidden, _ = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n            outputs[:, t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if teacher_force else top1\n            \n        return outputs\n\n    def predict(self, src, src_lens, max_len=30):\n        self.eval()\n        with torch.no_grad():\n            encoder_outputs, enc_hidden = self.encoder(src, src_lens)\n            src_len = encoder_outputs.size(1)\n            mask = self.create_mask(src_lens.tolist(), src_len)\n            \n            if isinstance(enc_hidden, tuple):\n                dec_hidden = enc_hidden\n            else:\n                dec_hidden = enc_hidden\n            \n            input_token = torch.tensor([1], device=self.device)\n            output_seq = []\n            for _ in range(max_len):\n                output, dec_hidden, _ = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n                top1 = output.argmax(1)\n                if top1.item() == 2:\n                    break\n                output_seq.append(top1.item())\n                input_token = top1\n        return output_seq\n\n# =======================\n# Training and Evaluation\n# =======================\ndef train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    \n    for batch in loader:\n        src, trg, src_lens, _, _, _ = batch\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model((src, src_lens), trg)\n        \n        # Calculate loss\n        output_dim = output.shape[-1]\n        loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        \n        # Calculate accuracy\n        pred = output.argmax(dim=2)\n        mask = (trg[:, 1:] != 0)\n        correct = ((pred[:, 1:] == trg[:, 1:]) & mask).sum().item()\n        total_correct += correct\n        total_tokens += mask.sum().item()\n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(loader)\n    accuracy = (total_correct / total_tokens) * 100 if total_tokens > 0 else 0\n    return avg_loss, accuracy\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    \n    with torch.no_grad():\n        for batch in loader:\n            src, trg, src_lens, _, _, _ = batch\n            src, trg = src.to(device), trg.to(device)\n            output = model((src, src_lens), trg, teacher_forcing_ratio=0)\n            \n            # Calculate loss\n            output_dim = output.shape[-1]\n            loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n            \n            # Calculate accuracy\n            pred = output.argmax(dim=2)\n            mask = (trg[:, 1:] != 0)\n            correct = ((pred[:, 1:] == trg[:, 1:]) & mask).sum().item()\n            total_correct += correct\n            total_tokens += mask.sum().item()\n            total_loss += loss.item()\n    \n    avg_loss = total_loss / len(loader)\n    accuracy = (total_correct / total_tokens) * 100 if total_tokens > 0 else 0\n    return avg_loss, accuracy\n\n# =======================\n# Main Execution\n# =======================\ndef main():\n    wandb.init(config=best_config, project=\"dakshina-translit\")\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Initialize vocabularies\n    inp_vocab = Vocab()\n    out_vocab = Vocab()\n\n    # Load datasets\n    train_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.train.tsv\", inp_vocab, out_vocab)\n    dev_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.dev.tsv\", inp_vocab, out_vocab)\n    test_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.test.tsv\", inp_vocab, out_vocab, is_test=True)\n\n    # Create data loaders\n    train_loader = DataLoader(train_data, batch_size=best_config['batch_size'], \n                            shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=best_config['batch_size'],\n                          shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    # Initialize model\n    encoder = Encoder(inp_vocab.size, best_config['embedding_dim'], \n                     best_config['hidden_dim'], best_config['enc_layers'], \n                     best_config['cell_type'], best_config['dropout'])\n    \n    decoder = Decoder(out_vocab.size, best_config['embedding_dim'],\n                     best_config['hidden_dim'], best_config['dec_layers'],\n                     best_config['cell_type'], best_config['dropout'],\n                     best_config['attention_type'])\n    \n    model = Seq2Seq(encoder, decoder, device).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=best_config['learning_rate'])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    # Training loop\n    best_val_loss = float('inf')\n    for epoch in range(best_config['epochs']):\n        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n        \n        print(f\"\\nEpoch {epoch+1}/{best_config['epochs']}\")\n        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc\n        })\n        \n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(\"Best model saved!\")\n\n    # Test evaluation\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    model.eval()\n    \n    total_correct = 0\n    total_samples = 0\n    predictions = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            src, lat, dev, src_lens = batch\n            src = src.to(device)\n            pred_ids = model.predict(src, src_lens)\n            pred_str = out_vocab.decode(pred_ids)\n            true_str = dev[0]\n            \n            predictions.append({\n                'input': lat[0],\n                'true': true_str,\n                'pred': pred_str\n            })\n            \n            if pred_str == true_str:\n                total_correct += 1\n            total_samples += 1\n\n    # Calculate accuracy\n    accuracy = 100 * total_correct / total_samples\n    print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n    wandb.log({\"test_acc\": accuracy})\n\n     # Create and log a table of predictions\n    table = wandb.Table(columns=[\"Input\", \"True\", \"Predicted\"])\n    for p in predictions[:20]:  # Log first 20 predictions\n        table.add_data(p['input'], p['true'], p['pred'])\n    \n    wandb.log({\n        \"predictions\": table,\n        \"test_accuracy\": accuracy\n    })\n\n    \n    # Save predictions\n    with open(\"test_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n        f.write(f\"Test Accuracy: {accuracy:.2f}%\\n\\n\")\n        for p in predictions[:20]:\n            f.write(f\"Input: {p['input']}\\n\")\n            f.write(f\"True: {p['true']}\\n\")\n            f.write(f\"Pred: {p['pred']}\\n\\n\")\n    \n    # Print random samples\n    print(\"\\nRandom Samples:\")\n    samples = random.sample(predictions, 5)\n    for sample in samples:\n        print(f\"Input: {sample['input']}\")\n        print(f\"True: {sample['true']}\")\n        print(f\"Pred: {sample['pred']}\\n\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:26:00.438223Z","iopub.execute_input":"2025-05-19T18:26:00.438816Z","iopub.status.idle":"2025-05-19T18:30:19.354782Z","shell.execute_reply.started":"2025-05-19T18:26:00.438778Z","shell.execute_reply":"2025-05-19T18:30:19.354011Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanglesh_dlass3\u001b[0m (\u001b[33mmanglesh_dl_ass3\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_182600-3pgstmly</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/3pgstmly' target=\"_blank\">frosty-wildflower-34</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/3pgstmly' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/3pgstmly</a>"},"metadata":{}},{"name":"stdout","text":"\nEpoch 1/15\nTrain Loss: 1.6700 | Train Acc: 49.26%\nVal Loss: 0.9560 | Val Acc: 69.54%\nBest model saved!\n\nEpoch 2/15\nTrain Loss: 0.8696 | Train Acc: 73.11%\nVal Loss: 0.9008 | Val Acc: 71.14%\nBest model saved!\n\nEpoch 3/15\nTrain Loss: 0.7541 | Train Acc: 76.55%\nVal Loss: 0.8106 | Val Acc: 73.87%\nBest model saved!\n\nEpoch 4/15\nTrain Loss: 0.6842 | Train Acc: 78.64%\nVal Loss: 0.8309 | Val Acc: 74.67%\n\nEpoch 5/15\nTrain Loss: 0.6603 | Train Acc: 79.25%\nVal Loss: 0.8221 | Val Acc: 75.44%\n\nEpoch 6/15\nTrain Loss: 0.6305 | Train Acc: 80.20%\nVal Loss: 0.7608 | Val Acc: 75.27%\nBest model saved!\n\nEpoch 7/15\nTrain Loss: 0.6054 | Train Acc: 80.90%\nVal Loss: 0.7964 | Val Acc: 75.49%\n\nEpoch 8/15\nTrain Loss: 0.5982 | Train Acc: 81.02%\nVal Loss: 0.8013 | Val Acc: 75.42%\n\nEpoch 9/15\nTrain Loss: 0.5822 | Train Acc: 81.49%\nVal Loss: 0.7886 | Val Acc: 75.25%\n\nEpoch 10/15\nTrain Loss: 0.5663 | Train Acc: 81.96%\nVal Loss: 0.7844 | Val Acc: 75.90%\n\nEpoch 11/15\nTrain Loss: 0.5470 | Train Acc: 82.60%\nVal Loss: 0.7712 | Val Acc: 75.82%\n\nEpoch 12/15\nTrain Loss: 0.5418 | Train Acc: 82.67%\nVal Loss: 0.7733 | Val Acc: 76.17%\n\nEpoch 13/15\nTrain Loss: 0.5286 | Train Acc: 83.13%\nVal Loss: 0.8127 | Val Acc: 75.94%\n\nEpoch 14/15\nTrain Loss: 0.5188 | Train Acc: 83.38%\nVal Loss: 0.7591 | Val Acc: 76.43%\nBest model saved!\n\nEpoch 15/15\nTrain Loss: 0.5177 | Train Acc: 83.30%\nVal Loss: 0.8155 | Val Acc: 76.18%\n\nTest Accuracy: 43.38%\n\nRandom Samples:\nInput: कोच्चि\nTrue: kochi\nPred: kochchi\n\nInput: जतरा\nTrue: jatara\nPred: jatra\n\nInput: लैंगडन\nTrue: lengdan\nPred: langdon\n\nInput: सभ्यता\nTrue: sabhyata\nPred: sabhyata\n\nInput: काजी\nTrue: kaaji\nPred: kaji\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport random\nimport wandb\n\n# =======================\n# Best Configuration\n# =======================\nbest_config = {\n    'embedding_dim': 256,\n    'hidden_dim': 256,\n    'enc_layers': 2,\n    'dec_layers': 2,\n    'cell_type': 'LSTM',\n    'dropout': 0.5,\n    'epochs': 15,\n    'beam_size': 5,\n    'attention_type': 'concat',\n    'batch_size': 256,\n    'learning_rate': 0.001\n}\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.char2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2char = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for char in text:\n                if char not in self.char2idx:\n                    self.char2idx[char] = self.size\n                    self.idx2char[self.size] = char\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.char2idx[c] for c in text]\n\n    def decode(self, idxs):\n        return ''.join([self.idx2char[i] for i in idxs if i not in [0, 1, 2]])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab, out_vocab, is_test=False):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        if not is_test:\n            inp_vocab.build([p[0] for p in self.pairs])\n            out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        if self.is_test:\n            return torch.tensor(x), lat, dev\n        y = [self.out_vocab.char2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.char2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y), lat, dev\n\ndef collate_fn(batch):\n    if len(batch[0]) == 3:  # Test batch\n        x_batch, lat, dev = zip(*batch)\n        x_lens = [len(x) for x in x_batch]\n        x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n        return x_pad, lat, dev, torch.tensor(x_lens)\n    else:  # Train/val batch\n        x_batch, y_batch, lat, dev = zip(*batch)\n        x_lens = [len(x) for x in x_batch]\n        y_lens = [len(y) for y in y_batch]\n        x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n        y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n        return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens), lat, dev\n\n# =======================\n# Model Components\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        return outputs, hidden\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_type='general'):\n        super().__init__()\n        self.attention_type = attention_type\n        if attention_type == 'general':\n            self.attn = nn.Linear(hidden_dim, hidden_dim)\n        elif attention_type == 'concat':\n            self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n            self.v = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs, mask=None):\n        batch_size, src_len, hidden_dim = encoder_outputs.size()\n        \n        if self.attention_type == 'general':\n            energy = torch.bmm(encoder_outputs, self.attn(hidden).unsqueeze(2)).squeeze(2)\n        elif self.attention_type == 'concat':\n            hidden_expanded = hidden.unsqueeze(1).repeat(1, src_len, 1)\n            concat = torch.cat((hidden_expanded, encoder_outputs), dim=2)\n            energy = self.v(torch.tanh(self.attn(concat))).squeeze(2)\n        else:  # dot\n            energy = torch.bmm(encoder_outputs, hidden.unsqueeze(2)).squeeze(2)\n        \n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, -1e10)\n        attention_weights = F.softmax(energy, dim=1)\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n        return context, attention_weights\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout, attention_type):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim + hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.attention = Attention(hidden_dim, attention_type)\n        self.out = nn.Linear(hidden_dim * 2, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n        if isinstance(hidden, tuple):  # LSTM\n            attn_hidden = hidden[0][-1]\n        else:  # GRU/RNN\n            attn_hidden = hidden[-1]\n        \n        context, _ = self.attention(attn_hidden, encoder_outputs, mask)\n        embedded = self.embedding(input_token)\n        rnn_input = torch.cat((embedded, context), dim=1).unsqueeze(1)\n        output, hidden = self.rnn(rnn_input, hidden)\n        \n        if isinstance(hidden, tuple):\n            output_hidden = hidden[0][-1]\n        else:\n            output_hidden = hidden[-1]\n        \n        output = torch.cat((output_hidden, context), dim=1)\n        output = self.dropout(output)\n        prediction = self.out(output)\n        return prediction, hidden, None\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def create_mask(self, src_lens, max_len):\n        batch_size = len(src_lens)\n        mask = torch.zeros(batch_size, max_len, device=self.device)\n        for i, length in enumerate(src_lens):\n            mask[i, :length] = 1\n        return mask\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        src_data, src_lens = src\n        encoder_outputs, enc_hidden = self.encoder(src_data, src_lens)\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n        \n        src_len = encoder_outputs.size(1)\n        mask = self.create_mask(src_lens, src_len)\n\n        if isinstance(enc_hidden, tuple):\n            dec_hidden = enc_hidden\n        else:\n            dec_hidden = enc_hidden\n\n        input_token = trg[:, 0]\n        for t in range(1, trg_len):\n            output, dec_hidden, _ = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n            outputs[:, t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if teacher_force else top1\n            \n        return outputs\n\n    def predict(self, src, src_lens, max_len=30):\n        self.eval()\n        with torch.no_grad():\n            encoder_outputs, enc_hidden = self.encoder(src, src_lens)\n            src_len = encoder_outputs.size(1)\n            mask = self.create_mask(src_lens.tolist(), src_len)\n            \n            if isinstance(enc_hidden, tuple):\n                dec_hidden = enc_hidden\n            else:\n                dec_hidden = enc_hidden\n            \n            input_token = torch.tensor([1], device=self.device)\n            output_seq = []\n            for _ in range(max_len):\n                output, dec_hidden, _ = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n                top1 = output.argmax(1)\n                if top1.item() == 2:\n                    break\n                output_seq.append(top1.item())\n                input_token = top1\n        return output_seq\n\n# =======================\n# Training and Evaluation\n# =======================\ndef train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    \n    for batch in loader:\n        src, trg, src_lens, _, _, _ = batch\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model((src, src_lens), trg)\n        \n        # Calculate loss\n        output_dim = output.shape[-1]\n        loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        \n        # Calculate accuracy\n        pred = output.argmax(dim=2)\n        mask = (trg[:, 1:] != 0)\n        correct = ((pred[:, 1:] == trg[:, 1:]) & mask).sum().item()\n        total_correct += correct\n        total_tokens += mask.sum().item()\n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(loader)\n    accuracy = (total_correct / total_tokens) * 100 if total_tokens > 0 else 0\n    return avg_loss, accuracy\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    \n    with torch.no_grad():\n        for batch in loader:\n            src, trg, src_lens, _, _, _ = batch\n            src, trg = src.to(device), trg.to(device)\n            output = model((src, src_lens), trg, teacher_forcing_ratio=0)\n            \n            # Calculate loss\n            output_dim = output.shape[-1]\n            loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n            \n            # Calculate accuracy\n            pred = output.argmax(dim=2)\n            mask = (trg[:, 1:] != 0)\n            correct = ((pred[:, 1:] == trg[:, 1:]) & mask).sum().item()\n            total_correct += correct\n            total_tokens += mask.sum().item()\n            total_loss += loss.item()\n    \n    avg_loss = total_loss / len(loader)\n    accuracy = (total_correct / total_tokens) * 100 if total_tokens > 0 else 0\n    return avg_loss, accuracy\n\n# =======================\n# Main Execution\n# =======================\ndef main():\n    wandb.init(config=best_config, project=\"dakshina-translit-test\")\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Initialize vocabularies\n    inp_vocab = Vocab()\n    out_vocab = Vocab()\n\n    # Load datasets\n    train_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.train.tsv\", inp_vocab, out_vocab)\n    dev_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.dev.tsv\", inp_vocab, out_vocab)\n    test_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.test.tsv\", inp_vocab, out_vocab, is_test=True)\n\n    # Create data loaders\n    train_loader = DataLoader(train_data, batch_size=best_config['batch_size'], \n                            shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=best_config['batch_size'],\n                          shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    # Initialize model\n    encoder = Encoder(inp_vocab.size, best_config['embedding_dim'], \n                     best_config['hidden_dim'], best_config['enc_layers'], \n                     best_config['cell_type'], best_config['dropout'])\n    \n    decoder = Decoder(out_vocab.size, best_config['embedding_dim'],\n                     best_config['hidden_dim'], best_config['dec_layers'],\n                     best_config['cell_type'], best_config['dropout'],\n                     best_config['attention_type'])\n    \n    model = Seq2Seq(encoder, decoder, device).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=best_config['learning_rate'])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    # Training loop\n    best_val_loss = float('inf')\n    for epoch in range(best_config['epochs']):\n        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n        \n        print(f\"\\nEpoch {epoch+1}/{best_config['epochs']}\")\n        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc\n        })\n        \n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(\"Best model saved!\")\n\n    # Test evaluation\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    model.eval()\n    \n    total_correct = 0\n    total_samples = 0\n    predictions = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            src, lat, dev, src_lens = batch\n            src = src.to(device)\n            pred_ids = model.predict(src, src_lens)\n            pred_str = out_vocab.decode(pred_ids)\n            true_str = dev[0]\n            \n            predictions.append({\n                'input': lat[0],\n                'true': true_str,\n                'pred': pred_str\n            })\n            \n            if pred_str == true_str:\n                total_correct += 1\n            total_samples += 1\n\n    # Calculate accuracy\n    accuracy = 100 * total_correct / total_samples\n    print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n    wandb.log({\"test_acc\": accuracy})\n\n     # Create and log a table of predictions\n    table = wandb.Table(columns=[\"Input\", \"True\", \"Predicted\"])\n    for p in predictions[:20]:  # Log first 20 predictions\n        table.add_data(p['input'], p['true'], p['pred'])\n    \n    wandb.log({\n        \"predictions\": table,\n        \"test_accuracy\": accuracy\n    })\n\n    \n    # Save predictions\n    with open(\"test_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n        f.write(f\"Test Accuracy: {accuracy:.2f}%\\n\\n\")\n        for p in predictions[:20]:\n            f.write(f\"Input: {p['input']}\\n\")\n            f.write(f\"True: {p['true']}\\n\")\n            f.write(f\"Pred: {p['pred']}\\n\\n\")\n    \n    # Print random samples\n    print(\"\\nRandom Samples:\")\n    samples = random.sample(predictions, 30)\n    for sample in samples:\n        print(f\"Input: {sample['input']}\")\n        print(f\"True: {sample['true']}\")\n        print(f\"Pred: {sample['pred']}\\n\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:50:05.406691Z","iopub.execute_input":"2025-05-20T09:50:05.406986Z","iopub.status.idle":"2025-05-20T09:53:44.738560Z","shell.execute_reply.started":"2025-05-20T09:50:05.406968Z","shell.execute_reply":"2025-05-20T09:53:44.737956Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▆▆▇▇▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▄▃▄▄▃▃▃▃▂▁▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>test_acc</td><td>42.84762</td></tr><tr><td>test_accuracy</td><td>42.84762</td></tr><tr><td>train_acc</td><td>83.81722</td></tr><tr><td>train_loss</td><td>0.50431</td></tr><tr><td>val_acc</td><td>76.17256</td></tr><tr><td>val_loss</td><td>0.79512</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">kind-cherry-77</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/rd3xc1k5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/rd3xc1k5</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_094635-rd3xc1k5/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_095005-whk8lioe</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/whk8lioe' target=\"_blank\">lively-serenity-2</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/whk8lioe' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/whk8lioe</a>"},"metadata":{}},{"name":"stdout","text":"\nEpoch 1/15\nTrain Loss: 1.6784 | Train Acc: 48.96%\nVal Loss: 1.0198 | Val Acc: 68.02%\nBest model saved!\n\nEpoch 2/15\nTrain Loss: 0.8552 | Train Acc: 73.62%\nVal Loss: 0.8788 | Val Acc: 72.32%\nBest model saved!\n\nEpoch 3/15\nTrain Loss: 0.7413 | Train Acc: 76.96%\nVal Loss: 0.8365 | Val Acc: 74.03%\nBest model saved!\n\nEpoch 4/15\nTrain Loss: 0.6776 | Train Acc: 78.97%\nVal Loss: 0.8265 | Val Acc: 74.80%\nBest model saved!\n\nEpoch 5/15\nTrain Loss: 0.6387 | Train Acc: 80.03%\nVal Loss: 0.8255 | Val Acc: 75.39%\nBest model saved!\n\nEpoch 6/15\nTrain Loss: 0.6241 | Train Acc: 80.46%\nVal Loss: 0.8059 | Val Acc: 75.23%\nBest model saved!\n\nEpoch 7/15\nTrain Loss: 0.6027 | Train Acc: 81.01%\nVal Loss: 0.8053 | Val Acc: 75.60%\nBest model saved!\n\nEpoch 8/15\nTrain Loss: 0.5784 | Train Acc: 81.79%\nVal Loss: 0.8597 | Val Acc: 74.63%\n\nEpoch 9/15\nTrain Loss: 0.5725 | Train Acc: 81.94%\nVal Loss: 0.7686 | Val Acc: 75.84%\nBest model saved!\n\nEpoch 10/15\nTrain Loss: 0.5498 | Train Acc: 82.56%\nVal Loss: 0.7721 | Val Acc: 75.92%\n\nEpoch 11/15\nTrain Loss: 0.5411 | Train Acc: 82.77%\nVal Loss: 0.8280 | Val Acc: 75.80%\n\nEpoch 12/15\nTrain Loss: 0.5314 | Train Acc: 83.12%\nVal Loss: 0.7815 | Val Acc: 76.07%\n\nEpoch 13/15\nTrain Loss: 0.5362 | Train Acc: 82.72%\nVal Loss: 0.7728 | Val Acc: 76.11%\n\nEpoch 14/15\nTrain Loss: 0.5185 | Train Acc: 83.35%\nVal Loss: 0.7741 | Val Acc: 76.25%\n\nEpoch 15/15\nTrain Loss: 0.5129 | Train Acc: 83.48%\nVal Loss: 0.7765 | Val Acc: 76.19%\n\nTest Accuracy: 41.51%\n\nRandom Samples:\nInput: रिचुअल\nTrue: richual\nPred: richual\n\nInput: उसमें\nTrue: usmein\nPred: usmen\n\nInput: एड्डी\nTrue: addi\nPred: addi\n\nInput: दुभाषियों\nTrue: dubhashiyon\nPred: dubhashiyon\n\nInput: बजना\nTrue: bajana\nPred: bajna\n\nInput: सवाल\nTrue: sawal\nPred: sawal\n\nInput: पिंटा\nTrue: pinta\nPred: pinta\n\nInput: आपदाएं\nTrue: aapdaein\nPred: aapdaeen\n\nInput: रूट्स\nTrue: routes\nPred: roots\n\nInput: गोचर\nTrue: gochar\nPred: gochar\n\nInput: रोपने\nTrue: ropaney\nPred: ropne\n\nInput: बनिया\nTrue: baniya\nPred: baniya\n\nInput: भाते\nTrue: bhate\nPred: bhate\n\nInput: बेवकूफाना\nTrue: bevkufana\nPred: bevkuofana\n\nInput: रेशे\nTrue: reshey\nPred: reshe\n\nInput: अरदास\nTrue: ardas\nPred: ardaas\n\nInput: चौपाल\nTrue: chaupal\nPred: chaupal\n\nInput: आगमन\nTrue: aagman\nPred: aagaman\n\nInput: डायरीज\nTrue: diaries\nPred: dirres\n\nInput: चर्चा\nTrue: churcha\nPred: charcha\n\nInput: थिरके\nTrue: thirke\nPred: thirke\n\nInput: धुरिया\nTrue: dhuriya\nPred: dhuriya\n\nInput: वादक\nTrue: vaadak\nPred: vadak\n\nInput: मालिश\nTrue: maalish\nPred: malish\n\nInput: जड़ित\nTrue: jadeite\nPred: jadit\n\nInput: जनहानि\nTrue: janahaani\nPred: janhaani\n\nInput: धारक\nTrue: dhaarak\nPred: dhaarak\n\nInput: थैले\nTrue: thaile\nPred: thaile\n\nInput: अमित\nTrue: amit\nPred: amit\n\nInput: ऐल\nTrue: el\nPred: all\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Add these lines at the top of your imports\nimport matplotlib\nfrom matplotlib import font_manager\n\n# Install Indic fonts (run once)\n!sudo apt-get install fonts-indic -y\n!wget \"https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSansDevanagari/NotoSansDevanagari-Regular.ttf\"\n\n# Add font to matplotlib\nfont_manager.fontManager.addfont('NotoSansDevanagari-Regular.ttf')\nmatplotlib.rc('font', family='NotoSans Devanagari')\n\n# Then proceed with your existing visualization code","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:13:56.550955Z","iopub.execute_input":"2025-05-20T11:13:56.551345Z","iopub.status.idle":"2025-05-20T11:14:12.470787Z","shell.execute_reply.started":"2025-05-20T11:13:56.551323Z","shell.execute_reply":"2025-05-20T11:14:12.469958Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  fonts-beng fonts-beng-extra fonts-deva fonts-deva-extra fonts-gargi\n  fonts-gubbi fonts-gujr fonts-gujr-extra fonts-guru fonts-guru-extra\n  fonts-kalapi fonts-knda fonts-lohit-beng-assamese fonts-lohit-beng-bengali\n  fonts-lohit-deva fonts-lohit-gujr fonts-lohit-guru fonts-lohit-knda\n  fonts-lohit-mlym fonts-lohit-orya fonts-lohit-taml\n  fonts-lohit-taml-classical fonts-lohit-telu fonts-mlym fonts-nakula\n  fonts-navilu fonts-orya fonts-orya-extra fonts-pagul fonts-sahadeva\n  fonts-samyak-deva fonts-samyak-gujr fonts-samyak-mlym fonts-samyak-taml\n  fonts-sarai fonts-smc fonts-smc-anjalioldlipi fonts-smc-chilanka\n  fonts-smc-dyuthi fonts-smc-gayathri fonts-smc-karumbi fonts-smc-keraleeyam\n  fonts-smc-manjari fonts-smc-meera fonts-smc-rachana\n  fonts-smc-raghumalayalamsans fonts-smc-suruma fonts-smc-uroob fonts-taml\n  fonts-telu fonts-telu-extra fonts-teluguvijayam fonts-yrsa-rasa\nThe following NEW packages will be installed:\n  fonts-beng fonts-beng-extra fonts-deva fonts-deva-extra fonts-gargi\n  fonts-gubbi fonts-gujr fonts-gujr-extra fonts-guru fonts-guru-extra\n  fonts-indic fonts-kalapi fonts-knda fonts-lohit-beng-assamese\n  fonts-lohit-beng-bengali fonts-lohit-deva fonts-lohit-gujr fonts-lohit-guru\n  fonts-lohit-knda fonts-lohit-mlym fonts-lohit-orya fonts-lohit-taml\n  fonts-lohit-taml-classical fonts-lohit-telu fonts-mlym fonts-nakula\n  fonts-navilu fonts-orya fonts-orya-extra fonts-pagul fonts-sahadeva\n  fonts-samyak-deva fonts-samyak-gujr fonts-samyak-mlym fonts-samyak-taml\n  fonts-sarai fonts-smc fonts-smc-anjalioldlipi fonts-smc-chilanka\n  fonts-smc-dyuthi fonts-smc-gayathri fonts-smc-karumbi fonts-smc-keraleeyam\n  fonts-smc-manjari fonts-smc-meera fonts-smc-rachana\n  fonts-smc-raghumalayalamsans fonts-smc-suruma fonts-smc-uroob fonts-taml\n  fonts-telu fonts-telu-extra fonts-teluguvijayam fonts-yrsa-rasa\n0 upgraded, 54 newly installed, 0 to remove and 87 not upgraded.\nNeed to get 7,482 kB of archives.\nAfter this operation, 28.8 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-deva-extra all 3.0-5 [577 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-kalapi all 1.0-4 [116 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc-anjalioldlipi all 7.1.2-2 [98.6 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc-chilanka all 1.540-1 [186 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc-dyuthi all 3.0.2-2 [112 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc-gayathri all 1.110-2-1 [335 kB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc-karumbi all 1.1.2-2 [213 kB]\nGet:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc-keraleeyam all 3.0.2-2 [98.7 kB]\nGet:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc-manjari all 2.000-3 [305 kB]\nGet:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc-meera all 7.0.3-1 [142 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc-rachana all 7.0.2-1build1 [270 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc-raghumalayalamsans all 2.2.1-1 [33.6 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc-suruma all 3.2.3-1 [81.0 kB]\nGet:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc-uroob all 2.0.2-1 [75.9 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-smc all 1:7.2 [4,412 B]\nGet:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-beng-extra all 3.2.1-1 [323 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lohit-beng-assamese all 2.91.5-2 [66.9 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lohit-beng-bengali all 2.91.5-2 [66.9 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-beng all 2:1.3 [3,032 B]\nGet:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-gargi all 2.0-5 [42.4 kB]\nGet:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lohit-deva all 2.95.4-4 [78.9 kB]\nGet:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-nakula all 1.0-4 [136 kB]\nGet:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-sahadeva all 1.0-5 [137 kB]\nGet:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-samyak-deva all 1.2.2-5build1 [53.9 kB]\nGet:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-deva all 2:1.3 [3,168 B]\nGet:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-gubbi all 1.3-5build1 [45.6 kB]\nGet:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-gujr-extra all 1.0.1-1 [150 kB]\nGet:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lohit-gujr all 2.92.4-4 [35.2 kB]\nGet:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-samyak-gujr all 1.2.2-5build1 [43.1 kB]\nGet:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-yrsa-rasa all 2.005-1 [809 kB]\nGet:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-gujr all 2:1.4 [3,108 B]\nGet:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-guru-extra all 2.0-5 [44.4 kB]\nGet:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lohit-guru all 2.91.2-2build1 [21.0 kB]\nGet:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-guru all 2:1.3 [3,036 B]\nGet:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lohit-knda all 2.5.4-3 [48.4 kB]\nGet:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-navilu all 1.2-3 [63.0 kB]\nGet:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-knda all 2:1.3 [3,488 B]\nGet:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lohit-mlym all 2.92.2-2 [32.9 kB]\nGet:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-samyak-mlym all 1.2.2-5build1 [26.5 kB]\nGet:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-mlym all 2:1.3 [3,076 B]\nGet:41 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lohit-orya all 2.91.2-2 [50.0 kB]\nGet:42 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-orya-extra all 2.0-6 [59.2 kB]\nGet:43 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-orya all 2:1.3 [3,068 B]\nGet:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-pagul all 1.0-8 [65.7 kB]\nGet:45 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lohit-taml all 2.91.3-2 [29.7 kB]\nGet:46 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lohit-taml-classical all 2.5.4-2 [32.6 kB]\nGet:47 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-samyak-taml all 1.2.2-5build1 [22.8 kB]\nGet:48 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-taml all 2:1.4 [3,216 B]\nGet:49 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lohit-telu all 2.5.5-2build1 [114 kB]\nGet:50 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-telu-extra all 2.0-5 [182 kB]\nGet:51 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-teluguvijayam all 2.1-1 [1,973 kB]\nGet:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-telu all 2:1.3 [3,080 B]\nGet:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-indic all 2:1.4 [3,456 B]\nGet:54 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-sarai all 1.0-3 [47.7 kB]\nFetched 7,482 kB in 2s (4,217 kB/s)     \ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 54.)\ndebconf: falling back to frontend: Readline\nExtracting templates from packages: 100%\nSelecting previously unselected package fonts-deva-extra.\n(Reading database ... 129184 files and directories currently installed.)\nPreparing to unpack .../00-fonts-deva-extra_3.0-5_all.deb ...\nUnpacking fonts-deva-extra (3.0-5) ...\nSelecting previously unselected package fonts-kalapi.\nPreparing to unpack .../01-fonts-kalapi_1.0-4_all.deb ...\nUnpacking fonts-kalapi (1.0-4) ...\nSelecting previously unselected package fonts-smc-anjalioldlipi.\nPreparing to unpack .../02-fonts-smc-anjalioldlipi_7.1.2-2_all.deb ...\nUnpacking fonts-smc-anjalioldlipi (7.1.2-2) ...\nSelecting previously unselected package fonts-smc-chilanka.\nPreparing to unpack .../03-fonts-smc-chilanka_1.540-1_all.deb ...\nUnpacking fonts-smc-chilanka (1.540-1) ...\nSelecting previously unselected package fonts-smc-dyuthi.\nPreparing to unpack .../04-fonts-smc-dyuthi_3.0.2-2_all.deb ...\nUnpacking fonts-smc-dyuthi (3.0.2-2) ...\nSelecting previously unselected package fonts-smc-gayathri.\nPreparing to unpack .../05-fonts-smc-gayathri_1.110-2-1_all.deb ...\nUnpacking fonts-smc-gayathri (1.110-2-1) ...\nSelecting previously unselected package fonts-smc-karumbi.\nPreparing to unpack .../06-fonts-smc-karumbi_1.1.2-2_all.deb ...\nUnpacking fonts-smc-karumbi (1.1.2-2) ...\nSelecting previously unselected package fonts-smc-keraleeyam.\nPreparing to unpack .../07-fonts-smc-keraleeyam_3.0.2-2_all.deb ...\nUnpacking fonts-smc-keraleeyam (3.0.2-2) ...\nSelecting previously unselected package fonts-smc-manjari.\nPreparing to unpack .../08-fonts-smc-manjari_2.000-3_all.deb ...\nUnpacking fonts-smc-manjari (2.000-3) ...\nSelecting previously unselected package fonts-smc-meera.\nPreparing to unpack .../09-fonts-smc-meera_7.0.3-1_all.deb ...\nUnpacking fonts-smc-meera (7.0.3-1) ...\nSelecting previously unselected package fonts-smc-rachana.\nPreparing to unpack .../10-fonts-smc-rachana_7.0.2-1build1_all.deb ...\nUnpacking fonts-smc-rachana (7.0.2-1build1) ...\nSelecting previously unselected package fonts-smc-raghumalayalamsans.\nPreparing to unpack .../11-fonts-smc-raghumalayalamsans_2.2.1-1_all.deb ...\nUnpacking fonts-smc-raghumalayalamsans (2.2.1-1) ...\nSelecting previously unselected package fonts-smc-suruma.\nPreparing to unpack .../12-fonts-smc-suruma_3.2.3-1_all.deb ...\nUnpacking fonts-smc-suruma (3.2.3-1) ...\nSelecting previously unselected package fonts-smc-uroob.\nPreparing to unpack .../13-fonts-smc-uroob_2.0.2-1_all.deb ...\nUnpacking fonts-smc-uroob (2.0.2-1) ...\nSelecting previously unselected package fonts-smc.\nPreparing to unpack .../14-fonts-smc_1%3a7.2_all.deb ...\nUnpacking fonts-smc (1:7.2) ...\nSelecting previously unselected package fonts-beng-extra.\nPreparing to unpack .../15-fonts-beng-extra_3.2.1-1_all.deb ...\nUnpacking fonts-beng-extra (3.2.1-1) ...\nSelecting previously unselected package fonts-lohit-beng-assamese.\nPreparing to unpack .../16-fonts-lohit-beng-assamese_2.91.5-2_all.deb ...\nUnpacking fonts-lohit-beng-assamese (2.91.5-2) ...\nSelecting previously unselected package fonts-lohit-beng-bengali.\nPreparing to unpack .../17-fonts-lohit-beng-bengali_2.91.5-2_all.deb ...\nUnpacking fonts-lohit-beng-bengali (2.91.5-2) ...\nSelecting previously unselected package fonts-beng.\nPreparing to unpack .../18-fonts-beng_2%3a1.3_all.deb ...\nUnpacking fonts-beng (2:1.3) ...\nSelecting previously unselected package fonts-gargi.\nPreparing to unpack .../19-fonts-gargi_2.0-5_all.deb ...\nUnpacking fonts-gargi (2.0-5) ...\nSelecting previously unselected package fonts-lohit-deva.\nPreparing to unpack .../20-fonts-lohit-deva_2.95.4-4_all.deb ...\nUnpacking fonts-lohit-deva (2.95.4-4) ...\nSelecting previously unselected package fonts-nakula.\nPreparing to unpack .../21-fonts-nakula_1.0-4_all.deb ...\nUnpacking fonts-nakula (1.0-4) ...\nSelecting previously unselected package fonts-sahadeva.\nPreparing to unpack .../22-fonts-sahadeva_1.0-5_all.deb ...\nUnpacking fonts-sahadeva (1.0-5) ...\nSelecting previously unselected package fonts-samyak-deva.\nPreparing to unpack .../23-fonts-samyak-deva_1.2.2-5build1_all.deb ...\nUnpacking fonts-samyak-deva (1.2.2-5build1) ...\nSelecting previously unselected package fonts-deva.\nPreparing to unpack .../24-fonts-deva_2%3a1.3_all.deb ...\nUnpacking fonts-deva (2:1.3) ...\nSelecting previously unselected package fonts-gubbi.\nPreparing to unpack .../25-fonts-gubbi_1.3-5build1_all.deb ...\nUnpacking fonts-gubbi (1.3-5build1) ...\nSelecting previously unselected package fonts-gujr-extra.\nPreparing to unpack .../26-fonts-gujr-extra_1.0.1-1_all.deb ...\nUnpacking fonts-gujr-extra (1.0.1-1) ...\nSelecting previously unselected package fonts-lohit-gujr.\nPreparing to unpack .../27-fonts-lohit-gujr_2.92.4-4_all.deb ...\nUnpacking fonts-lohit-gujr (2.92.4-4) ...\nSelecting previously unselected package fonts-samyak-gujr.\nPreparing to unpack .../28-fonts-samyak-gujr_1.2.2-5build1_all.deb ...\nUnpacking fonts-samyak-gujr (1.2.2-5build1) ...\nSelecting previously unselected package fonts-yrsa-rasa.\nPreparing to unpack .../29-fonts-yrsa-rasa_2.005-1_all.deb ...\nUnpacking fonts-yrsa-rasa (2.005-1) ...\nSelecting previously unselected package fonts-gujr.\nPreparing to unpack .../30-fonts-gujr_2%3a1.4_all.deb ...\nUnpacking fonts-gujr (2:1.4) ...\nSelecting previously unselected package fonts-guru-extra.\nPreparing to unpack .../31-fonts-guru-extra_2.0-5_all.deb ...\nUnpacking fonts-guru-extra (2.0-5) ...\nSelecting previously unselected package fonts-lohit-guru.\nPreparing to unpack .../32-fonts-lohit-guru_2.91.2-2build1_all.deb ...\nUnpacking fonts-lohit-guru (2.91.2-2build1) ...\nSelecting previously unselected package fonts-guru.\nPreparing to unpack .../33-fonts-guru_2%3a1.3_all.deb ...\nUnpacking fonts-guru (2:1.3) ...\nSelecting previously unselected package fonts-lohit-knda.\nPreparing to unpack .../34-fonts-lohit-knda_2.5.4-3_all.deb ...\nUnpacking fonts-lohit-knda (2.5.4-3) ...\nSelecting previously unselected package fonts-navilu.\nPreparing to unpack .../35-fonts-navilu_1.2-3_all.deb ...\nUnpacking fonts-navilu (1.2-3) ...\nSelecting previously unselected package fonts-knda.\nPreparing to unpack .../36-fonts-knda_2%3a1.3_all.deb ...\nUnpacking fonts-knda (2:1.3) ...\nSelecting previously unselected package fonts-lohit-mlym.\nPreparing to unpack .../37-fonts-lohit-mlym_2.92.2-2_all.deb ...\nUnpacking fonts-lohit-mlym (2.92.2-2) ...\nSelecting previously unselected package fonts-samyak-mlym.\nPreparing to unpack .../38-fonts-samyak-mlym_1.2.2-5build1_all.deb ...\nUnpacking fonts-samyak-mlym (1.2.2-5build1) ...\nSelecting previously unselected package fonts-mlym.\nPreparing to unpack .../39-fonts-mlym_2%3a1.3_all.deb ...\nUnpacking fonts-mlym (2:1.3) ...\nSelecting previously unselected package fonts-lohit-orya.\nPreparing to unpack .../40-fonts-lohit-orya_2.91.2-2_all.deb ...\nUnpacking fonts-lohit-orya (2.91.2-2) ...\nSelecting previously unselected package fonts-orya-extra.\nPreparing to unpack .../41-fonts-orya-extra_2.0-6_all.deb ...\nUnpacking fonts-orya-extra (2.0-6) ...\nSelecting previously unselected package fonts-orya.\nPreparing to unpack .../42-fonts-orya_2%3a1.3_all.deb ...\nUnpacking fonts-orya (2:1.3) ...\nSelecting previously unselected package fonts-pagul.\nPreparing to unpack .../43-fonts-pagul_1.0-8_all.deb ...\nUnpacking fonts-pagul (1.0-8) ...\nSelecting previously unselected package fonts-lohit-taml.\nPreparing to unpack .../44-fonts-lohit-taml_2.91.3-2_all.deb ...\nUnpacking fonts-lohit-taml (2.91.3-2) ...\nSelecting previously unselected package fonts-lohit-taml-classical.\nPreparing to unpack .../45-fonts-lohit-taml-classical_2.5.4-2_all.deb ...\nUnpacking fonts-lohit-taml-classical (2.5.4-2) ...\nSelecting previously unselected package fonts-samyak-taml.\nPreparing to unpack .../46-fonts-samyak-taml_1.2.2-5build1_all.deb ...\nUnpacking fonts-samyak-taml (1.2.2-5build1) ...\nSelecting previously unselected package fonts-taml.\nPreparing to unpack .../47-fonts-taml_2%3a1.4_all.deb ...\nUnpacking fonts-taml (2:1.4) ...\nSelecting previously unselected package fonts-lohit-telu.\nPreparing to unpack .../48-fonts-lohit-telu_2.5.5-2build1_all.deb ...\nUnpacking fonts-lohit-telu (2.5.5-2build1) ...\nSelecting previously unselected package fonts-telu-extra.\nPreparing to unpack .../49-fonts-telu-extra_2.0-5_all.deb ...\nUnpacking fonts-telu-extra (2.0-5) ...\nSelecting previously unselected package fonts-teluguvijayam.\nPreparing to unpack .../50-fonts-teluguvijayam_2.1-1_all.deb ...\nUnpacking fonts-teluguvijayam (2.1-1) ...\nSelecting previously unselected package fonts-telu.\nPreparing to unpack .../51-fonts-telu_2%3a1.3_all.deb ...\nUnpacking fonts-telu (2:1.3) ...\nSelecting previously unselected package fonts-indic.\nPreparing to unpack .../52-fonts-indic_2%3a1.4_all.deb ...\nUnpacking fonts-indic (2:1.4) ...\nSelecting previously unselected package fonts-sarai.\nPreparing to unpack .../53-fonts-sarai_1.0-3_all.deb ...\nUnpacking fonts-sarai (1.0-3) ...\nSetting up fonts-samyak-taml (1.2.2-5build1) ...\nSetting up fonts-lohit-guru (2.91.2-2build1) ...\nSetting up fonts-sahadeva (1.0-5) ...\nSetting up fonts-smc-rachana (7.0.2-1build1) ...\nSetting up fonts-lohit-orya (2.91.2-2) ...\nSetting up fonts-smc-meera (7.0.3-1) ...\nSetting up fonts-pagul (1.0-8) ...\nSetting up fonts-lohit-taml (2.91.3-2) ...\nSetting up fonts-lohit-deva (2.95.4-4) ...\nSetting up fonts-smc-keraleeyam (3.0.2-2) ...\nSetting up fonts-gujr-extra (1.0.1-1) ...\nSetting up fonts-lohit-beng-bengali (2.91.5-2) ...\nSetting up fonts-beng-extra (3.2.1-1) ...\nSetting up fonts-smc-anjalioldlipi (7.1.2-2) ...\nSetting up fonts-smc-manjari (2.000-3) ...\nSetting up fonts-lohit-mlym (2.92.2-2) ...\nSetting up fonts-lohit-knda (2.5.4-3) ...\nSetting up fonts-samyak-gujr (1.2.2-5build1) ...\nSetting up fonts-nakula (1.0-4) ...\nSetting up fonts-samyak-mlym (1.2.2-5build1) ...\nSetting up fonts-lohit-beng-assamese (2.91.5-2) ...\nSetting up fonts-smc-gayathri (1.110-2-1) ...\nSetting up fonts-navilu (1.2-3) ...\nSetting up fonts-smc-raghumalayalamsans (2.2.1-1) ...\nSetting up fonts-lohit-taml-classical (2.5.4-2) ...\nSetting up fonts-smc-chilanka (1.540-1) ...\nSetting up fonts-lohit-telu (2.5.5-2build1) ...\nSetting up fonts-smc-uroob (2.0.2-1) ...\nSetting up fonts-gargi (2.0-5) ...\nSetting up fonts-smc-karumbi (1.1.2-2) ...\nSetting up fonts-smc-dyuthi (3.0.2-2) ...\nSetting up fonts-kalapi (1.0-4) ...\nSetting up fonts-gubbi (1.3-5build1) ...\nSetting up fonts-deva-extra (3.0-5) ...\nSetting up fonts-samyak-deva (1.2.2-5build1) ...\nSetting up fonts-beng (2:1.3) ...\nSetting up fonts-smc-suruma (3.2.3-1) ...\nSetting up fonts-guru-extra (2.0-5) ...\nSetting up fonts-knda (2:1.3) ...\nSetting up fonts-orya-extra (2.0-6) ...\nSetting up fonts-teluguvijayam (2.1-1) ...\nSetting up fonts-lohit-gujr (2.92.4-4) ...\nSetting up fonts-sarai (1.0-3) ...\nSetting up fonts-yrsa-rasa (2.005-1) ...\nSetting up fonts-telu-extra (2.0-5) ...\nSetting up fonts-smc (1:7.2) ...\nSetting up fonts-orya (2:1.3) ...\nSetting up fonts-telu (2:1.3) ...\nSetting up fonts-guru (2:1.3) ...\nSetting up fonts-taml (2:1.4) ...\nSetting up fonts-gujr (2:1.4) ...\nSetting up fonts-deva (2:1.3) ...\nSetting up fonts-mlym (2:1.3) ...\nSetting up fonts-indic (2:1.4) ...\nProcessing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n--2025-05-20 11:14:11--  https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSansDevanagari/NotoSansDevanagari-Regular.ttf\nResolving github.com (github.com)... 140.82.113.3\nConnecting to github.com (github.com)|140.82.113.3|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/notofonts/noto-fonts/raw/main/hinted/ttf/NotoSansDevanagari/NotoSansDevanagari-Regular.ttf [following]\n--2025-05-20 11:14:12--  https://github.com/notofonts/noto-fonts/raw/main/hinted/ttf/NotoSansDevanagari/NotoSansDevanagari-Regular.ttf\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/notofonts/noto-fonts/main/hinted/ttf/NotoSansDevanagari/NotoSansDevanagari-Regular.ttf [following]\n--2025-05-20 11:14:12--  https://raw.githubusercontent.com/notofonts/noto-fonts/main/hinted/ttf/NotoSansDevanagari/NotoSansDevanagari-Regular.ttf\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 219212 (214K) [application/octet-stream]\nSaving to: ‘NotoSansDevanagari-Regular.ttf’\n\nNotoSansDevanagari- 100%[===================>] 214.07K  --.-KB/s    in 0.02s   \n\n2025-05-20 11:14:12 (8.71 MB/s) - ‘NotoSansDevanagari-Regular.ttf’ saved [219212/219212]\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# =======================\n# 0. Setup & Font Configuration\n# =======================\n# Run this cell FIRST, then RESTART RUNTIME before proceeding\n\n!sudo apt-get install fonts-indic -y\n!wget -q \"https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSansDevanagari/NotoSansDevanagari-Regular.ttf\" -O NotoSansDevanagari.ttf\n\nimport matplotlib\nimport matplotlib.font_manager as fm\nfrom matplotlib import rcParams\n\n# Add custom font\nfont_path = \"NotoSansDevanagari.ttf\"\nfont_prop = fm.FontProperties(fname=font_path)\nfm.fontManager.addfont(font_path)\nrcParams['font.family'] = font_prop.get_name()\n\n# Verify font installation\nprint(\"\\nAvailable fonts:\")\nprint([f.name for f in fm.fontManager.ttflist if 'NotoSansDevanagari' in f.name])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:51:54.029596Z","iopub.execute_input":"2025-05-20T11:51:54.030340Z","iopub.status.idle":"2025-05-20T11:51:56.981899Z","shell.execute_reply.started":"2025-05-20T11:51:54.030309Z","shell.execute_reply":"2025-05-20T11:51:56.980732Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nfonts-indic is already the newest version (2:1.4).\n0 upgraded, 0 newly installed, 0 to remove and 87 not upgraded.\n\nAvailable fonts:\n[]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport random\nimport wandb\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# =======================\n# Best Configuration\n# =======================\nbest_config = {\n    'embedding_dim': 256,\n    'hidden_dim': 256,\n    'enc_layers': 2,\n    'dec_layers': 2,\n    'cell_type': 'LSTM',\n    'dropout': 0.5,\n    'epochs': 1,\n    'beam_size': 5,\n    'attention_type': 'concat',\n    'batch_size': 256,\n    'learning_rate': 0.001\n}\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.char2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2char = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for char in text:\n                if char not in self.char2idx:\n                    self.char2idx[char] = self.size\n                    self.idx2char[self.size] = char\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.char2idx[c] for c in text]\n\n    def decode(self, idxs):\n        return ''.join([self.idx2char[i] for i in idxs if i not in [0, 1, 2]])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab, out_vocab, is_test=False):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        if not is_test:\n            inp_vocab.build([p[0] for p in self.pairs])\n            out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        if self.is_test:\n            return torch.tensor(x), lat, dev\n        y = [self.out_vocab.char2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.char2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y), lat, dev\n\ndef collate_fn(batch):\n    if len(batch[0]) == 3:  # Test batch\n        x_batch, lat, dev = zip(*batch)\n        x_lens = [len(x) for x in x_batch]\n        x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n        return x_pad, lat, dev, torch.tensor(x_lens)\n    else:  # Train/val batch\n        x_batch, y_batch, lat, dev = zip(*batch)\n        x_lens = [len(x) for x in x_batch]\n        y_lens = [len(y) for y in y_batch]\n        x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n        y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n        return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens), lat, dev\n\n# =======================\n# Model Components\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        return outputs, hidden\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_type='general'):\n        super().__init__()\n        self.attention_type = attention_type\n        if attention_type == 'general':\n            self.attn = nn.Linear(hidden_dim, hidden_dim)\n        elif attention_type == 'concat':\n            self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n            self.v = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs, mask=None):\n        batch_size, src_len, hidden_dim = encoder_outputs.size()\n        \n        if self.attention_type == 'general':\n            energy = torch.bmm(encoder_outputs, self.attn(hidden).unsqueeze(2)).squeeze(2)\n        elif self.attention_type == 'concat':\n            hidden_expanded = hidden.unsqueeze(1).repeat(1, src_len, 1)\n            concat = torch.cat((hidden_expanded, encoder_outputs), dim=2)\n            energy = self.v(torch.tanh(self.attn(concat))).squeeze(2)\n        else:  # dot\n            energy = torch.bmm(encoder_outputs, hidden.unsqueeze(2)).squeeze(2)\n        \n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, -1e10)\n        attention_weights = F.softmax(energy, dim=1)\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n\n        return context, attention_weights\n\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout, attention_type):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim + hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.attention = Attention(hidden_dim, attention_type)\n        self.out = nn.Linear(hidden_dim * 2, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n        if isinstance(hidden, tuple):  # LSTM\n            attn_hidden = hidden[0][-1]\n        else:  # GRU/RNN\n            attn_hidden = hidden[-1]\n        \n        context, attention_weights = self.attention(attn_hidden, encoder_outputs, mask)\n        \n        embedded = self.embedding(input_token)\n        rnn_input = torch.cat((embedded, context), dim=1).unsqueeze(1)\n        output, hidden = self.rnn(rnn_input, hidden)\n        \n        if isinstance(hidden, tuple):\n            output_hidden = hidden[0][-1]\n        else:\n            output_hidden = hidden[-1]\n        \n        output = torch.cat((output_hidden, context), dim=1)\n        output = self.dropout(output)\n        prediction = self.out(output)\n        \n        return prediction, hidden, attention_weights\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def create_mask(self, src_lens, max_len):\n        batch_size = len(src_lens)\n        mask = torch.zeros(batch_size, max_len, device=self.device)\n        for i, length in enumerate(src_lens):\n            mask[i, :length] = 1\n        return mask\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        src_data, src_lens = src\n        encoder_outputs, enc_hidden = self.encoder(src_data, src_lens)\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n        \n        src_len = encoder_outputs.size(1)\n        mask = self.create_mask(src_lens, src_len)\n\n        if isinstance(enc_hidden, tuple):\n            dec_hidden = enc_hidden\n        else:\n            dec_hidden = enc_hidden\n\n        input_token = trg[:, 0]\n        for t in range(1, trg_len):\n            output, dec_hidden, _ = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n            outputs[:, t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if teacher_force else top1\n            \n        return outputs\n\n    def predict(self, src, src_lens, max_len=30):\n        self.eval()\n        with torch.no_grad():\n            encoder_outputs, enc_hidden = self.encoder(src, src_lens)\n            src_len = encoder_outputs.size(1)\n            mask = self.create_mask(src_lens.tolist(), src_len)\n            \n            if isinstance(enc_hidden, tuple):\n                dec_hidden = enc_hidden\n            else:\n                dec_hidden = enc_hidden\n            \n            input_token = torch.tensor([1], device=self.device)\n            output_seq = []\n            attentions = []\n            for _ in range(max_len):\n                output, dec_hidden, attn_weights = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n                top1 = output.argmax(1)\n                attentions.append(attn_weights.squeeze(0).cpu().numpy())\n                if top1.item() == 2:  # <eos> token\n                    break\n                output_seq.append(top1.item())\n                input_token = top1\n            return output_seq, attentions\n\n\ndef train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    \n    for batch in loader:\n        src, trg, src_lens, _, _, _ = batch\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model((src, src_lens), trg)\n        \n        # Calculate loss\n        output_dim = output.shape[-1]\n        loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        \n        # Calculate accuracy\n        pred = output.argmax(dim=2)\n        mask = (trg[:, 1:] != 0)\n        correct = ((pred[:, 1:] == trg[:, 1:]) & mask).sum().item()\n        total_correct += correct\n        total_tokens += mask.sum().item()\n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(loader)\n    accuracy = (total_correct / total_tokens) * 100 if total_tokens > 0 else 0\n    return avg_loss, accuracy\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    \n    with torch.no_grad():\n        for batch in loader:\n            src, trg, src_lens, _, _, _ = batch\n            src, trg = src.to(device), trg.to(device)\n            output = model((src, src_lens), trg, teacher_forcing_ratio=0)\n            \n            # Calculate loss\n            output_dim = output.shape[-1]\n            loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n            \n            # Calculate accuracy\n            pred = output.argmax(dim=2)\n            mask = (trg[:, 1:] != 0)\n            correct = ((pred[:, 1:] == trg[:, 1:]) & mask).sum().item()\n            total_correct += correct\n            total_tokens += mask.sum().item()\n            total_loss += loss.item()\n    \n    avg_loss = total_loss / len(loader)\n    accuracy = (total_correct / total_tokens) * 100 if total_tokens > 0 else 0\n    return avg_loss, accuracy\n\ndef visualize_attention_heatmaps(predictions, num_samples=9):\n    \"\"\"\n    Visualize attention heatmaps for the specified samples\n    \n    Args:\n        predictions: List of prediction dictionaries\n        num_samples: Number of samples to visualize\n    \"\"\"\n    # Configure matplotlib to use a font that supports Devanagari\n    import matplotlib.font_manager as fm\n    \n    # Try to find a font that supports Devanagari - fallback gracefully if not found\n    # Common fonts that might support Devanagari\n    devanagari_fonts = [\n        'Nirmala UI', 'Mangal', 'Arial Unicode MS', 'Noto Sans Devanagari',\n        'Lohit Devanagari', 'Gargi', 'Sarai', 'FreeSans'\n    ]\n    \n    # Find a suitable font from the system\n    font_found = False\n    for font_name in devanagari_fonts:\n        font_list = [f for f in fm.findSystemFonts() if font_name.lower() in f.lower()]\n        if font_list:\n            plt.rcParams['font.family'] = 'sans-serif'\n            plt.rcParams['font.sans-serif'] = [font_name] + plt.rcParams['font.sans-serif']\n            font_found = True\n            print(f\"Using font: {font_name} for Devanagari rendering\")\n            break\n    \n    if not font_found:\n        print(\"Warning: No suitable Devanagari font found. Using ASCII labels instead.\")\n    \n    # Ensure we don't try to visualize more samples than we have\n    num_samples = min(num_samples, len(predictions))\n    \n    # Calculate the grid dimensions\n    grid_size = int(np.ceil(np.sqrt(num_samples)))\n    \n    # Create grid of heatmaps\n    fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n    \n    # Flatten the axes array for easier indexing\n    if grid_size > 1:\n        axes = axes.flatten()\n    else:\n        axes = [axes]  # Make it iterable for a single subplot\n    \n    for i in range(num_samples):\n        sample = predictions[i]\n        ax = axes[i]\n        \n        # Get attention data\n        attn = np.array(sample['attentions'])\n        input_chars = sample['input_tokens']\n        \n        # Handle Devanagari characters - use ASCII representation if font not found\n        if not font_found:\n            # Use positional indices for output characters to avoid font issues\n            output_chars = [f\"Out_{idx}\" for idx in range(len(sample['output_tokens']))]\n        else:\n            output_chars = sample['output_tokens']\n        \n        # Create heatmap\n        sns.heatmap(attn, ax=ax, xticklabels=input_chars, yticklabels=output_chars,\n                   cmap=\"YlGnBu\", annot=False)\n        \n        # Set title and labels - use ASCII for predicted output if font issues\n        if not font_found:\n            ax.set_title(f\"Input: {sample['input']}\\nTrue/Pred: (see legend)\", fontsize=8)\n        else:\n            ax.set_title(f\"Input: {sample['input']}\\nTrue: {sample['true']}\\nPred: {sample['pred']}\", fontsize=8)\n        \n        ax.set_xlabel(\"Input Characters\")\n        ax.set_ylabel(\"Output Characters\")\n        \n        # Rotate x-axis labels for better readability\n        ax.set_xticklabels(input_chars, rotation=45, ha='right')\n    \n    # Hide any unused subplots\n    for j in range(num_samples, grid_size * grid_size):\n        if j < len(axes):\n            axes[j].axis('off')\n    \n    plt.tight_layout()\n    return fig\n\ndef main():\n    wandb.init(config=best_config, project=\"dakshina-translit-test\")\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Initialize vocabularies\n    inp_vocab = Vocab()\n    out_vocab = Vocab()\n\n    # Load datasets\n    train_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.train.tsv\", inp_vocab, out_vocab)\n    dev_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.dev.tsv\", inp_vocab, out_vocab)\n    test_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.test.tsv\", inp_vocab, out_vocab, is_test=True)\n\n    # Create data loaders\n    train_loader = DataLoader(train_data, batch_size=best_config['batch_size'], \n                            shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=best_config['batch_size'],\n                          shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    # Initialize model\n    encoder = Encoder(inp_vocab.size, best_config['embedding_dim'], \n                     best_config['hidden_dim'], best_config['enc_layers'], \n                     best_config['cell_type'], best_config['dropout'])\n    \n    decoder = Decoder(out_vocab.size, best_config['embedding_dim'],\n                     best_config['hidden_dim'], best_config['dec_layers'],\n                     best_config['cell_type'], best_config['dropout'],\n                     best_config['attention_type'])\n    \n    model = Seq2Seq(encoder, decoder, device).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=best_config['learning_rate'])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    # Training loop\n    best_val_loss = float('inf')\n    for epoch in range(best_config['epochs']):\n        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n        \n        print(f\"\\nEpoch {epoch+1}/{best_config['epochs']}\")\n        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc\n        })\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(\"Best model saved!\")\n            \n\n# =======================\n# 3. Training Pipeline\n# =======================\ndef main():\n    # ... (original main setup)\n    \n    # After training, generate visualizations\n    generate_visualizations(model, test_loader, inp_vocab, out_vocab, device)\n\n# =======================\n# 4. Visualization Functions\n# =======================\ndef generate_visualizations(model, test_loader, inp_vocab, out_vocab, device):\n    model.eval()\n    predictions = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            # ... (original prediction code)\n            predictions.append({\n                'input': lat[0],\n                'true': true_str,\n                'pred': pred_str,\n                'input_tokens': input_tokens,\n                'output_tokens': output_tokens,\n                'attentions': np.array(attentions)\n            })\n\n    # Generate heatmaps\n    plot_heatmaps(predictions[:9])  # 3x3 grid\n\ndef plot_heatmaps(samples):\n    fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n    \n    for idx, sample in enumerate(samples):\n        ax = axes[idx//3, idx%3]\n        attn = sample['attentions']\n        \n        sns.heatmap(attn, ax=ax,\n                    xticklabels=sample['input_tokens'],\n                    yticklabels=sample['output_tokens'],\n                    cmap=\"YlGnBu\")\n        \n        # Set fonts explicitly\n        ax.set_title(f\"Input: {sample['input']}\\nTrue: {sample['true']}\\nPred: {sample['pred']}\", \n                    fontproperties=font_prop)\n        ax.set_xticklabels(ax.get_xticklabels(), fontproperties=font_prop)\n        ax.set_yticklabels(ax.get_yticklabels(), fontproperties=font_prop)\n    \n    plt.tight_layout()\n    wandb.log({\"Attention Heatmaps\": wandb.Image(fig)})\n    plt.close()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport random\nimport wandb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.font_manager as fm\nfrom matplotlib.font_manager import FontProperties\n\n# =======================\n# Best Configuration\n# =======================\nbest_config = {\n    'embedding_dim': 256,\n    'hidden_dim': 256,\n    'enc_layers': 2,\n    'dec_layers': 2,\n    'cell_type': 'LSTM',\n    'dropout': 0.5,\n    'epochs': 15,\n    'beam_size': 5,\n    'attention_type': 'concat',\n    'batch_size': 256,\n    'learning_rate': 0.001\n}\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.char2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2char = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for char in text:\n                if char not in self.char2idx:\n                    self.char2idx[char] = self.size\n                    self.idx2char[self.size] = char\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.char2idx[c] for c in text]\n\n    def decode(self, idxs):\n        return ''.join([self.idx2char[i] for i in idxs if i not in [0, 1, 2]])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab, out_vocab, is_test=False):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        if not is_test:\n            inp_vocab.build([p[0] for p in self.pairs])\n            out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        if self.is_test:\n            return torch.tensor(x), lat, dev\n        y = [self.out_vocab.char2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.char2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y), lat, dev\n\ndef collate_fn(batch):\n    if len(batch[0]) == 3:  # Test batch\n        x_batch, lat, dev = zip(*batch)\n        x_lens = [len(x) for x in x_batch]\n        x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n        return x_pad, lat, dev, torch.tensor(x_lens)\n    else:  # Train/val batch\n        x_batch, y_batch, lat, dev = zip(*batch)\n        x_lens = [len(x) for x in x_batch]\n        y_lens = [len(y) for y in y_batch]\n        x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n        y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n        return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens), lat, dev\n\n# =======================\n# Model Components\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        return outputs, hidden\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_type='general'):\n        super().__init__()\n        self.attention_type = attention_type\n        if attention_type == 'general':\n            self.attn = nn.Linear(hidden_dim, hidden_dim)\n        elif attention_type == 'concat':\n            self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n            self.v = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs, mask=None):\n        batch_size, src_len, hidden_dim = encoder_outputs.size()\n        \n        if self.attention_type == 'general':\n            energy = torch.bmm(encoder_outputs, self.attn(hidden).unsqueeze(2)).squeeze(2)\n        elif self.attention_type == 'concat':\n            hidden_expanded = hidden.unsqueeze(1).repeat(1, src_len, 1)\n            concat = torch.cat((hidden_expanded, encoder_outputs), dim=2)\n            energy = self.v(torch.tanh(self.attn(concat))).squeeze(2)\n        else:  # dot\n            energy = torch.bmm(encoder_outputs, hidden.unsqueeze(2)).squeeze(2)\n        \n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, -1e10)\n        attention_weights = F.softmax(energy, dim=1)\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n        return context, attention_weights\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout, attention_type):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim + hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.attention = Attention(hidden_dim, attention_type)\n        self.out = nn.Linear(hidden_dim * 2, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n        if isinstance(hidden, tuple):  # LSTM\n            attn_hidden = hidden[0][-1]\n        else:  # GRU/RNN\n            attn_hidden = hidden[-1]\n        \n        context, attn_weights = self.attention(attn_hidden, encoder_outputs, mask)\n        embedded = self.embedding(input_token)\n        rnn_input = torch.cat((embedded, context), dim=1).unsqueeze(1)\n        output, hidden = self.rnn(rnn_input, hidden)\n        \n        if isinstance(hidden, tuple):\n            output_hidden = hidden[0][-1]\n        else:\n            output_hidden = hidden[-1]\n        \n        output = torch.cat((output_hidden, context), dim=1)\n        output = self.dropout(output)\n        prediction = self.out(output)\n        return prediction, hidden, attn_weights\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def create_mask(self, src_lens, max_len):\n        batch_size = len(src_lens)\n        mask = torch.zeros(batch_size, max_len, device=self.device)\n        for i, length in enumerate(src_lens):\n            mask[i, :length] = 1\n        return mask\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        src_data, src_lens = src\n        encoder_outputs, enc_hidden = self.encoder(src_data, src_lens)\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n        \n        src_len = encoder_outputs.size(1)\n        mask = self.create_mask(src_lens, src_len)\n\n        if isinstance(enc_hidden, tuple):\n            dec_hidden = enc_hidden\n        else:\n            dec_hidden = enc_hidden\n\n        input_token = trg[:, 0]\n        for t in range(1, trg_len):\n            output, dec_hidden, _ = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n            outputs[:, t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if teacher_force else top1\n            \n        return outputs\n\n    def predict(self, src, src_lens, max_len=30):\n        self.eval()\n        with torch.no_grad():\n            encoder_outputs, enc_hidden = self.encoder(src, src_lens)\n            src_len = encoder_outputs.size(1)\n            mask = self.create_mask(src_lens.tolist(), src_len)\n            \n            if isinstance(enc_hidden, tuple):\n                dec_hidden = enc_hidden\n            else:\n                dec_hidden = enc_hidden\n            \n            input_token = torch.tensor([1], device=self.device)\n            output_seq = []\n            attention_weights = []\n            \n            for _ in range(max_len):\n                output, dec_hidden, attn_weights = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n                top1 = output.argmax(1)\n                if top1.item() == 2:  # <eos> token\n                    break\n                output_seq.append(top1.item())\n                attention_weights.append(attn_weights.cpu().numpy())\n                input_token = top1\n                \n        return output_seq, attention_weights\n\n# =======================\n# Training and Evaluation\n# =======================\ndef train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    \n    for batch in loader:\n        src, trg, src_lens, _, _, _ = batch\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model((src, src_lens), trg)\n        \n        # Calculate loss\n        output_dim = output.shape[-1]\n        loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        \n        # Calculate accuracy\n        pred = output.argmax(dim=2)\n        mask = (trg[:, 1:] != 0)\n        correct = ((pred[:, 1:] == trg[:, 1:]) & mask).sum().item()\n        total_correct += correct\n        total_tokens += mask.sum().item()\n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(loader)\n    accuracy = (total_correct / total_tokens) * 100 if total_tokens > 0 else 0\n    return avg_loss, accuracy\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    \n    with torch.no_grad():\n        for batch in loader:\n            src, trg, src_lens, _, _, _ = batch\n            src, trg = src.to(device), trg.to(device)\n            output = model((src, src_lens), trg, teacher_forcing_ratio=0)\n            \n            # Calculate loss\n            output_dim = output.shape[-1]\n            loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n            \n            # Calculate accuracy\n            pred = output.argmax(dim=2)\n            mask = (trg[:, 1:] != 0)\n            correct = ((pred[:, 1:] == trg[:, 1:]) & mask).sum().item()\n            total_correct += correct\n            total_tokens += mask.sum().item()\n            total_loss += loss.item()\n    \n    avg_loss = total_loss / len(loader)\n    accuracy = (total_correct / total_tokens) * 100 if total_tokens > 0 else 0\n    return avg_loss, accuracy\n\n# =======================\n# Attention Visualization\n# =======================\ndef show_attention_grid(samples):\n    # Try to load a font that supports Devanagari script\n    # Use a default font if specific font not available\n    try:\n        devanagari_font = FontProperties(family='Nirmala UI', size=10)\n    except:\n        devanagari_font = FontProperties(size=10)\n\n    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n    axes = axes.flatten()\n\n    for idx, (input_seq, true_output, predicted_output, attentions) in enumerate(samples):\n        if idx >= 9:  # Only show 9 samples in the 3x3 grid\n            break\n            \n        ax = axes[idx]\n        input_chars = list(input_seq)\n        output_chars = list(predicted_output)\n        attn_matrix = np.array(attentions).squeeze(1)\n\n        sns.heatmap(\n            attn_matrix[:len(output_chars), :len(input_chars)],\n            xticklabels=input_chars,\n            yticklabels=output_chars,\n            cmap='viridis',\n            ax=ax,\n            cbar=False\n        )\n        ax.set_yticklabels(ax.get_yticklabels(), fontproperties=devanagari_font, rotation=0)\n        ax.set_title(f\"In: {input_seq}\\nGT: {true_output}\\nPred: {predicted_output}\", \n                     fontsize=9, fontproperties=devanagari_font)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n\n    # Hide any unused subplots\n    for j in range(len(samples), 9):\n        fig.delaxes(axes[j])\n\n    plt.tight_layout()\n    plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n    wandb.log({\"attention_heatmap\": wandb.Image('attention_heatmap.png')})\n    plt.show()\n    plt.close()\n\n# =======================\n# Main Execution\n# =======================\ndef main():\n    wandb.init(config=best_config, project=\"dakshina-translit-test\")\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Initialize vocabularies\n    inp_vocab = Vocab()\n    out_vocab = Vocab()\n\n    # Load datasets\n    train_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.train.tsv\", inp_vocab, out_vocab)\n    dev_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.dev.tsv\", inp_vocab, out_vocab)\n    test_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.test.tsv\", inp_vocab, out_vocab, is_test=True)\n\n    # Create data loaders\n    train_loader = DataLoader(train_data, batch_size=best_config['batch_size'], \n                            shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=best_config['batch_size'],\n                          shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    # Initialize model\n    encoder = Encoder(inp_vocab.size, best_config['embedding_dim'], \n                     best_config['hidden_dim'], best_config['enc_layers'], \n                     best_config['cell_type'], best_config['dropout'])\n    \n    decoder = Decoder(out_vocab.size, best_config['embedding_dim'],\n                     best_config['hidden_dim'], best_config['dec_layers'],\n                     best_config['cell_type'], best_config['dropout'],\n                     best_config['attention_type'])\n    \n    model = Seq2Seq(encoder, decoder, device).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=best_config['learning_rate'])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    # Training loop\n    best_val_loss = float('inf')\n    for epoch in range(best_config['epochs']):\n        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n        \n        print(f\"\\nEpoch {epoch+1}/{best_config['epochs']}\")\n        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc\n        })\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(\"Best model saved!\")\n\n    # Test evaluation\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    model.eval()\n    \n    total_correct = 0\n    total_samples = 0\n    predictions = []\n    attention_samples = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            src, lat, dev, src_lens = batch\n            src = src.to(device)\n            pred_ids, attention_weights = model.predict(src, src_lens)\n            pred_str = out_vocab.decode(pred_ids)\n            true_str = dev[0]\n            \n            predictions.append({\n                'input': lat[0],\n                'true': true_str,\n                'pred': pred_str\n            })\n            \n            # Save attention weights for visualization\n            attention_samples.append((lat[0], true_str, pred_str, attention_weights))\n            \n            if pred_str == true_str:\n                total_correct += 1\n            total_samples += 1\n            \n            # Only collect 9 samples for visualization\n            if len(attention_samples) >= 10:\n                break\n\n    # Calculate accuracy\n    accuracy = 100 * total_correct / total_samples\n    print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n    wandb.log({\"test_acc\": accuracy})\n\n    # Visualize attention for 9 samples\n    show_attention_grid(attention_samples)\n\n    # Create and log a table of predictions\n    table = wandb.Table(columns=[\"Input\", \"True\", \"Predicted\"])\n    for p in predictions[:20]:  # Log first 20 predictions\n        table.add_data(p['input'], p['true'], p['pred'])\n    \n    wandb.log({\n        \"predictions\": table,\n        \"test_accuracy\": accuracy\n    })\n    \n    # Save predictions\n    with open(\"test_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n        f.write(f\"Test Accuracy: {accuracy:.2f}%\\n\\n\")\n        for p in predictions[:20]:\n            f.write(f\"Input: {p['input']}\\n\")\n            f.write(f\"True: {p['true']}\\n\")\n            f.write(f\"Pred: {p['pred']}\\n\\n\")\n    \n    # Print random samples\n    print(\"\\nRandom Samples:\")\n    samples = random.sample(predictions, min(30, len(predictions)))\n    for sample in samples:\n        print(f\"Input: {sample['input']}\")\n        print(f\"True: {sample['true']}\")\n        print(f\"Pred: {sample['pred']}\\n\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:00:53.170076Z","iopub.execute_input":"2025-05-20T14:00:53.170677Z","iopub.status.idle":"2025-05-20T14:03:57.299773Z","shell.execute_reply.started":"2025-05-20T14:00:53.170655Z","shell.execute_reply":"2025-05-20T14:03:57.299180Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>train_acc</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>test_acc</td><td>44.44444</td></tr><tr><td>train_acc</td><td>48.38749</td></tr><tr><td>train_loss</td><td>1.68961</td></tr><tr><td>val_acc</td><td>67.75846</td></tr><tr><td>val_loss</td><td>1.01482</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">elated-jazz-14</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/ioc8muk8' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/ioc8muk8</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_135924-ioc8muk8/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_140053-49u7a6pk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/49u7a6pk' target=\"_blank\">robust-butterfly-15</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/49u7a6pk' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/49u7a6pk</a>"},"metadata":{}},{"name":"stdout","text":"\nEpoch 1/15\nTrain Loss: 1.6909 | Train Acc: 48.27%\nVal Loss: 1.1365 | Val Acc: 63.81%\nBest model saved!\n\nEpoch 2/15\nTrain Loss: 0.8609 | Train Acc: 73.36%\nVal Loss: 0.8622 | Val Acc: 72.99%\nBest model saved!\n\nEpoch 3/15\nTrain Loss: 0.7459 | Train Acc: 76.86%\nVal Loss: 0.8102 | Val Acc: 74.20%\nBest model saved!\n\nEpoch 4/15\nTrain Loss: 0.6808 | Train Acc: 78.82%\nVal Loss: 0.8150 | Val Acc: 74.47%\n\nEpoch 5/15\nTrain Loss: 0.6465 | Train Acc: 79.85%\nVal Loss: 0.7820 | Val Acc: 75.08%\nBest model saved!\n\nEpoch 6/15\nTrain Loss: 0.6277 | Train Acc: 80.23%\nVal Loss: 0.7887 | Val Acc: 75.49%\n\nEpoch 7/15\nTrain Loss: 0.6009 | Train Acc: 81.08%\nVal Loss: 0.8053 | Val Acc: 75.23%\n\nEpoch 8/15\nTrain Loss: 0.5870 | Train Acc: 81.46%\nVal Loss: 0.8006 | Val Acc: 75.72%\n\nEpoch 9/15\nTrain Loss: 0.5657 | Train Acc: 82.14%\nVal Loss: 0.8220 | Val Acc: 76.02%\n\nEpoch 10/15\nTrain Loss: 0.5604 | Train Acc: 82.26%\nVal Loss: 0.7560 | Val Acc: 76.09%\nBest model saved!\n\nEpoch 11/15\nTrain Loss: 0.5327 | Train Acc: 83.13%\nVal Loss: 0.8166 | Val Acc: 76.01%\n\nEpoch 12/15\nTrain Loss: 0.5378 | Train Acc: 82.89%\nVal Loss: 0.7670 | Val Acc: 76.19%\n\nEpoch 13/15\nTrain Loss: 0.5390 | Train Acc: 82.83%\nVal Loss: 0.7693 | Val Acc: 75.67%\n\nEpoch 14/15\nTrain Loss: 0.5348 | Train Acc: 82.72%\nVal Loss: 0.7976 | Val Acc: 76.36%\n\nEpoch 15/15\nTrain Loss: 0.5167 | Train Acc: 83.37%\nVal Loss: 0.7828 | Val Acc: 76.13%\n\nTest Accuracy: 44.44%\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/3128727622.py:343: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3128727622.py:343: UserWarning: Matplotlib currently does not support Devanagari natively.\n  plt.tight_layout()\n/tmp/ipykernel_35/3128727622.py:343: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3128727622.py:343: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3128727622.py:343: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3128727622.py:343: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3128727622.py:343: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3128727622.py:344: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/3128727622.py:344: UserWarning: Matplotlib currently does not support Devanagari natively.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/3128727622.py:344: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/3128727622.py:344: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/3128727622.py:344: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/3128727622.py:344: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/3128727622.py:344: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/3128727622.py:344: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/3128727622.py:344: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/3128727622.py:344: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x1500 with 9 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nRandom Samples:\nInput: अंकित\nTrue: ankit\nPred: ankit\n\nInput: अंक\nTrue: ank\nPred: ank\n\nInput: अंकों\nTrue: ankon\nPred: ankon\n\nInput: अंकों\nTrue: anakon\nPred: ankon\n\nInput: अंकोर\nTrue: angkor\nPred: ankor\n\nInput: अंक\nTrue: anka\nPred: ank\n\nInput: अंकोर\nTrue: ankor\nPred: ankor\n\nInput: अंगारक\nTrue: angaarak\nPred: angarak\n\nInput: अंकों\nTrue: ankhon\nPred: ankon\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport random\nimport wandb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.font_manager as fm\nfrom matplotlib.font_manager import FontProperties\n\n# =======================\n# Best Configuration\n# =======================\nbest_config = {\n    'embedding_dim': 256,\n    'hidden_dim': 256,\n    'enc_layers': 2,\n    'dec_layers': 2,\n    'cell_type': 'LSTM',\n    'dropout': 0.5,\n    'epochs': 15,\n    'beam_size': 5,\n    'attention_type': 'concat',\n    'batch_size': 256,\n    'learning_rate': 0.001\n}\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.char2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2char = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for char in text:\n                if char not in self.char2idx:\n                    self.char2idx[char] = self.size\n                    self.idx2char[self.size] = char\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.char2idx[c] for c in text]\n\n    def decode(self, idxs):\n        return ''.join([self.idx2char[i] for i in idxs if i not in [0, 1, 2]])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab, out_vocab, is_test=False):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        if not is_test:\n            inp_vocab.build([p[0] for p in self.pairs])\n            out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        if self.is_test:\n            return torch.tensor(x), lat, dev\n        y = [self.out_vocab.char2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.char2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y), lat, dev\n\ndef collate_fn(batch):\n    if len(batch[0]) == 3:  # Test batch\n        x_batch, lat, dev = zip(*batch)\n        x_lens = [len(x) for x in x_batch]\n        x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n        return x_pad, lat, dev, torch.tensor(x_lens)\n    else:  # Train/val batch\n        x_batch, y_batch, lat, dev = zip(*batch)\n        x_lens = [len(x) for x in x_batch]\n        y_lens = [len(y) for y in y_batch]\n        x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n        y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n        return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens), lat, dev\n\n# =======================\n# Model Components\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        return outputs, hidden\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_type='general'):\n        super().__init__()\n        self.attention_type = attention_type\n        if attention_type == 'general':\n            self.attn = nn.Linear(hidden_dim, hidden_dim)\n        elif attention_type == 'concat':\n            self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n            self.v = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs, mask=None):\n        batch_size, src_len, hidden_dim = encoder_outputs.size()\n        \n        if self.attention_type == 'general':\n            energy = torch.bmm(encoder_outputs, self.attn(hidden).unsqueeze(2)).squeeze(2)\n        elif self.attention_type == 'concat':\n            hidden_expanded = hidden.unsqueeze(1).repeat(1, src_len, 1)\n            concat = torch.cat((hidden_expanded, encoder_outputs), dim=2)\n            energy = self.v(torch.tanh(self.attn(concat))).squeeze(2)\n        else:  # dot\n            energy = torch.bmm(encoder_outputs, hidden.unsqueeze(2)).squeeze(2)\n        \n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, -1e10)\n        attention_weights = F.softmax(energy, dim=1)\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n        return context, attention_weights\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout, attention_type):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim + hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.attention = Attention(hidden_dim, attention_type)\n        self.out = nn.Linear(hidden_dim * 2, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n        if isinstance(hidden, tuple):  # LSTM\n            attn_hidden = hidden[0][-1]\n        else:  # GRU/RNN\n            attn_hidden = hidden[-1]\n        \n        context, attn_weights = self.attention(attn_hidden, encoder_outputs, mask)\n        embedded = self.embedding(input_token)\n        rnn_input = torch.cat((embedded, context), dim=1).unsqueeze(1)\n        output, hidden = self.rnn(rnn_input, hidden)\n        \n        if isinstance(hidden, tuple):\n            output_hidden = hidden[0][-1]\n        else:\n            output_hidden = hidden[-1]\n        \n        output = torch.cat((output_hidden, context), dim=1)\n        output = self.dropout(output)\n        prediction = self.out(output)\n        return prediction, hidden, attn_weights\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def create_mask(self, src_lens, max_len):\n        batch_size = len(src_lens)\n        mask = torch.zeros(batch_size, max_len, device=self.device)\n        for i, length in enumerate(src_lens):\n            mask[i, :length] = 1\n        return mask\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        src_data, src_lens = src\n        encoder_outputs, enc_hidden = self.encoder(src_data, src_lens)\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n        \n        src_len = encoder_outputs.size(1)\n        mask = self.create_mask(src_lens, src_len)\n\n        if isinstance(enc_hidden, tuple):\n            dec_hidden = enc_hidden\n        else:\n            dec_hidden = enc_hidden\n\n        input_token = trg[:, 0]\n        for t in range(1, trg_len):\n            output, dec_hidden, _ = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n            outputs[:, t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if teacher_force else top1\n            \n        return outputs\n\n    def predict(self, src, src_lens, max_len=30):\n        self.eval()\n        with torch.no_grad():\n            encoder_outputs, enc_hidden = self.encoder(src, src_lens)\n            src_len = encoder_outputs.size(1)\n            mask = self.create_mask(src_lens.tolist(), src_len)\n            \n            if isinstance(enc_hidden, tuple):\n                dec_hidden = enc_hidden\n            else:\n                dec_hidden = enc_hidden\n            \n            input_token = torch.tensor([1], device=self.device)\n            output_seq = []\n            attention_weights = []\n            \n            for _ in range(max_len):\n                output, dec_hidden, attn_weights = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n                top1 = output.argmax(1)\n                if top1.item() == 2:  # <eos> token\n                    break\n                output_seq.append(top1.item())\n                attention_weights.append(attn_weights.cpu().numpy())\n                input_token = top1\n                \n        return output_seq, attention_weights\n\n# =======================\n# Training and Evaluation with Word-Level Accuracy\n# =======================\ndef train(model, loader, criterion, optimizer, device, out_vocab):\n    model.train()\n    total_loss = 0\n    total_correct_chars = 0\n    total_chars = 0\n    \n    # Word-level tracking\n    total_correct_words = 0\n    total_words = 0\n    \n    for batch in loader:\n        src, trg, src_lens, _, lat, dev = batch\n        src, trg = src.to(device), trg.to(device)\n        \n        optimizer.zero_grad()\n        output = model((src, src_lens), trg)\n        \n        # Calculate loss\n        output_dim = output.shape[-1]\n        loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        \n        # Calculate character-level accuracy\n        pred = output.argmax(dim=2)\n        mask = (trg[:, 1:] != 0)  # Ignore padding\n        correct_chars = ((pred[:, 1:] == trg[:, 1:]) & mask).sum().item()\n        total_correct_chars += correct_chars\n        total_chars += mask.sum().item()\n        \n        # Calculate word-level accuracy\n        batch_size = trg.size(0)\n        for i in range(batch_size):\n            # Get predicted sequence without padding, sos, eos\n            pred_seq = [idx.item() for idx in pred[i, 1:] if idx.item() not in [0, 1, 2]]\n            # Convert to string\n            pred_word = out_vocab.decode(pred_seq)\n            \n            # Get true word\n            true_word = dev[i]\n            \n            # Increment counts\n            total_words += 1\n            if pred_word == true_word:\n                total_correct_words += 1\n                \n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(loader)\n    char_accuracy = (total_correct_chars / total_chars) * 100 if total_chars > 0 else 0\n    word_accuracy = (total_correct_words / total_words) * 100 if total_words > 0 else 0\n    \n    return avg_loss, char_accuracy, word_accuracy\n\ndef evaluate(model, loader, criterion, device, out_vocab):\n    model.eval()\n    total_loss = 0\n    total_correct_chars = 0\n    total_chars = 0\n    \n    # Word-level tracking\n    total_correct_words = 0\n    total_words = 0\n    \n    with torch.no_grad():\n        for batch in loader:\n            src, trg, src_lens, _, lat, dev = batch\n            src, trg = src.to(device), trg.to(device)\n            \n            output = model((src, src_lens), trg, teacher_forcing_ratio=0)\n            \n            # Calculate loss\n            output_dim = output.shape[-1]\n            loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n            \n            # Calculate character-level accuracy\n            pred = output.argmax(dim=2)\n            mask = (trg[:, 1:] != 0)  # Ignore padding\n            correct_chars = ((pred[:, 1:] == trg[:, 1:]) & mask).sum().item()\n            total_correct_chars += correct_chars\n            total_chars += mask.sum().item()\n            \n            # Calculate word-level accuracy\n            batch_size = trg.size(0)\n            for i in range(batch_size):\n                # Get predicted sequence without padding, sos, eos\n                pred_seq = [idx.item() for idx in pred[i, 1:] if idx.item() not in [0, 1, 2]]\n                # Convert to string\n                pred_word = out_vocab.decode(pred_seq)\n                \n                # Get true word\n                true_word = dev[i]\n                \n                # Increment counts\n                total_words += 1\n                if pred_word == true_word:\n                    total_correct_words += 1\n                    \n            total_loss += loss.item()\n    \n    avg_loss = total_loss / len(loader)\n    char_accuracy = (total_correct_chars / total_chars) * 100 if total_chars > 0 else 0\n    word_accuracy = (total_correct_words / total_words) * 100 if total_words > 0 else 0\n    \n    return avg_loss, char_accuracy, word_accuracy\n\n# =======================\n# Attention Visualization\n# =======================\ndef show_attention_grid(samples):\n    # Try to load a font that supports Devanagari script\n    # Use a default font if specific font not available\n    try:\n        devanagari_font = FontProperties(family='Nirmala UI', size=10)\n    except:\n        devanagari_font = FontProperties(size=10)\n\n    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n    axes = axes.flatten()\n\n    for idx, (input_seq, true_output, predicted_output, attentions) in enumerate(samples):\n        if idx >= 9:  # Only show 9 samples in the 3x3 grid\n            break\n            \n        ax = axes[idx]\n        input_chars = list(input_seq)\n        output_chars = list(predicted_output)\n        attn_matrix = np.array(attentions).squeeze(1)\n\n        sns.heatmap(\n            attn_matrix[:len(output_chars), :len(input_chars)],\n            xticklabels=input_chars,\n            yticklabels=output_chars,\n            cmap='viridis',\n            ax=ax,\n            cbar=False\n        )\n        ax.set_yticklabels(ax.get_yticklabels(), fontproperties=devanagari_font, rotation=0)\n        ax.set_title(f\"In: {input_seq}\\nGT: {true_output}\\nPred: {predicted_output}\", \n                     fontsize=9, fontproperties=devanagari_font)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n\n    # Hide any unused subplots\n    for j in range(len(samples), 9):\n        fig.delaxes(axes[j])\n\n    plt.tight_layout()\n    plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n    wandb.log({\"attention_heatmap\": wandb.Image('attention_heatmap.png')})\n    plt.show()\n    plt.close()\n\n# =======================\n# Main Execution\n# =======================\ndef main():\n    wandb.init(config=best_config, project=\"dakshina-translit-test\")\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Initialize vocabularies\n    inp_vocab = Vocab()\n    out_vocab = Vocab()\n\n    # Load datasets\n    train_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.train.tsv\", inp_vocab, out_vocab)\n    dev_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.dev.tsv\", inp_vocab, out_vocab)\n    test_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.test.tsv\", inp_vocab, out_vocab, is_test=True)\n\n    # Create data loaders\n    train_loader = DataLoader(train_data, batch_size=best_config['batch_size'], \n                            shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=best_config['batch_size'],\n                          shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    # Initialize model\n    encoder = Encoder(inp_vocab.size, best_config['embedding_dim'], \n                     best_config['hidden_dim'], best_config['enc_layers'], \n                     best_config['cell_type'], best_config['dropout'])\n    \n    decoder = Decoder(out_vocab.size, best_config['embedding_dim'],\n                     best_config['hidden_dim'], best_config['dec_layers'],\n                     best_config['cell_type'], best_config['dropout'],\n                     best_config['attention_type'])\n    \n    model = Seq2Seq(encoder, decoder, device).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=best_config['learning_rate'])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    # Training loop\n    best_val_loss = float('inf')\n    for epoch in range(best_config['epochs']):\n        train_loss, train_char_acc, train_word_acc = train(model, train_loader, criterion, optimizer, device, out_vocab)\n        val_loss, val_char_acc, val_word_acc = evaluate(model, dev_loader, criterion, device, out_vocab)\n        \n        print(f\"\\nEpoch {epoch+1}/{best_config['epochs']}\")\n        print(f\"Train Loss: {train_loss:.4f} | Train Char Acc: {train_char_acc:.2f}% | Train Word Acc: {train_word_acc:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f} | Val Char Acc: {val_char_acc:.2f}% | Val Word Acc: {val_word_acc:.2f}%\")\n\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_char_acc\": train_char_acc,\n            \"train_word_acc\": train_word_acc,\n            \"val_loss\": val_loss,\n            \"val_char_acc\": val_char_acc,\n            \"val_word_acc\": val_word_acc\n        })\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(\"Best model saved!\")\n\n    # Test evaluation\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    model.eval()\n    \n    total_char_correct = 0\n    total_chars = 0\n    total_word_correct = 0\n    total_words = 0\n    predictions = []\n    attention_samples = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            src, lat, dev, src_lens = batch\n            src = src.to(device)\n            pred_ids, attention_weights = model.predict(src, src_lens)\n            pred_str = out_vocab.decode(pred_ids)\n            true_str = dev[0]\n            \n            # Word-level accuracy\n            total_words += 1\n            if pred_str == true_str:\n                total_word_correct += 1\n            \n            # Character-level accuracy (for comparison)\n            min_len = min(len(pred_str), len(true_str))\n            for i in range(min_len):\n                total_chars += 1\n                if pred_str[i] == true_str[i]:\n                    total_char_correct += 1\n            # Count remaining characters in longer string as errors\n            total_chars += abs(len(pred_str) - len(true_str))\n            \n            predictions.append({\n                'input': lat[0],\n                'true': true_str,\n                'pred': pred_str\n            })\n            \n            # Save attention weights for visualization\n            attention_samples.append((lat[0], true_str, pred_str, attention_weights))\n            \n            # Only collect 9 samples for visualization\n            if len(attention_samples) >= 10:\n                break\n\n    # Calculate accuracies\n    word_accuracy = 100 * total_word_correct / total_words if total_words > 0 else 0\n    char_accuracy = 100 * total_char_correct / total_chars if total_chars > 0 else 0\n    \n    print(f\"\\nTest Word Accuracy: {word_accuracy:.2f}%\")\n    print(f\"Test Character Accuracy: {char_accuracy:.2f}%\")\n    \n    wandb.log({\n        \"test_word_acc\": word_accuracy,\n        \"test_char_acc\": char_accuracy\n    })\n\n    # Visualize attention for 9 samples\n    show_attention_grid(attention_samples)\n\n    # Create and log a table of predictions\n    table = wandb.Table(columns=[\"Input\", \"True\", \"Predicted\", \"Correct\"])\n    for p in predictions[:20]:  # Log first 20 predictions\n        is_correct = p['true'] == p['pred']\n        table.add_data(p['input'], p['true'], p['pred'], is_correct)\n    \n    wandb.log({\n        \"predictions\": table,\n        \"test_word_accuracy\": word_accuracy,\n        \"test_char_accuracy\": char_accuracy\n    })\n    \n    # Save predictions\n    with open(\"test_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n        f.write(f\"Test Word Accuracy: {word_accuracy:.2f}%\\n\")\n        f.write(f\"Test Character Accuracy: {char_accuracy:.2f}%\\n\\n\")\n        for p in predictions[:20]:\n            f.write(f\"Input: {p['input']}\\n\")\n            f.write(f\"True: {p['true']}\\n\")\n            f.write(f\"Pred: {p['pred']}\\n\")\n            f.write(f\"Correct: {p['true'] == p['pred']}\\n\\n\")\n    \n    # Print random samples\n    print(\"\\nRandom Samples:\")\n    samples = random.sample(predictions, min(30, len(predictions)))\n    for sample in samples:\n        print(f\"Input: {sample['input']}\")\n        print(f\"True: {sample['true']}\")\n        print(f\"Pred: {sample['pred']}\")\n        print(f\"Correct: {sample['true'] == sample['pred']}\\n\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:28:31.563489Z","iopub.execute_input":"2025-05-20T14:28:31.563790Z","iopub.status.idle":"2025-05-20T14:34:47.686622Z","shell.execute_reply.started":"2025-05-20T14:28:31.563768Z","shell.execute_reply":"2025-05-20T14:34:47.686062Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▇▇▇█▇████████</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▂▂▂▂▁▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>test_acc</td><td>44.44444</td></tr><tr><td>test_accuracy</td><td>44.44444</td></tr><tr><td>train_acc</td><td>83.37274</td></tr><tr><td>train_loss</td><td>0.51671</td></tr><tr><td>val_acc</td><td>76.13206</td></tr><tr><td>val_loss</td><td>0.78276</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">robust-butterfly-15</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/49u7a6pk' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/49u7a6pk</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test</a><br>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_140053-49u7a6pk/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_142831-m7p25uta</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/m7p25uta' target=\"_blank\">chocolate-dust-16</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/m7p25uta' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/m7p25uta</a>"},"metadata":{}},{"name":"stdout","text":"\nEpoch 1/15\nTrain Loss: 1.6923 | Train Char Acc: 48.68% | Train Word Acc: 5.39%\nVal Loss: 0.9486 | Val Char Acc: 69.98% | Val Word Acc: 28.32%\nBest model saved!\n\nEpoch 2/15\nTrain Loss: 0.8568 | Train Char Acc: 73.71% | Train Word Acc: 21.05%\nVal Loss: 0.8781 | Val Char Acc: 71.93% | Val Word Acc: 32.58%\nBest model saved!\n\nEpoch 3/15\nTrain Loss: 0.7478 | Train Char Acc: 76.81% | Train Word Acc: 24.22%\nVal Loss: 0.8738 | Val Char Acc: 73.08% | Val Word Acc: 38.07%\nBest model saved!\n\nEpoch 4/15\nTrain Loss: 0.6857 | Train Char Acc: 78.68% | Train Word Acc: 24.36%\nVal Loss: 0.8099 | Val Char Acc: 74.70% | Val Word Acc: 39.10%\nBest model saved!\n\nEpoch 5/15\nTrain Loss: 0.6487 | Train Char Acc: 79.70% | Train Word Acc: 26.29%\nVal Loss: 0.8061 | Val Char Acc: 74.79% | Val Word Acc: 40.32%\nBest model saved!\n\nEpoch 6/15\nTrain Loss: 0.6261 | Train Char Acc: 80.36% | Train Word Acc: 27.30%\nVal Loss: 0.7846 | Val Char Acc: 75.52% | Val Word Acc: 40.82%\nBest model saved!\n\nEpoch 7/15\nTrain Loss: 0.5919 | Train Char Acc: 81.38% | Train Word Acc: 26.94%\nVal Loss: 0.8054 | Val Char Acc: 75.10% | Val Word Acc: 41.51%\n\nEpoch 8/15\nTrain Loss: 0.5876 | Train Char Acc: 81.43% | Train Word Acc: 28.57%\nVal Loss: 0.8151 | Val Char Acc: 75.57% | Val Word Acc: 40.34%\n\nEpoch 9/15\nTrain Loss: 0.5609 | Train Char Acc: 82.20% | Train Word Acc: 26.76%\nVal Loss: 0.8538 | Val Char Acc: 75.80% | Val Word Acc: 39.01%\n\nEpoch 10/15\nTrain Loss: 0.5543 | Train Char Acc: 82.37% | Train Word Acc: 22.97%\nVal Loss: 0.7827 | Val Char Acc: 76.11% | Val Word Acc: 27.70%\nBest model saved!\n\nEpoch 11/15\nTrain Loss: 0.5464 | Train Char Acc: 82.59% | Train Word Acc: 20.19%\nVal Loss: 0.8029 | Val Char Acc: 75.96% | Val Word Acc: 31.69%\n\nEpoch 12/15\nTrain Loss: 0.5309 | Train Char Acc: 83.02% | Train Word Acc: 21.10%\nVal Loss: 0.8105 | Val Char Acc: 76.11% | Val Word Acc: 35.29%\n\nEpoch 13/15\nTrain Loss: 0.5297 | Train Char Acc: 82.95% | Train Word Acc: 20.08%\nVal Loss: 0.7932 | Val Char Acc: 76.09% | Val Word Acc: 33.25%\n\nEpoch 14/15\nTrain Loss: 0.5091 | Train Char Acc: 83.68% | Train Word Acc: 20.64%\nVal Loss: 0.7914 | Val Char Acc: 76.00% | Val Word Acc: 30.06%\n\nEpoch 15/15\nTrain Loss: 0.5150 | Train Char Acc: 83.48% | Train Word Acc: 17.63%\nVal Loss: 0.7903 | Val Char Acc: 75.89% | Val Word Acc: 27.42%\n\nTest Word Accuracy: 50.00%\nTest Character Accuracy: 70.91%\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/1848154066.py:391: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/1848154066.py:391: UserWarning: Matplotlib currently does not support Devanagari natively.\n  plt.tight_layout()\n/tmp/ipykernel_35/1848154066.py:391: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/1848154066.py:391: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/1848154066.py:391: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/1848154066.py:391: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/1848154066.py:391: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/1848154066.py:392: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/1848154066.py:392: UserWarning: Matplotlib currently does not support Devanagari natively.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/1848154066.py:392: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/1848154066.py:392: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/1848154066.py:392: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/1848154066.py:392: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/1848154066.py:392: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/1848154066.py:392: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/1848154066.py:392: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/1848154066.py:392: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x1500 with 9 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nRandom Samples:\nInput: अंकों\nTrue: anakon\nPred: ankon\nCorrect: False\n\nInput: अंगारक\nTrue: angarak\nPred: angarak\nCorrect: True\n\nInput: अंकोर\nTrue: ankor\nPred: ankor\nCorrect: True\n\nInput: अंकों\nTrue: ankon\nPred: ankon\nCorrect: True\n\nInput: अंक\nTrue: ank\nPred: ank\nCorrect: True\n\nInput: अंकोर\nTrue: angkor\nPred: ankor\nCorrect: False\n\nInput: अंगारक\nTrue: angaarak\nPred: angarak\nCorrect: False\n\nInput: अंकों\nTrue: ankhon\nPred: ankon\nCorrect: False\n\nInput: अंकित\nTrue: ankit\nPred: ankit\nCorrect: True\n\nInput: अंक\nTrue: anka\nPred: ank\nCorrect: False\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport random\nimport wandb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.font_manager as fm\nfrom matplotlib.font_manager import FontProperties\n\n# =======================\n# Best Configuration\n# =======================\nbest_config = {\n    'embedding_dim': 256,\n    'hidden_dim': 256,\n    'enc_layers': 2,\n    'dec_layers': 2,\n    'cell_type': 'LSTM',\n    'dropout': 0.5,\n    'epochs': 15,\n    'beam_size': 5,\n    'attention_type': 'concat',\n    'batch_size': 256,\n    'learning_rate': 0.001\n}\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.char2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2char = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for char in text:\n                if char not in self.char2idx:\n                    self.char2idx[char] = self.size\n                    self.idx2char[self.size] = char\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.char2idx[c] for c in text]\n\n    def decode(self, idxs):\n        return ''.join([self.idx2char[i] for i in idxs if i not in [0, 1, 2]])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab, out_vocab, is_test=False):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        if not is_test:\n            inp_vocab.build([p[0] for p in self.pairs])\n            out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        if self.is_test:\n            return torch.tensor(x), lat, dev\n        y = [self.out_vocab.char2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.char2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y), lat, dev\n\ndef collate_fn(batch):\n    if len(batch[0]) == 3:  # Test batch\n        x_batch, lat, dev = zip(*batch)\n        x_lens = [len(x) for x in x_batch]\n        x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n        return x_pad, lat, dev, torch.tensor(x_lens)\n    else:  # Train/val batch\n        x_batch, y_batch, lat, dev = zip(*batch)\n        x_lens = [len(x) for x in x_batch]\n        y_lens = [len(y) for y in y_batch]\n        x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n        y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n        return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens), lat, dev\n\n# =======================\n# Model Components\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        return outputs, hidden\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim, attention_type='general'):\n        super().__init__()\n        self.attention_type = attention_type\n        if attention_type == 'general':\n            self.attn = nn.Linear(hidden_dim, hidden_dim)\n        elif attention_type == 'concat':\n            self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n            self.v = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs, mask=None):\n        batch_size, src_len, hidden_dim = encoder_outputs.size()\n        \n        if self.attention_type == 'general':\n            energy = torch.bmm(encoder_outputs, self.attn(hidden).unsqueeze(2)).squeeze(2)\n        elif self.attention_type == 'concat':\n            hidden_expanded = hidden.unsqueeze(1).repeat(1, src_len, 1)\n            concat = torch.cat((hidden_expanded, encoder_outputs), dim=2)\n            energy = self.v(torch.tanh(self.attn(concat))).squeeze(2)\n        else:  # dot\n            energy = torch.bmm(encoder_outputs, hidden.unsqueeze(2)).squeeze(2)\n        \n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, -1e10)\n        attention_weights = F.softmax(energy, dim=1)\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n        return context, attention_weights\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout, attention_type):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim + hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.attention = Attention(hidden_dim, attention_type)\n        self.out = nn.Linear(hidden_dim * 2, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n        if isinstance(hidden, tuple):  # LSTM\n            attn_hidden = hidden[0][-1]\n        else:  # GRU/RNN\n            attn_hidden = hidden[-1]\n        \n        context, attn_weights = self.attention(attn_hidden, encoder_outputs, mask)\n        embedded = self.embedding(input_token)\n        rnn_input = torch.cat((embedded, context), dim=1).unsqueeze(1)\n        output, hidden = self.rnn(rnn_input, hidden)\n        \n        if isinstance(hidden, tuple):\n            output_hidden = hidden[0][-1]\n        else:\n            output_hidden = hidden[-1]\n        \n        output = torch.cat((output_hidden, context), dim=1)\n        output = self.dropout(output)\n        prediction = self.out(output)\n        return prediction, hidden, attn_weights\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def create_mask(self, src_lens, max_len):\n        batch_size = len(src_lens)\n        mask = torch.zeros(batch_size, max_len, device=self.device)\n        for i, length in enumerate(src_lens):\n            mask[i, :length] = 1\n        return mask\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        src_data, src_lens = src\n        encoder_outputs, enc_hidden = self.encoder(src_data, src_lens)\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n        \n        src_len = encoder_outputs.size(1)\n        mask = self.create_mask(src_lens, src_len)\n\n        if isinstance(enc_hidden, tuple):\n            dec_hidden = enc_hidden\n        else:\n            dec_hidden = enc_hidden\n\n        input_token = trg[:, 0]\n        for t in range(1, trg_len):\n            output, dec_hidden, _ = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n            outputs[:, t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if teacher_force else top1\n            \n        return outputs\n\n    def predict(self, src, src_lens, max_len=30):\n        self.eval()\n        with torch.no_grad():\n            encoder_outputs, enc_hidden = self.encoder(src, src_lens)\n            src_len = encoder_outputs.size(1)\n            mask = self.create_mask(src_lens.tolist(), src_len)\n            \n            if isinstance(enc_hidden, tuple):\n                dec_hidden = enc_hidden\n            else:\n                dec_hidden = enc_hidden\n            \n            input_token = torch.tensor([1], device=self.device)  # <sos> token\n            output_seq = []\n            attention_weights = []\n            \n            for _ in range(max_len):\n                output, dec_hidden, attn_weights = self.decoder(input_token, dec_hidden, encoder_outputs, mask)\n                top1 = output.argmax(1)\n                if top1.item() == 2:  # <eos> token\n                    break\n                output_seq.append(top1.item())\n                attention_weights.append(attn_weights.cpu().numpy())\n                input_token = top1\n                \n        return output_seq, attention_weights\n\n# =======================\n# Training and Evaluation with Word-Level Accuracy\n# =======================\ndef train(model, loader, criterion, optimizer, device, out_vocab):\n    model.train()\n    total_loss = 0\n    total_correct_chars = 0\n    total_chars = 0\n    \n    # Word-level tracking\n    total_correct_words = 0\n    total_words = 0\n    \n    for batch in loader:\n        src, trg, src_lens, _, lat, dev = batch\n        src, trg = src.to(device), trg.to(device)\n        \n        optimizer.zero_grad()\n        output = model((src, src_lens), trg)\n        \n        # Calculate loss\n        output_dim = output.shape[-1]\n        loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        \n        # Calculate character-level accuracy\n        pred = output.argmax(dim=2)\n        mask = (trg[:, 1:] != 0)  # Ignore padding\n        correct_chars = ((pred[:, 1:] == trg[:, 1:]) & mask).sum().item()\n        total_correct_chars += correct_chars\n        total_chars += mask.sum().item()\n        \n        # Calculate word-level accuracy\n        batch_size = trg.size(0)\n        for i in range(batch_size):\n            # Get predicted sequence without padding, sos, eos\n            pred_seq = [idx.item() for idx in pred[i, 1:] if idx.item() not in [0, 1, 2]]\n            # Convert to string\n            pred_word = out_vocab.decode(pred_seq)\n            \n            # Get true word\n            true_word = dev[i]\n            \n            # Increment counts\n            total_words += 1\n            if pred_word == true_word:\n                total_correct_words += 1\n                \n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(loader)\n    char_accuracy = (total_correct_chars / total_chars) * 100 if total_chars > 0 else 0\n    word_accuracy = (total_correct_words / total_words) * 100 if total_words > 0 else 0\n    \n    return avg_loss, char_accuracy, word_accuracy\n\ndef evaluate(model, loader, criterion, device, out_vocab):\n    model.eval()\n    total_loss = 0\n    total_correct_chars = 0\n    total_chars = 0\n    \n    # Word-level tracking\n    total_correct_words = 0\n    total_words = 0\n    \n    with torch.no_grad():\n        for batch in loader:\n            src, trg, src_lens, _, lat, dev = batch\n            src, trg = src.to(device), trg.to(device)\n            \n            output = model((src, src_lens), trg, teacher_forcing_ratio=0)\n            \n            # Calculate loss\n            output_dim = output.shape[-1]\n            loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n            \n            # Calculate character-level accuracy\n            pred = output.argmax(dim=2)\n            mask = (trg[:, 1:] != 0)  # Ignore padding\n            correct_chars = ((pred[:, 1:] == trg[:, 1:]) & mask).sum().item()\n            total_correct_chars += correct_chars\n            total_chars += mask.sum().item()\n            \n            # Calculate word-level accuracy\n            batch_size = trg.size(0)\n            for i in range(batch_size):\n                # Get predicted sequence without padding, sos, eos\n                pred_seq = [idx.item() for idx in pred[i, 1:] if idx.item() not in [0, 1, 2]]\n                # Convert to string\n                pred_word = out_vocab.decode(pred_seq)\n                \n                # Get true word\n                true_word = dev[i]\n                \n                # Increment counts\n                total_words += 1\n                if pred_word == true_word:\n                    total_correct_words += 1\n                    \n            total_loss += loss.item()\n    \n    avg_loss = total_loss / len(loader)\n    char_accuracy = (total_correct_chars / total_chars) * 100 if total_chars > 0 else 0\n    word_accuracy = (total_correct_words / total_words) * 100 if total_words > 0 else 0\n    \n    return avg_loss, char_accuracy, word_accuracy\n\n# =======================\n# Attention Visualization with Custom Font\n# =======================\ndef show_attention_grid(samples, font_path):\n    # Load custom Devanagari font\n    try:\n        if font_path and os.path.exists(font_path):\n            font_prop = FontProperties(fname=font_path, size=12)\n            print(f\"Loaded Devanagari font from {font_path}\")\n        else:\n            font_prop = FontProperties(size=12)\n            print(\"Using default font; Devanagari may not render.\")\n    except Exception as e:\n        print(f\"Font error: {e}\")\n        font_prop = FontProperties(size=12)\n\n    # Create a 3x4 grid for 10 samples (adjust as needed)\n    fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n    axes = axes.flatten()\n\n    for idx, (input_seq, true_output, predicted_output, attentions) in enumerate(samples[:12]):\n        ax = axes[idx]\n        input_chars = list(input_seq)\n        output_chars = list(predicted_output)\n        \n        # Handle cases where attention might be empty\n        if len(attentions) == 0:\n            print(f\"Skipping sample {idx} (no attention weights)\")\n            continue\n        \n        attn_matrix = np.array(attentions).squeeze(1)\n        if attn_matrix.ndim == 1:\n            attn_matrix = attn_matrix.reshape(1, -1)\n        \n        # Ensure the matrix matches the lengths\n        max_input_len = len(input_chars)\n        max_output_len = len(output_chars)\n        attn_matrix = attn_matrix[:max_output_len, :max_input_len]\n        \n        sns.heatmap(\n            attn_matrix,\n            xticklabels=input_chars,\n            yticklabels=output_chars,\n            cmap='viridis',\n            ax=ax,\n            cbar=False\n        )\n        ax.set_yticklabels(ax.get_yticklabels(), fontproperties=font_prop, rotation=0)\n        ax.set_title(f\"In: {input_seq}\\nGT: {true_output}\\nPred: {predicted_output}\", \n                     fontsize=8, fontproperties=font_prop)\n        ax.set_xlabel('Input (Latin)', fontsize=8)\n        ax.set_ylabel('Output (Devanagari)', fontproperties=font_prop, fontsize=8)\n\n    # Hide unused subplots\n    for j in range(len(samples), 12):\n        fig.delaxes(axes[j])\n\n    plt.tight_layout()\n    plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n    wandb.log({\"attention_heatmap\": wandb.Image('attention_heatmap.png')})\n    plt.close()\n\n# =======================\n# Main Execution\n# =======================\ndef main():\n    wandb.init(config=best_config, project=\"dakshina-translit-test\")\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Use the specific devanagiri.ttf file\n    font_path = '/kaggle/input/devnagirihindi/devanagari.ttf'\n    \n    # Check if the font file exists\n    if os.path.exists(font_path):\n        print(f\"Using Devanagari font file: {font_path}\")\n    else:\n        # Try alternative paths\n        alternative_paths = [\n            '/kaggle/input/devnagiridata/devanagiri.ttf',\n            '/kaggle/working/devanagiri.ttf',\n            # Add more potential paths if needed\n        ]\n        \n        for alt_path in alternative_paths:\n            if os.path.exists(alt_path):\n                font_path = alt_path\n                print(f\"Found Devanagari font file at alternative path: {font_path}\")\n                break\n        \n        if not os.path.exists(font_path):\n            print(f\"Warning: Font file not found at {font_path}\")\n            print(\"Searching for any TTF file in Kaggle input directories...\")\n            \n            # Fallback: search for any TTF file\n            found_font = False\n            for root, dirs, files in os.walk('/kaggle/input/devnagirihindi/devanagari.ttf'):\n                for file in files:\n                    if file.endswith('.ttf'):\n                        font_path = os.path.join(root, file)\n                        print(f\"Found alternative font file: {font_path}\")\n                        found_font = True\n                        break\n                if found_font:\n                    break\n            \n            if not found_font:\n                print(\"No TTF font file found in Kaggle input, will use system fonts\")\n                font_path = None\n        \n    # Initialize vocabularies\n    inp_vocab = Vocab()\n    out_vocab = Vocab()\n\n    # Load datasets\n    train_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.train.tsv\", inp_vocab, out_vocab)\n    dev_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.dev.tsv\", inp_vocab, out_vocab)\n    test_data = TransliterationDataset(\"/kaggle/input/devnagiridata/hi.translit.sampled.test.tsv\", inp_vocab, out_vocab, is_test=True)\n\n    print(f\"Train dataset size: {len(train_data)}\")\n    print(f\"Dev dataset size: {len(dev_data)}\")\n    print(f\"Test dataset size: {len(test_data)}\")\n\n    # Create data loaders\n    train_loader = DataLoader(train_data, batch_size=best_config['batch_size'], \n                            shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=best_config['batch_size'],\n                          shuffle=False, collate_fn=collate_fn)\n    # Use batch size of 1 for test to get accurate per-word results\n    test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    # Initialize model\n    encoder = Encoder(inp_vocab.size, best_config['embedding_dim'], \n                     best_config['hidden_dim'], best_config['enc_layers'], \n                     best_config['cell_type'], best_config['dropout'])\n    \n    decoder = Decoder(out_vocab.size, best_config['embedding_dim'],\n                     best_config['hidden_dim'], best_config['dec_layers'],\n                     best_config['cell_type'], best_config['dropout'],\n                     best_config['attention_type'])\n    \n    model = Seq2Seq(encoder, decoder, device).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=best_config['learning_rate'])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    # Print model information\n    print(f\"Encoder vocabulary size: {inp_vocab.size}\")\n    print(f\"Decoder vocabulary size: {out_vocab.size}\")\n    \n    # Count model parameters\n    def count_parameters(model):\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(f\"Model has {count_parameters(model):,} trainable parameters\")\n\n    # Training loop\n    best_val_loss = float('inf')\n    for epoch in range(best_config['epochs']):\n        train_loss, train_char_acc, train_word_acc = train(model, train_loader, criterion, optimizer, device, out_vocab)\n        val_loss, val_char_acc, val_word_acc = evaluate(model, dev_loader, criterion, device, out_vocab)\n        \n        print(f\"\\nEpoch {epoch+1}/{best_config['epochs']}\")\n        print(f\"Train Loss: {train_loss:.4f} | Train Char Acc: {train_char_acc:.2f}% | Train Word Acc: {train_word_acc:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f} | Val Char Acc: {val_char_acc:.2f}% | Val Word Acc: {val_word_acc:.2f}%\")\n\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_char_acc\": train_char_acc,\n            \"train_word_acc\": train_word_acc,\n            \"val_loss\": val_loss,\n            \"val_char_acc\": val_char_acc,\n            \"val_word_acc\": val_word_acc\n        })\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(\"Best model saved!\")\n\n    # Test evaluation\n    print(\"\\nLoading best model for testing...\")\n    if os.path.exists(\"best_model.pth\"):\n        model.load_state_dict(torch.load(\"best_model.pth\"))\n        print(\"Loaded best model from best_model.pth\")\n    else:\n        print(\"No saved model found. Using current model state.\")\n    \n    model.eval()\n    \n    total_char_correct = 0\n    total_chars = 0\n    total_word_correct = 0\n    total_words = 0\n    predictions = []\n    attention_samples = []\n    \n    print(\"\\nEvaluating on full test dataset...\")\n    with torch.no_grad():\n        for i, batch in enumerate(test_loader):\n            src, lat, dev, src_lens = batch\n            src = src.to(device)\n            pred_ids, attention_weights = model.predict(src, src_lens)\n            pred_str = out_vocab.decode(pred_ids)\n            true_str = dev[0]\n            \n            # Word-level accuracy\n            total_words += 1\n            word_correct = pred_str == true_str\n            if word_correct:\n                total_word_correct += 1\n            \n            # Character-level accuracy\n            min_len = min(len(pred_str), len(true_str))\n            for i in range(min_len):\n                total_chars += 1\n                if pred_str[i] == true_str[i]:\n                    total_char_correct += 1\n            # Count remaining characters in longer string as errors\n            total_chars += abs(len(pred_str) - len(true_str))\n            \n            predictions.append({\n                'input': lat[0],\n                'true': true_str,\n                'pred': pred_str,\n                'correct': word_correct\n            })\n            \n            # Collect attention weights for first 10 samples for visualization\n            if len(attention_samples) < 10:\n                attention_samples.append((lat[0], true_str, pred_str, attention_weights))\n            \n            # Show progress for large test sets\n            if (i + 1) % 100 == 0:\n                print(f\"Processed {i + 1}/{len(test_loader)} test samples\")\n\n    # Calculate accuracies\n    word_accuracy = 100 * total_word_correct / total_words if total_words > 0 else 0\n    char_accuracy = 100 * total_char_correct / total_chars if total_chars > 0 else 0\n    \n    print(f\"\\nFull Test Results:\")\n    print(f\"Test Word Accuracy: {word_accuracy:.2f}% ({total_word_correct}/{total_words})\")\n    print(f\"Test Character Accuracy: {char_accuracy:.2f}% ({total_char_correct}/{total_chars})\")\n    \n    wandb.log({\n        \"test_word_acc\": word_accuracy,\n        \"test_char_acc\": char_accuracy,\n        \"total_words_correct\": total_word_correct,\n        \"total_words\": total_words\n    })\n\n    # Visualize attention for 10 samples with custom font\n    if font_path:\n        print(f\"Visualizing attention with custom font: {font_path}\")\n    else:\n        print(\"Visualizing attention with system fonts\")\n    show_attention_grid(attention_samples, font_path)\n\n    # Save and display font sample\n    if font_path and os.path.exists(font_path):\n        # Create a simple visualization of the Devanagari font\n        sample_text = \"देवनागरी हिंदी नमस्ते भारत\"\n        \n        plt.figure(figsize=(12, 3))\n        plt.text(0.5, 0.5, sample_text, \n                 fontproperties=FontProperties(fname=font_path, size=24),\n                 ha='center', va='center')\n        plt.axis('off')\n        plt.title(\"Devanagari Font Sample\")\n        plt.savefig('font_sample.png', dpi=150, bbox_inches='tight')\n        wandb.log({\"font_sample\": wandb.Image('font_sample.png')})\n        plt.close()\n        print(\"Saved font sample visualization\")\n    \n    # Create and log a table of predictions\n    table = wandb.Table(columns=[\"Input\", \"True\", \"Predicted\", \"Correct\"])\n    for p in predictions[:100]:  # Log first 100 predictions\n        table.add_data(p['input'], p['true'], p['pred'], p['correct'])\n    \n    wandb.log({\"predictions\": table})\n    \n    # Save detailed results\n    with open(\"test_results.txt\", \"w\", encoding=\"utf-8\") as f:\n        f.write(f\"Test Word Accuracy: {word_accuracy:.2f}% ({total_word_correct}/{total_words})\\n\")\n        f.write(f\"Test Character Accuracy: {char_accuracy:.2f}% ({total_char_correct}/{total_chars})\\n\\n\")\n        f.write(\"Sample Predictions:\\n\\n\")\n        \n        # Save first 20 and a random sample of 10 other predictions\n        random_samples = random.sample(predictions[20:], min(10, max(0, len(predictions) - 20)))\n        for p in predictions[:20] + random_samples:\n            f.write(f\"Input: {p['input']}\\n\")\n            f.write(f\"True: {p['true']}\\n\")\n            f.write(f\"Pred: {p['pred']}\\n\")\n            f.write(f\"Correct: {p['correct']}\\n\\n\")\n\n    # Print exactly 10 random samples\n    print(\"\\n10 Random Test Samples:\")\n    # Get 10 random indices for consistent set of samples\n    sample_indices = random.sample(range(len(predictions)), min(10, len(predictions)))\n    samples = [predictions[i] for i in sample_indices]\n    \n    for i, sample in enumerate(samples):\n        print(f\"Sample {i+1}:\")\n        print(f\"Input (Latin): {sample['input']}\")\n        print(f\"True (Devanagari): {sample['true']}\")\n        print(f\"Predicted: {sample['pred']}\")\n        print(f\"Correct: {sample['correct']}\\n\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:42:37.124208Z","iopub.execute_input":"2025-05-20T15:42:37.124796Z","iopub.status.idle":"2025-05-20T15:49:33.658515Z","shell.execute_reply.started":"2025-05-20T15:42:37.124771Z","shell.execute_reply":"2025-05-20T15:49:33.657750Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>test_char_acc</td><td>▁</td></tr><tr><td>test_word_acc</td><td>▁</td></tr><tr><td>total_words</td><td>▁</td></tr><tr><td>total_words_correct</td><td>▁</td></tr><tr><td>train_char_acc</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>train_word_acc</td><td>▁</td></tr><tr><td>val_char_acc</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_word_acc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>test_char_acc</td><td>63.43256</td></tr><tr><td>test_word_acc</td><td>19.83563</td></tr><tr><td>total_words</td><td>4502</td></tr><tr><td>total_words_correct</td><td>893</td></tr><tr><td>train_char_acc</td><td>49.24582</td></tr><tr><td>train_loss</td><td>1.66897</td></tr><tr><td>train_word_acc</td><td>5.78454</td></tr><tr><td>val_char_acc</td><td>66.08026</td></tr><tr><td>val_loss</td><td>1.03417</td></tr><tr><td>val_word_acc</td><td>19.98623</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">devoted-surf-24</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/mkq22s4d' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/mkq22s4d</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test</a><br>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_153826-mkq22s4d/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_154237-i4mw4kay</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/i4mw4kay' target=\"_blank\">wobbly-leaf-25</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/i4mw4kay' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/i4mw4kay</a>"},"metadata":{}},{"name":"stdout","text":"Using device: cuda\nUsing Devanagari font file: /kaggle/input/devnagirihindi/devanagari.ttf\nTrain dataset size: 44204\nDev dataset size: 4358\nTest dataset size: 4502\nEncoder vocabulary size: 66\nDecoder vocabulary size: 29\nModel has 2,538,269 trainable parameters\n\nEpoch 1/15\nTrain Loss: 1.6945 | Train Char Acc: 48.13% | Train Word Acc: 4.92%\nVal Loss: 0.9855 | Val Char Acc: 68.49% | Val Word Acc: 25.06%\nBest model saved!\n\nEpoch 2/15\nTrain Loss: 0.8773 | Train Char Acc: 72.82% | Train Word Acc: 19.51%\nVal Loss: 0.8569 | Val Char Acc: 72.60% | Val Word Acc: 35.34%\nBest model saved!\n\nEpoch 3/15\nTrain Loss: 0.7369 | Train Char Acc: 77.28% | Train Word Acc: 21.59%\nVal Loss: 0.8386 | Val Char Acc: 74.12% | Val Word Acc: 38.76%\nBest model saved!\n\nEpoch 4/15\nTrain Loss: 0.6836 | Train Char Acc: 78.76% | Train Word Acc: 18.65%\nVal Loss: 0.8167 | Val Char Acc: 74.57% | Val Word Acc: 38.23%\nBest model saved!\n\nEpoch 5/15\nTrain Loss: 0.6463 | Train Char Acc: 79.86% | Train Word Acc: 18.93%\nVal Loss: 0.8274 | Val Char Acc: 74.97% | Val Word Acc: 41.17%\n\nEpoch 6/15\nTrain Loss: 0.6236 | Train Char Acc: 80.45% | Train Word Acc: 19.18%\nVal Loss: 0.8124 | Val Char Acc: 74.63% | Val Word Acc: 40.04%\nBest model saved!\n\nEpoch 7/15\nTrain Loss: 0.6023 | Train Char Acc: 81.05% | Train Word Acc: 15.74%\nVal Loss: 0.8065 | Val Char Acc: 75.30% | Val Word Acc: 40.48%\nBest model saved!\n\nEpoch 8/15\nTrain Loss: 0.5879 | Train Char Acc: 81.46% | Train Word Acc: 16.23%\nVal Loss: 0.7922 | Val Char Acc: 75.43% | Val Word Acc: 40.84%\nBest model saved!\n\nEpoch 9/15\nTrain Loss: 0.5655 | Train Char Acc: 82.12% | Train Word Acc: 16.29%\nVal Loss: 0.7635 | Val Char Acc: 76.00% | Val Word Acc: 38.99%\nBest model saved!\n\nEpoch 10/15\nTrain Loss: 0.5603 | Train Char Acc: 82.10% | Train Word Acc: 16.47%\nVal Loss: 0.7919 | Val Char Acc: 75.86% | Val Word Acc: 39.51%\n\nEpoch 11/15\nTrain Loss: 0.5420 | Train Char Acc: 82.78% | Train Word Acc: 14.96%\nVal Loss: 0.8242 | Val Char Acc: 76.05% | Val Word Acc: 40.48%\n\nEpoch 12/15\nTrain Loss: 0.5352 | Train Char Acc: 82.82% | Train Word Acc: 15.58%\nVal Loss: 0.7910 | Val Char Acc: 76.30% | Val Word Acc: 33.69%\n\nEpoch 13/15\nTrain Loss: 0.5237 | Train Char Acc: 83.22% | Train Word Acc: 17.66%\nVal Loss: 0.8125 | Val Char Acc: 76.13% | Val Word Acc: 41.72%\n\nEpoch 14/15\nTrain Loss: 0.5217 | Train Char Acc: 83.26% | Train Word Acc: 15.53%\nVal Loss: 0.7760 | Val Char Acc: 76.41% | Val Word Acc: 36.78%\n\nEpoch 15/15\nTrain Loss: 0.5126 | Train Char Acc: 83.48% | Train Word Acc: 17.66%\nVal Loss: 0.7798 | Val Char Acc: 76.25% | Val Word Acc: 41.72%\n\nLoading best model for testing...\nLoaded best model from best_model.pth\n\nEvaluating on full test dataset...\n\nFull Test Results:\nTest Word Accuracy: 41.38% (1863/4502)\nTest Character Accuracy: 72.67% (22780/31346)\nVisualizing attention with custom font: /kaggle/input/devnagirihindi/devanagari.ttf\nLoaded Devanagari font from /kaggle/input/devnagirihindi/devanagari.ttf\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/2485064011.py:408: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/2485064011.py:408: UserWarning: Matplotlib currently does not support Devanagari natively.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/2485064011.py:408: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/2485064011.py:408: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/2485064011.py:408: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/2485064011.py:408: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/2485064011.py:408: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/2485064011.py:408: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/2485064011.py:408: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/2485064011.py:408: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n","output_type":"stream"},{"name":"stdout","text":"Saved font sample visualization\n\n10 Random Test Samples:\nSample 1:\nInput (Latin): विशेषण\nTrue (Devanagari): visheshan\nPredicted: visheshan\nCorrect: True\n\nSample 2:\nInput (Latin): दुखों\nTrue (Devanagari): dukhon\nPredicted: dukhon\nCorrect: True\n\nSample 3:\nInput (Latin): अलंकार\nTrue (Devanagari): alankar\nPredicted: alankaar\nCorrect: False\n\nSample 4:\nInput (Latin): लुमडिंग\nTrue (Devanagari): lumding\nPredicted: lumding\nCorrect: True\n\nSample 5:\nInput (Latin): क्षीण\nTrue (Devanagari): shin\nPredicted: shin\nCorrect: True\n\nSample 6:\nInput (Latin): जताती\nTrue (Devanagari): jataati\nPredicted: jatati\nCorrect: False\n\nSample 7:\nInput (Latin): स्वीट\nTrue (Devanagari): sweet\nPredicted: sweet\nCorrect: True\n\nSample 8:\nInput (Latin): उभारा\nTrue (Devanagari): ubhara\nPredicted: ubhara\nCorrect: True\n\nSample 9:\nInput (Latin): स्लाइस\nTrue (Devanagari): slice\nPredicted: slise\nCorrect: False\n\nSample 10:\nInput (Latin): वंशवाद\nTrue (Devanagari): vanshvaad\nPredicted: vanshvad\nCorrect: False\n\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mwobbly-leaf-25\u001b[0m at: \u001b[34mhttps://wandb.ai/manglesh_dl_ass3/dakshina-translit-test/runs/i4mw4kay\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250520_154237-i4mw4kay/logs\u001b[0m\n","output_type":"stream"}],"execution_count":9}]}