{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11851130,"sourceType":"datasetVersion","datasetId":7446697}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"  !pip install wandb\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wandb login 58a0b576fd5221cd0d63b154deaabbe535e853c6","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CHARACTER LEVEL WITHOUT ATTENTION","metadata":{}},{"cell_type":"code","source":"# =======================\n# Imports and Sweep Config\n# =======================\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport wandb\nimport os\nimport math\n\nsweep_config = {\n    'method': 'bayes',\n    'metric': {\n        'name': 'val_loss',\n        'goal': 'minimize'\n    },\n    'parameters': {\n        'embedding_dim': {'values': [64,128,256]},\n        'hidden_dim': {'values': [64, 128,256 ]},\n        'enc_layers': {'values': [1, 2,3]},\n        'dec_layers': {'values': [1, 2,3]},\n        'cell_type': {'values': ['GRU', 'LSTM', 'RNN']},\n        'dropout': {'values': [0.2, 0.3,0.5]},\n        'epochs': {'values': [20, 15]},\n        'beam_size': {'values': [1, 3, 5]},\n        'batch_size': {'values': [64, 128, 256]},\n        'learning_rate': {'values': [0.001, 0.0005, 0.0001]}\n    }\n}\n\ndefault_config = {\n    'embedding_dim': 32,\n    'hidden_dim': 64,\n    'enc_layers': 1,\n    'dec_layers': 1,\n    'cell_type': 'LSTM',\n    'dropout': 0.2,\n    'epochs': 10,\n    'beam_size': 1,\n    'batch_size': 64,\n    'learning_rate': 0.001\n}\n\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.char2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2char = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for char in text:\n                if char not in self.char2idx:\n                    self.char2idx[char] = self.size\n                    self.idx2char[self.size] = char\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.char2idx[c] for c in text]\n\n    def decode(self, idxs):\n        return ''.join([self.idx2char[i] for i in idxs if i > 2])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab, out_vocab):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        inp_vocab.build([p[0] for p in self.pairs])\n        out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        y = [self.out_vocab.char2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.char2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y)\n\ndef collate_fn(batch):\n    x_batch, y_batch = zip(*batch)\n    x_lens = [len(x) for x in x_batch]\n    y_lens = [len(y) for y in y_batch]\n    x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n    y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n    return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens)\n\n# =======================\n# Encoder and Decoder\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.out = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, input_token, hidden):\n        embedded = self.embedding(input_token).unsqueeze(1)\n        output, hidden = self.rnn(embedded, hidden)\n        output = self.out(output.squeeze(1))\n        return output, hidden\n\n# =======================\n# Seq2Seq Model with Beam Search\n# =======================\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, enc_layers, dec_layers, cell_type, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.cell_type = cell_type\n        self.enc_layers = enc_layers\n        self.dec_layers = dec_layers\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n        enc_hidden = self.encoder(src[0], src[1])\n\n        if self.cell_type == \"LSTM\":\n            h, c = enc_hidden\n            h = self._match_layers(h)\n            c = self._match_layers(c)\n            dec_hidden = (h, c)\n        else:\n            dec_hidden = self._match_layers(enc_hidden)\n\n        input_token = trg[:, 0]\n        for t in range(1, trg_len):\n            output, dec_hidden = self.decoder(input_token, dec_hidden)\n            outputs[:, t] = output\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n        return outputs\n\n    def _match_layers(self, hidden):\n        if self.enc_layers == self.dec_layers:\n            return hidden\n        elif self.enc_layers > self.dec_layers:\n            return hidden[:self.dec_layers]\n        else:\n            pad = hidden.new_zeros((self.dec_layers - self.enc_layers, *hidden.shape[1:]))\n            return torch.cat([hidden, pad], dim=0)\n\n# =======================\n# Train & Eval\n# =======================\ndef train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss, total_correct, total_count = 0, 0, 0\n    for src, trg, src_lens, _ in loader:\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model((src, src_lens), trg)\n        output_dim = output.shape[-1]\n        loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n        pred = output.argmax(2)\n        correct = ((pred[:, 1:] == trg[:, 1:]) & (trg[:, 1:] != 0)).sum().item()\n        total_correct += correct\n        total_count += (trg[:, 1:] != 0).sum().item()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    acc = 100.0 * total_correct / total_count\n    print(f\"Train Loss: {total_loss / len(loader):.4f}, Acc: {acc:.2f}%\")\n    return total_loss / len(loader), acc\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss, total_correct, total_count = 0, 0, 0\n    with torch.no_grad():\n        for src, trg, src_lens, _ in loader:\n            src, trg = src.to(device), trg.to(device)\n            output = model((src, src_lens), trg, teacher_forcing_ratio=0)\n            output_dim = output.shape[-1]\n            loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n            pred = output.argmax(2)\n            correct = ((pred[:, 1:] == trg[:, 1:]) & (trg[:, 1:] != 0)).sum().item()\n            total_correct += correct\n            total_count += (trg[:, 1:] != 0).sum().item()\n            total_loss += loss.item()\n    acc = 100.0 * total_correct / total_count\n    print(f\"Val Loss: {total_loss / len(loader):.4f}, Acc: {acc:.2f}%\")\n    return total_loss / len(loader), acc\n\n# =======================\n# Main\n# =======================\ndef main():\n    wandb.init(config=default_config, project=\"dakshina-transliteration\")\n    config = wandb.config\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    inp_vocab, out_vocab = Vocab(), Vocab()\n    train_path = \"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n    dev_path = \"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n    train_data = TransliterationDataset(train_path, inp_vocab, out_vocab)\n    dev_data = TransliterationDataset(dev_path, inp_vocab, out_vocab)\n    \n    # Use config.batch_size for DataLoader\n    train_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(inp_vocab.size, config.embedding_dim, config.hidden_dim, config.enc_layers, config.cell_type, config.dropout)\n    decoder = Decoder(out_vocab.size, config.embedding_dim, config.hidden_dim, config.dec_layers, config.cell_type, config.dropout)\n    model = Seq2Seq(encoder, decoder, config.enc_layers, config.dec_layers, config.cell_type, device).to(device)\n    \n    # Use config.learning_rate for optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    for epoch in range(config.epochs):\n        print(f\"Epoch {epoch+1}/{config.epochs}\")\n        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n        wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc, \"epoch\": epoch+1})\n\n# =======================\n\nif __name__ == '__main__':\n    sweep_id = wandb.sweep(sweep_config, project=\"dakshina-translit\")\n    wandb.agent(sweep_id, function=main,count=30)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T15:43:07.385184Z","iopub.execute_input":"2025-05-18T15:43:07.385441Z","iopub.status.idle":"2025-05-18T17:17:26.551419Z","shell.execute_reply.started":"2025-05-18T15:43:07.385415Z","shell.execute_reply":"2025-05-18T17:17:26.550739Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: q6l1non5\nSweep URL: https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t64a62c3 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanglesh_dlass3\u001b[0m (\u001b[33mmanglesh_dl_ass3\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_154328-t64a62c3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/t64a62c3' target=\"_blank\">avid-sweep-1</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/t64a62c3' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/t64a62c3</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 1.9554, Acc: 42.42%\nVal Loss: 1.7184, Acc: 46.60%\nEpoch 2/20\nTrain Loss: 1.5413, Acc: 52.60%\nVal Loss: 1.5627, Acc: 50.22%\nEpoch 3/20\nTrain Loss: 1.3921, Acc: 56.85%\nVal Loss: 1.4492, Acc: 53.92%\nEpoch 4/20\nTrain Loss: 1.3088, Acc: 59.20%\nVal Loss: 1.3864, Acc: 55.18%\nEpoch 5/20\nTrain Loss: 1.2589, Acc: 60.54%\nVal Loss: 1.3533, Acc: 56.45%\nEpoch 6/20\nTrain Loss: 1.2227, Acc: 61.63%\nVal Loss: 1.3185, Acc: 57.52%\nEpoch 7/20\nTrain Loss: 1.1909, Acc: 62.43%\nVal Loss: 1.2978, Acc: 57.83%\nEpoch 8/20\nTrain Loss: 1.1754, Acc: 62.80%\nVal Loss: 1.3139, Acc: 57.95%\nEpoch 9/20\nTrain Loss: 1.1584, Acc: 63.26%\nVal Loss: 1.3047, Acc: 58.31%\nEpoch 10/20\nTrain Loss: 1.1579, Acc: 63.28%\nVal Loss: 1.2658, Acc: 58.91%\nEpoch 11/20\nTrain Loss: 1.1431, Acc: 63.65%\nVal Loss: 1.2666, Acc: 58.62%\nEpoch 12/20\nTrain Loss: 1.1346, Acc: 63.93%\nVal Loss: 1.2665, Acc: 59.07%\nEpoch 13/20\nTrain Loss: 1.1267, Acc: 64.30%\nVal Loss: 1.2796, Acc: 58.97%\nEpoch 14/20\nTrain Loss: 1.1270, Acc: 64.23%\nVal Loss: 1.2751, Acc: 58.30%\nEpoch 15/20\nTrain Loss: 1.1248, Acc: 64.35%\nVal Loss: 1.2468, Acc: 59.49%\nEpoch 16/20\nTrain Loss: 1.1172, Acc: 64.61%\nVal Loss: 1.2565, Acc: 59.42%\nEpoch 17/20\nTrain Loss: 1.1262, Acc: 64.29%\nVal Loss: 1.2558, Acc: 59.29%\nEpoch 18/20\nTrain Loss: 1.1209, Acc: 64.37%\nVal Loss: 1.2533, Acc: 59.64%\nEpoch 19/20\nTrain Loss: 1.1233, Acc: 64.39%\nVal Loss: 1.2715, Acc: 58.66%\nEpoch 20/20\nTrain Loss: 1.1194, Acc: 64.44%\nVal Loss: 1.2734, Acc: 58.95%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▄▆▆▇▇▇▇████████████</td></tr><tr><td>train_loss</td><td>█▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▅▆▆▇▇▇▇█▇██▇████▇█</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_acc</td><td>64.44335</td></tr><tr><td>train_loss</td><td>1.11939</td></tr><tr><td>val_acc</td><td>58.94795</td></tr><tr><td>val_loss</td><td>1.27341</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">avid-sweep-1</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/t64a62c3' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/t64a62c3</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_154328-t64a62c3/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 43bhqd7m with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_155402-43bhqd7m</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/43bhqd7m' target=\"_blank\">legendary-sweep-2</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/43bhqd7m' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/43bhqd7m</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 1.2089, Acc: 62.40%\nVal Loss: 1.0044, Acc: 66.71%\nEpoch 2/20\nTrain Loss: 0.7046, Acc: 77.35%\nVal Loss: 0.9199, Acc: 70.11%\nEpoch 3/20\nTrain Loss: 0.6128, Acc: 80.12%\nVal Loss: 0.8909, Acc: 70.57%\nEpoch 4/20\nTrain Loss: 0.5654, Acc: 81.55%\nVal Loss: 0.8812, Acc: 71.34%\nEpoch 5/20\nTrain Loss: 0.5351, Acc: 82.43%\nVal Loss: 0.8653, Acc: 72.36%\nEpoch 6/20\nTrain Loss: 0.5084, Acc: 83.16%\nVal Loss: 0.8699, Acc: 71.60%\nEpoch 7/20\nTrain Loss: 0.4888, Acc: 83.65%\nVal Loss: 0.8993, Acc: 72.44%\nEpoch 8/20\nTrain Loss: 0.4742, Acc: 84.01%\nVal Loss: 0.8733, Acc: 72.06%\nEpoch 9/20\nTrain Loss: 0.4585, Acc: 84.46%\nVal Loss: 0.8709, Acc: 72.51%\nEpoch 10/20\nTrain Loss: 0.4489, Acc: 84.60%\nVal Loss: 0.8825, Acc: 72.18%\nEpoch 11/20\nTrain Loss: 0.4411, Acc: 84.71%\nVal Loss: 0.8838, Acc: 71.96%\nEpoch 12/20\nTrain Loss: 0.4308, Acc: 85.04%\nVal Loss: 0.8976, Acc: 72.58%\nEpoch 13/20\nTrain Loss: 0.4262, Acc: 85.14%\nVal Loss: 0.8819, Acc: 72.63%\nEpoch 14/20\nTrain Loss: 0.4173, Acc: 85.30%\nVal Loss: 0.8973, Acc: 71.99%\nEpoch 15/20\nTrain Loss: 0.4168, Acc: 85.29%\nVal Loss: 0.8640, Acc: 72.54%\nEpoch 16/20\nTrain Loss: 0.4033, Acc: 85.69%\nVal Loss: 0.9230, Acc: 72.36%\nEpoch 17/20\nTrain Loss: 0.4059, Acc: 85.49%\nVal Loss: 0.9040, Acc: 72.23%\nEpoch 18/20\nTrain Loss: 0.3992, Acc: 85.64%\nVal Loss: 0.9062, Acc: 72.65%\nEpoch 19/20\nTrain Loss: 0.3914, Acc: 85.91%\nVal Loss: 0.9174, Acc: 72.37%\nEpoch 20/20\nTrain Loss: 0.3866, Acc: 86.03%\nVal Loss: 0.9294, Acc: 72.85%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇▇████████████</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▅▆▇▇█▇█▇▇██▇█▇▇█▇█</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▃▁▁▂▂▃▂▃▁▄▃▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_acc</td><td>86.02775</td></tr><tr><td>train_loss</td><td>0.38659</td></tr><tr><td>val_acc</td><td>72.84801</td></tr><tr><td>val_loss</td><td>0.92941</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">legendary-sweep-2</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/43bhqd7m' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/43bhqd7m</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_155402-43bhqd7m/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e7g40ay4 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_160341-e7g40ay4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/e7g40ay4' target=\"_blank\">eager-sweep-3</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/e7g40ay4' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/e7g40ay4</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 2.4500, Acc: 28.85%\nVal Loss: 2.4601, Acc: 28.56%\nEpoch 2/20\nTrain Loss: 2.3409, Acc: 31.34%\nVal Loss: 2.4687, Acc: 28.86%\nEpoch 3/20\nTrain Loss: 2.3143, Acc: 31.78%\nVal Loss: 2.4092, Acc: 29.53%\nEpoch 4/20\nTrain Loss: 2.2936, Acc: 32.03%\nVal Loss: 2.4002, Acc: 29.70%\nEpoch 5/20\nTrain Loss: 2.2704, Acc: 32.39%\nVal Loss: 2.3694, Acc: 29.94%\nEpoch 6/20\nTrain Loss: 2.2433, Acc: 33.27%\nVal Loss: 2.3417, Acc: 30.40%\nEpoch 7/20\nTrain Loss: 2.2204, Acc: 33.78%\nVal Loss: 2.3271, Acc: 30.81%\nEpoch 8/20\nTrain Loss: 2.2067, Acc: 34.07%\nVal Loss: 2.3166, Acc: 31.04%\nEpoch 9/20\nTrain Loss: 2.1921, Acc: 34.47%\nVal Loss: 2.3318, Acc: 30.08%\nEpoch 10/20\nTrain Loss: 2.1785, Acc: 34.84%\nVal Loss: 2.2838, Acc: 31.51%\nEpoch 11/20\nTrain Loss: 2.1686, Acc: 35.11%\nVal Loss: 2.2786, Acc: 31.32%\nEpoch 12/20\nTrain Loss: 2.1576, Acc: 35.43%\nVal Loss: 2.2727, Acc: 31.99%\nEpoch 13/20\nTrain Loss: 2.1538, Acc: 35.70%\nVal Loss: 2.2715, Acc: 31.96%\nEpoch 14/20\nTrain Loss: 2.1487, Acc: 35.84%\nVal Loss: 2.2751, Acc: 31.81%\nEpoch 15/20\nTrain Loss: 2.1426, Acc: 36.00%\nVal Loss: 2.2852, Acc: 31.37%\nEpoch 16/20\nTrain Loss: 2.1409, Acc: 36.03%\nVal Loss: 2.2805, Acc: 31.42%\nEpoch 17/20\nTrain Loss: 2.1355, Acc: 36.10%\nVal Loss: 2.2587, Acc: 31.93%\nEpoch 18/20\nTrain Loss: 2.1317, Acc: 36.25%\nVal Loss: 2.2599, Acc: 32.09%\nEpoch 19/20\nTrain Loss: 2.1251, Acc: 36.45%\nVal Loss: 2.2503, Acc: 32.70%\nEpoch 20/20\nTrain Loss: 2.1175, Acc: 36.60%\nVal Loss: 2.2687, Acc: 31.78%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▃▄▄▄▅▅▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▃▃▃▄▅▅▄▆▆▇▇▆▆▆▇▇█▆</td></tr><tr><td>val_loss</td><td>██▆▆▅▄▃▃▄▂▂▂▂▂▂▂▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_acc</td><td>36.59542</td></tr><tr><td>train_loss</td><td>2.11748</td></tr><tr><td>val_acc</td><td>31.78149</td></tr><tr><td>val_loss</td><td>2.26875</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">eager-sweep-3</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/e7g40ay4' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/e7g40ay4</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_160341-e7g40ay4/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c7io9ymw with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_161440-c7io9ymw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/c7io9ymw' target=\"_blank\">glamorous-sweep-4</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/c7io9ymw' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/c7io9ymw</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 1.5128, Acc: 53.67%\nVal Loss: 1.0247, Acc: 66.20%\nEpoch 2/20\nTrain Loss: 0.7835, Acc: 74.70%\nVal Loss: 0.8963, Acc: 70.52%\nEpoch 3/20\nTrain Loss: 0.6661, Acc: 78.44%\nVal Loss: 0.8549, Acc: 71.99%\nEpoch 4/20\nTrain Loss: 0.6077, Acc: 80.13%\nVal Loss: 0.8247, Acc: 73.12%\nEpoch 5/20\nTrain Loss: 0.5660, Acc: 81.45%\nVal Loss: 0.8111, Acc: 73.62%\nEpoch 6/20\nTrain Loss: 0.5420, Acc: 82.15%\nVal Loss: 0.7976, Acc: 73.75%\nEpoch 7/20\nTrain Loss: 0.5224, Acc: 82.66%\nVal Loss: 0.8217, Acc: 73.50%\nEpoch 8/20\nTrain Loss: 0.4994, Acc: 83.36%\nVal Loss: 0.8166, Acc: 74.01%\nEpoch 9/20\nTrain Loss: 0.4875, Acc: 83.70%\nVal Loss: 0.8052, Acc: 74.29%\nEpoch 10/20\nTrain Loss: 0.4792, Acc: 83.79%\nVal Loss: 0.7925, Acc: 74.62%\nEpoch 11/20\nTrain Loss: 0.4688, Acc: 84.05%\nVal Loss: 0.8034, Acc: 74.40%\nEpoch 12/20\nTrain Loss: 0.4586, Acc: 84.36%\nVal Loss: 0.8043, Acc: 74.71%\nEpoch 13/20\nTrain Loss: 0.4470, Acc: 84.70%\nVal Loss: 0.7969, Acc: 74.60%\nEpoch 14/20\nTrain Loss: 0.4453, Acc: 84.69%\nVal Loss: 0.8088, Acc: 74.39%\nEpoch 15/20\nTrain Loss: 0.4346, Acc: 84.97%\nVal Loss: 0.8122, Acc: 74.66%\nEpoch 16/20\nTrain Loss: 0.4282, Acc: 85.16%\nVal Loss: 0.7950, Acc: 74.96%\nEpoch 17/20\nTrain Loss: 0.4219, Acc: 85.29%\nVal Loss: 0.8136, Acc: 74.75%\nEpoch 18/20\nTrain Loss: 0.4170, Acc: 85.41%\nVal Loss: 0.8113, Acc: 74.73%\nEpoch 19/20\nTrain Loss: 0.4132, Acc: 85.48%\nVal Loss: 0.7943, Acc: 75.13%\nEpoch 20/20\nTrain Loss: 0.4123, Acc: 85.39%\nVal Loss: 0.8161, Acc: 74.77%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▆▆▇▇▇▇█████████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▆▇▇▇▇▇█▇██▇██████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▂▂▁▁▁▁▁▁▂▁▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_acc</td><td>85.3929</td></tr><tr><td>train_loss</td><td>0.41227</td></tr><tr><td>val_acc</td><td>74.77214</td></tr><tr><td>val_loss</td><td>0.81614</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">glamorous-sweep-4</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/c7io9ymw' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/c7io9ymw</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_161440-c7io9ymw/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c85kqv7k with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_162724-c85kqv7k</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/c85kqv7k' target=\"_blank\">iconic-sweep-5</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/c85kqv7k' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/c85kqv7k</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 1.1142, Acc: 64.86%\nVal Loss: 1.0020, Acc: 67.36%\nEpoch 2/20\nTrain Loss: 0.6762, Acc: 78.09%\nVal Loss: 0.8703, Acc: 71.29%\nEpoch 3/20\nTrain Loss: 0.5970, Acc: 80.56%\nVal Loss: 0.8354, Acc: 72.93%\nEpoch 4/20\nTrain Loss: 0.5581, Acc: 81.70%\nVal Loss: 0.8565, Acc: 72.84%\nEpoch 5/20\nTrain Loss: 0.5316, Acc: 82.40%\nVal Loss: 0.8047, Acc: 73.68%\nEpoch 6/20\nTrain Loss: 0.5105, Acc: 83.05%\nVal Loss: 0.8650, Acc: 73.44%\nEpoch 7/20\nTrain Loss: 0.4946, Acc: 83.45%\nVal Loss: 0.8216, Acc: 73.90%\nEpoch 8/20\nTrain Loss: 0.4833, Acc: 83.69%\nVal Loss: 0.8341, Acc: 74.03%\nEpoch 9/20\nTrain Loss: 0.4706, Acc: 84.07%\nVal Loss: 0.8223, Acc: 74.26%\nEpoch 10/20\nTrain Loss: 0.4656, Acc: 84.13%\nVal Loss: 0.8191, Acc: 74.33%\nEpoch 11/20\nTrain Loss: 0.4578, Acc: 84.32%\nVal Loss: 0.8504, Acc: 73.84%\nEpoch 12/20\nTrain Loss: 0.4480, Acc: 84.67%\nVal Loss: 0.8551, Acc: 74.53%\nEpoch 13/20\nTrain Loss: 0.4431, Acc: 84.73%\nVal Loss: 0.8584, Acc: 74.01%\nEpoch 14/20\nTrain Loss: 0.4395, Acc: 84.85%\nVal Loss: 0.8334, Acc: 74.40%\nEpoch 15/20\nTrain Loss: 0.4376, Acc: 84.85%\nVal Loss: 0.8332, Acc: 74.33%\nEpoch 16/20\nTrain Loss: 0.4325, Acc: 84.96%\nVal Loss: 0.8591, Acc: 74.43%\nEpoch 17/20\nTrain Loss: 0.4313, Acc: 84.96%\nVal Loss: 0.8191, Acc: 74.29%\nEpoch 18/20\nTrain Loss: 0.4297, Acc: 85.06%\nVal Loss: 0.8194, Acc: 74.23%\nEpoch 19/20\nTrain Loss: 0.4264, Acc: 85.11%\nVal Loss: 0.8471, Acc: 74.27%\nEpoch 20/20\nTrain Loss: 0.4244, Acc: 85.10%\nVal Loss: 0.8437, Acc: 74.21%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▆▆▇▇▇▇█████████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▆▇▇▇███▇█▇███████</td></tr><tr><td>val_loss</td><td>█▃▂▃▁▃▂▂▂▂▃▃▃▂▂▃▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_acc</td><td>85.10375</td></tr><tr><td>train_loss</td><td>0.4244</td></tr><tr><td>val_acc</td><td>74.21371</td></tr><tr><td>val_loss</td><td>0.84367</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">iconic-sweep-5</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/c85kqv7k' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/c85kqv7k</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_162724-c85kqv7k/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uusrluco with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_163944-uusrluco</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/uusrluco' target=\"_blank\">amber-sweep-6</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/uusrluco' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/uusrluco</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.2784, Acc: 60.16%\nVal Loss: 0.9592, Acc: 67.37%\nEpoch 2/15\nTrain Loss: 0.7300, Acc: 76.16%\nVal Loss: 0.8863, Acc: 70.76%\nEpoch 3/15\nTrain Loss: 0.6225, Acc: 79.61%\nVal Loss: 0.8220, Acc: 73.20%\nEpoch 4/15\nTrain Loss: 0.5709, Acc: 81.22%\nVal Loss: 0.7955, Acc: 74.00%\nEpoch 5/15\nTrain Loss: 0.5343, Acc: 82.26%\nVal Loss: 0.8036, Acc: 74.23%\nEpoch 6/15\nTrain Loss: 0.5075, Acc: 83.06%\nVal Loss: 0.8155, Acc: 74.17%\nEpoch 7/15\nTrain Loss: 0.4901, Acc: 83.58%\nVal Loss: 0.8072, Acc: 74.55%\nEpoch 8/15\nTrain Loss: 0.4723, Acc: 84.02%\nVal Loss: 0.7788, Acc: 75.01%\nEpoch 9/15\nTrain Loss: 0.4612, Acc: 84.25%\nVal Loss: 0.7711, Acc: 74.84%\nEpoch 10/15\nTrain Loss: 0.4470, Acc: 84.72%\nVal Loss: 0.7934, Acc: 74.72%\nEpoch 11/15\nTrain Loss: 0.4407, Acc: 84.77%\nVal Loss: 0.7924, Acc: 74.49%\nEpoch 12/15\nTrain Loss: 0.4334, Acc: 84.91%\nVal Loss: 0.7796, Acc: 74.98%\nEpoch 13/15\nTrain Loss: 0.4254, Acc: 85.13%\nVal Loss: 0.7988, Acc: 75.26%\nEpoch 14/15\nTrain Loss: 0.4186, Acc: 85.32%\nVal Loss: 0.7903, Acc: 75.23%\nEpoch 15/15\nTrain Loss: 0.4127, Acc: 85.44%\nVal Loss: 0.7851, Acc: 74.98%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▇▇▇▇███▇████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▃▂▁▁▂▂▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>85.44449</td></tr><tr><td>train_loss</td><td>0.41267</td></tr><tr><td>val_acc</td><td>74.97758</td></tr><tr><td>val_loss</td><td>0.78508</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">amber-sweep-6</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/uusrluco' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/uusrluco</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_163944-uusrluco/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: auhmazwj with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_164948-auhmazwj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/auhmazwj' target=\"_blank\">dashing-sweep-7</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/auhmazwj' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/auhmazwj</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.2531, Acc: 61.12%\nVal Loss: 0.9923, Acc: 66.54%\nEpoch 2/15\nTrain Loss: 0.7041, Acc: 77.19%\nVal Loss: 0.9107, Acc: 70.33%\nEpoch 3/15\nTrain Loss: 0.5975, Acc: 80.67%\nVal Loss: 0.8480, Acc: 71.72%\nEpoch 4/15\nTrain Loss: 0.5443, Acc: 82.15%\nVal Loss: 0.8583, Acc: 72.36%\nEpoch 5/15\nTrain Loss: 0.5079, Acc: 83.24%\nVal Loss: 0.8587, Acc: 72.00%\nEpoch 6/15\nTrain Loss: 0.4793, Acc: 84.04%\nVal Loss: 0.8415, Acc: 72.81%\nEpoch 7/15\nTrain Loss: 0.4575, Acc: 84.58%\nVal Loss: 0.8272, Acc: 72.99%\nEpoch 8/15\nTrain Loss: 0.4458, Acc: 84.80%\nVal Loss: 0.8464, Acc: 72.68%\nEpoch 9/15\nTrain Loss: 0.4274, Acc: 85.30%\nVal Loss: 0.8649, Acc: 73.26%\nEpoch 10/15\nTrain Loss: 0.4155, Acc: 85.53%\nVal Loss: 0.8249, Acc: 72.98%\nEpoch 11/15\nTrain Loss: 0.4023, Acc: 85.88%\nVal Loss: 0.8622, Acc: 73.24%\nEpoch 12/15\nTrain Loss: 0.3925, Acc: 86.01%\nVal Loss: 0.8631, Acc: 73.50%\nEpoch 13/15\nTrain Loss: 0.3876, Acc: 86.09%\nVal Loss: 0.8518, Acc: 73.48%\nEpoch 14/15\nTrain Loss: 0.3782, Acc: 86.32%\nVal Loss: 0.8308, Acc: 73.46%\nEpoch 15/15\nTrain Loss: 0.3703, Acc: 86.47%\nVal Loss: 0.8612, Acc: 73.47%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▇▆▇▇▇█▇█████</td></tr><tr><td>val_loss</td><td>█▅▂▂▂▂▁▂▃▁▃▃▂▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>86.46588</td></tr><tr><td>train_loss</td><td>0.37031</td></tr><tr><td>val_acc</td><td>73.47299</td></tr><tr><td>val_loss</td><td>0.86123</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dashing-sweep-7</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/auhmazwj' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/auhmazwj</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_164948-auhmazwj/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 06wd9s9q with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_165826-06wd9s9q</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/06wd9s9q' target=\"_blank\">stellar-sweep-8</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/06wd9s9q' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/06wd9s9q</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.2404, Acc: 61.32%\nVal Loss: 1.0299, Acc: 66.60%\nEpoch 2/15\nTrain Loss: 0.7024, Acc: 77.37%\nVal Loss: 0.9366, Acc: 69.63%\nEpoch 3/15\nTrain Loss: 0.5955, Acc: 80.70%\nVal Loss: 0.9018, Acc: 71.22%\nEpoch 4/15\nTrain Loss: 0.5368, Acc: 82.48%\nVal Loss: 0.8550, Acc: 72.43%\nEpoch 5/15\nTrain Loss: 0.5084, Acc: 83.21%\nVal Loss: 0.8227, Acc: 73.14%\nEpoch 6/15\nTrain Loss: 0.4794, Acc: 84.07%\nVal Loss: 0.8357, Acc: 72.74%\nEpoch 7/15\nTrain Loss: 0.4645, Acc: 84.33%\nVal Loss: 0.8322, Acc: 73.36%\nEpoch 8/15\nTrain Loss: 0.4445, Acc: 84.87%\nVal Loss: 0.8412, Acc: 72.96%\nEpoch 9/15\nTrain Loss: 0.4277, Acc: 85.29%\nVal Loss: 0.8504, Acc: 73.55%\nEpoch 10/15\nTrain Loss: 0.4206, Acc: 85.30%\nVal Loss: 0.8272, Acc: 73.47%\nEpoch 11/15\nTrain Loss: 0.4021, Acc: 85.83%\nVal Loss: 0.8659, Acc: 72.86%\nEpoch 12/15\nTrain Loss: 0.3951, Acc: 86.00%\nVal Loss: 0.8559, Acc: 72.98%\nEpoch 13/15\nTrain Loss: 0.3908, Acc: 86.03%\nVal Loss: 0.8343, Acc: 73.26%\nEpoch 14/15\nTrain Loss: 0.3788, Acc: 86.35%\nVal Loss: 0.8614, Acc: 73.39%\nEpoch 15/15\nTrain Loss: 0.3782, Acc: 86.27%\nVal Loss: 0.8477, Acc: 73.61%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▇█▇█▇██▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▂▁▁▁▂▂▁▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>86.26999</td></tr><tr><td>train_loss</td><td>0.37821</td></tr><tr><td>val_acc</td><td>73.61477</td></tr><tr><td>val_loss</td><td>0.84769</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">stellar-sweep-8</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/06wd9s9q' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/06wd9s9q</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_165826-06wd9s9q/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wwp9svdf with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_170614-wwp9svdf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/wwp9svdf' target=\"_blank\">good-sweep-9</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/wwp9svdf' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/wwp9svdf</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.3025, Acc: 58.90%\nVal Loss: 0.9886, Acc: 67.72%\nEpoch 2/15\nTrain Loss: 0.8000, Acc: 74.11%\nVal Loss: 0.8877, Acc: 71.44%\nEpoch 3/15\nTrain Loss: 0.7076, Acc: 77.02%\nVal Loss: 0.8823, Acc: 71.97%\nEpoch 4/15\nTrain Loss: 0.6651, Acc: 78.39%\nVal Loss: 0.8240, Acc: 72.54%\nEpoch 5/15\nTrain Loss: 0.6355, Acc: 79.34%\nVal Loss: 0.8382, Acc: 73.07%\nEpoch 6/15\nTrain Loss: 0.6140, Acc: 80.00%\nVal Loss: 0.8411, Acc: 73.22%\nEpoch 7/15\nTrain Loss: 0.6091, Acc: 80.12%\nVal Loss: 0.8301, Acc: 73.74%\nEpoch 8/15\nTrain Loss: 0.5941, Acc: 80.48%\nVal Loss: 0.8555, Acc: 73.52%\nEpoch 9/15\nTrain Loss: 0.5839, Acc: 80.81%\nVal Loss: 0.8194, Acc: 73.89%\nEpoch 10/15\nTrain Loss: 0.5770, Acc: 80.96%\nVal Loss: 0.7889, Acc: 74.27%\nEpoch 11/15\nTrain Loss: 0.5721, Acc: 81.22%\nVal Loss: 0.8292, Acc: 74.45%\nEpoch 12/15\nTrain Loss: 0.5699, Acc: 81.22%\nVal Loss: 0.8168, Acc: 73.96%\nEpoch 13/15\nTrain Loss: 0.5597, Acc: 81.57%\nVal Loss: 0.8346, Acc: 73.96%\nEpoch 14/15\nTrain Loss: 0.5593, Acc: 81.63%\nVal Loss: 0.8127, Acc: 74.33%\nEpoch 15/15\nTrain Loss: 0.5629, Acc: 81.38%\nVal Loss: 0.8034, Acc: 74.55%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▅▆▆▇▇▇▇██▇▇██</td></tr><tr><td>val_loss</td><td>█▄▄▂▃▃▂▃▂▁▂▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>81.38486</td></tr><tr><td>train_loss</td><td>0.56288</td></tr><tr><td>val_acc</td><td>74.55224</td></tr><tr><td>val_loss</td><td>0.80338</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">good-sweep-9</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/wwp9svdf' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/wwp9svdf</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_170614-wwp9svdf/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1zswwpkt with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_171708-1zswwpkt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt' target=\"_blank\">vibrant-sweep-10</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.2082, Acc: 62.37%\nVal Loss: 1.0177, Acc: 66.39%\nEpoch 2/20\nTrain Loss: 0.7004, Acc: 77.47%\nVal Loss: 0.9014, Acc: 70.19%\nEpoch 3/20\nTrain Loss: 0.6098, Acc: 80.10%\nVal Loss: 0.8823, Acc: 71.00%\nEpoch 4/20\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"**TESTING without Attention**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport wandb\nimport random\nimport numpy as np\n\n# =======================\n# Fixed Best Configuration\n# =======================\nbest_config = {\n    'embedding_dim': 128,\n    'hidden_dim': 256,\n    'enc_layers': 3,\n    'dec_layers': 3,\n    'cell_type': 'LSTM',\n    'dropout': 0.2,\n    'epochs': 15,\n    'beam_size': 5,\n    'batch_size': 128,  # Added\n    'learning_rate': 0.001  # Added\n}\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nwandb.init(project=\"dakshina-transliteration\", config=config)\nconfig = wandb.config\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.char2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2char = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for char in text:\n                if char not in self.char2idx:\n                    self.char2idx[char] = self.size\n                    self.idx2char[self.size] = char\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.char2idx[c] for c in text]\n\n    def decode(self, idxs):\n        return ''.join([self.idx2char[i] for i in idxs if i > 2])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab=None, out_vocab=None):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        if inp_vocab and out_vocab:\n            inp_vocab.build([p[0] for p in self.pairs])\n            out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        y = [self.out_vocab.char2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.char2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y)\n\ndef collate_fn(batch):\n    x_batch, y_batch = zip(*batch)\n    x_lens = [len(x) for x in x_batch]\n    y_lens = [len(y) for y in y_batch]\n    x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n    y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n    return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens)\n\n# =======================\n# Encoder / Decoder / Seq2Seq\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.out = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, input_token, hidden):\n        embedded = self.embedding(input_token).unsqueeze(1)\n        output, hidden = self.rnn(embedded, hidden)\n        output = self.out(output.squeeze(1))\n        return output, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, enc_layers, dec_layers, cell_type, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.cell_type = cell_type\n        self.enc_layers = enc_layers\n        self.dec_layers = dec_layers\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n\n        enc_hidden = self.encoder(src[0], src[1])\n\n        if self.cell_type == \"LSTM\":\n            h, c = enc_hidden\n            h = self._match_layers(h)\n            c = self._match_layers(c)\n            dec_hidden = (h, c)\n        else:\n            dec_hidden = self._match_layers(enc_hidden)\n\n        input_token = trg[:, 0]\n        for t in range(1, trg_len):\n            output, dec_hidden = self.decoder(input_token, dec_hidden)\n            outputs[:, t] = output\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if random.random() < teacher_forcing_ratio else top1\n        return outputs\n\n    def _match_layers(self, hidden):\n        if self.enc_layers == self.dec_layers:\n            return hidden\n        elif self.enc_layers > self.dec_layers:\n            return hidden[:self.dec_layers]\n        else:\n            pad = hidden.new_zeros((self.dec_layers - self.enc_layers, *hidden.shape[1:]))\n            return torch.cat([hidden, pad], dim=0)\n\n# =======================\n# Train / Evaluate\n# =======================\ndef compute_accuracy(preds, targets):\n    preds = preds.argmax(-1)\n    correct = ((preds == targets) & (targets != 0)).sum().item()\n    total = (targets != 0).sum().item()\n    return correct / total\n\ndef train_eval(model, loader, criterion, optimizer, is_train):\n    model.train() if is_train else model.eval()\n    total_loss, total_acc = 0, 0\n    with torch.set_grad_enabled(is_train):\n        for src, trg, src_lens, _ in loader:\n            src, trg = src.to(device), trg.to(device)\n            if is_train: optimizer.zero_grad()\n            output = model((src, src_lens), trg)\n            loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))\n            acc = compute_accuracy(output[:, 1:], trg[:, 1:])\n            if is_train:\n                loss.backward()\n                optimizer.step()\n            total_loss += loss.item()\n            total_acc += acc\n    return total_loss / len(loader), total_acc / len(loader)\n\n# =======================\n# Train and Save Best\n# =======================\ninp_vocab, out_vocab = Vocab(), Vocab()\ntrain_set = TransliterationDataset(\"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\", inp_vocab, out_vocab)\ndev_set = TransliterationDataset(\"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\", inp_vocab, out_vocab)\ntest_set = TransliterationDataset(\"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\", inp_vocab, out_vocab)\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dev_set, batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_set, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\nencoder = Encoder(inp_vocab.size, config.embedding_dim, config.hidden_dim, config.enc_layers, config.cell_type, config.dropout)\ndecoder = Decoder(out_vocab.size, config.embedding_dim, config.hidden_dim, config.dec_layers, config.cell_type, config.dropout)\nmodel = Seq2Seq(encoder, decoder, config.enc_layers, config.dec_layers, config.cell_type, device).to(device)\n\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\nbest_val_acc = 0.0\nfor epoch in range(config.epochs):\n    train_loss, train_acc = train_eval(model, train_loader, criterion, optimizer, is_train=True)\n    val_loss, val_acc = train_eval(model, dev_loader, criterion, optimizer, is_train=False)\n    wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc, \"epoch\": epoch})\n    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} Acc={train_acc:.4f}, Val Loss={val_loss:.4f} Acc={val_acc:.4f}\")\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), \"best_model.pt\")\n\n# =======================\n# Test Evaluation\n# =======================\nmodel.load_state_dict(torch.load(\"best_model.pt\"))\nmodel.eval()\n\nos.makedirs(\"predictions_vanilla\", exist_ok=True)\nwith open(\"predictions_vanilla/preds.txt\", \"w\", encoding=\"utf-8\") as f:\n    correct, total = 0, 0\n    samples = []\n    for src, trg, src_lens, _ in test_loader:\n        src, trg = src.to(device), trg.to(device)\n        output = model((src, src_lens), trg, teacher_forcing_ratio=0.0)\n        pred_idxs = output.argmax(-1)[0].tolist()\n        true_idxs = trg[0].tolist()\n        pred_str = out_vocab.decode(pred_idxs)\n        true_str = out_vocab.decode(true_idxs)\n        input_str = inp_vocab.decode(src[0].tolist())\n        f.write(f\"{input_str}\\t{true_str}\\t{pred_str}\\n\")\n        if pred_str == true_str:\n            correct += 1\n        total += 1\n        samples.append((input_str, true_str, pred_str))\n    test_acc = correct / total\n    wandb.log({\"test_accuracy\": test_acc})\n    print(\"Test Accuracy:\", test_acc)\n\n# Sample Grid (for visualization)\nprint(\"\\nSample Predictions:\")\nprint(\"{:<20} | {:<20} | {:<20}\".format(\"Input\", \"Reference\", \"Prediction\"))\nprint(\"=\" * 65)\nfor s in random.sample(samples, 10):\n    print(\"{:<20} | {:<20} | {:<20}\".format(*s))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:29:24.615586Z","iopub.execute_input":"2025-05-18T17:29:24.616199Z","iopub.status.idle":"2025-05-18T17:40:19.768549Z","shell.execute_reply.started":"2025-05-18T17:29:24.616174Z","shell.execute_reply":"2025-05-18T17:40:19.767864Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">evaluate_test</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_172129-1zswwpkt/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_172924-1zswwpkt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt' target=\"_blank\">evaluate_test</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 0: Train Loss=1.2103 Acc=0.6221, Val Loss=0.7996 Acc=0.7455\nEpoch 1: Train Loss=0.7029 Acc=0.7740, Val Loss=0.6784 Acc=0.7765\nEpoch 2: Train Loss=0.6081 Acc=0.8035, Val Loss=0.6208 Acc=0.8025\nEpoch 3: Train Loss=0.5603 Acc=0.8173, Val Loss=0.6208 Acc=0.8002\nEpoch 4: Train Loss=0.5335 Acc=0.8238, Val Loss=0.6086 Acc=0.8038\nEpoch 5: Train Loss=0.5105 Acc=0.8311, Val Loss=0.6006 Acc=0.8093\nEpoch 6: Train Loss=0.4914 Acc=0.8355, Val Loss=0.5927 Acc=0.8080\nEpoch 7: Train Loss=0.4737 Acc=0.8404, Val Loss=0.6052 Acc=0.8078\nEpoch 8: Train Loss=0.4590 Acc=0.8435, Val Loss=0.6019 Acc=0.8034\nEpoch 9: Train Loss=0.4495 Acc=0.8457, Val Loss=0.5979 Acc=0.8129\nEpoch 10: Train Loss=0.4416 Acc=0.8482, Val Loss=0.6096 Acc=0.8070\nEpoch 11: Train Loss=0.4337 Acc=0.8492, Val Loss=0.6045 Acc=0.8113\nEpoch 12: Train Loss=0.4265 Acc=0.8510, Val Loss=0.5946 Acc=0.8153\nEpoch 13: Train Loss=0.4222 Acc=0.8518, Val Loss=0.6018 Acc=0.8093\nEpoch 14: Train Loss=0.4120 Acc=0.8546, Val Loss=0.6180 Acc=0.8116\nEpoch 15: Train Loss=0.4099 Acc=0.8541, Val Loss=0.6080 Acc=0.8095\nEpoch 16: Train Loss=0.4018 Acc=0.8564, Val Loss=0.6017 Acc=0.8107\nEpoch 17: Train Loss=0.4019 Acc=0.8554, Val Loss=0.6075 Acc=0.8112\nEpoch 18: Train Loss=0.3944 Acc=0.8577, Val Loss=0.6101 Acc=0.8076\nEpoch 19: Train Loss=0.3893 Acc=0.8585, Val Loss=0.6184 Acc=0.8105\nTest Accuracy: 0.3676143936028432\n\nSample Predictions:\nInput                | Reference            | Prediction          \n=================================================================\nगदाधर                | gadadhar             | gadadhar            \nचकत्ते               | chaktte              | chaktte             \nख़ासतौर              | khaastaur            | khastaur            \nहां                  | haan                 | haan                \nपल्मोनरी             | pulmonary            | palmanori           \nब्रोकन               | broken               | brokan              \nटार्च                | torch                | tarch               \nकरघे                 | karghe               | karghe              \nमलार                 | malar                | malar               \nलक्ष्य               | lakshy               | lakshya             \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# =======================\n# Imports and Sweep Config\n# =======================\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport wandb\nimport os\nimport math\nfrom tqdm import tqdm\n\nos.makedirs(\"predictions_vanilla\", exist_ok=True)\n\nbest_config = {\n    'embedding_dim': 128,\n    'hidden_dim': 256,\n    'enc_layers': 3,\n    'dec_layers': 3,\n    'cell_type': 'LSTM',\n    'dropout': 0.2,\n    'epochs': 15,\n    'beam_size': 5\n    'batch_size': 128,  # Added\n    'learning_rate': 0.001  # Added\n}\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.char2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2char = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for char in text:\n                if char not in self.char2idx:\n                    self.char2idx[char] = self.size\n                    self.idx2char[self.size] = char\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.char2idx[c] for c in text]\n\n    def decode(self, idxs):\n        return ''.join([self.idx2char[i] for i in idxs if i > 2])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab, out_vocab, is_test=False):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        if not is_test:\n            inp_vocab.build([p[0] for p in self.pairs])\n            out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        y = [self.out_vocab.char2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.char2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y), lat, dev\n\ndef collate_fn(batch):\n    x_batch, y_batch, lat, dev = zip(*batch)\n    x_lens = [len(x) for x in x_batch]\n    y_lens = [len(y) for y in y_batch]\n    x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n    y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n    return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens), lat, dev\n\n# =======================\n# Encoder, Decoder, Seq2Seq\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.out = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, input_token, hidden):\n        embedded = self.embedding(input_token).unsqueeze(1)\n        output, hidden = self.rnn(embedded, hidden)\n        output = self.out(output.squeeze(1))\n        return output, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, enc_layers, dec_layers, cell_type, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.cell_type = cell_type\n        self.enc_layers = enc_layers\n        self.dec_layers = dec_layers\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n        enc_hidden = self.encoder(src[0], src[1])\n        if self.cell_type == \"LSTM\":\n            h, c = enc_hidden\n            h = self._match_layers(h)\n            c = self._match_layers(c)\n            dec_hidden = (h, c)\n        else:\n            dec_hidden = self._match_layers(enc_hidden)\n        input_token = trg[:, 0]\n        for t in range(1, trg_len):\n            output, dec_hidden = self.decoder(input_token, dec_hidden)\n            outputs[:, t] = output\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n        return outputs\n\n    def _match_layers(self, hidden):\n        if self.enc_layers == self.dec_layers:\n            return hidden\n        elif self.enc_layers > self.dec_layers:\n            return hidden[:self.dec_layers]\n        else:\n            pad = hidden.new_zeros((self.dec_layers - self.enc_layers, *hidden.shape[1:]))\n            return torch.cat([hidden, pad], dim=0)\n\n    def predict(self, src_tensor, src_len, max_len=30):\n        self.eval()\n        with torch.no_grad():\n            enc_hidden = self.encoder(src_tensor.unsqueeze(0), torch.tensor([src_len]))\n            if self.cell_type == \"LSTM\":\n                h, c = enc_hidden\n                h = self._match_layers(h)\n                c = self._match_layers(c)\n                dec_hidden = (h, c)\n            else:\n                dec_hidden = self._match_layers(enc_hidden)\n            input_token = torch.tensor([2]).to(self.device)  # <sos>\n            output_seq = []\n            for _ in range(max_len):\n                output, dec_hidden = self.decoder(input_token, dec_hidden)\n                top1 = output.argmax(1)\n                if top1.item() == 2: break  # <eos>\n                output_seq.append(top1.item())\n                input_token = top1\n        return output_seq\n\n# =======================\n# Train & Eval\n# =======================\ndef train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss, total_correct, total_count = 0, 0, 0\n    for src, trg, src_lens, _, _, _ in loader:\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model((src, src_lens), trg)\n        loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))\n        pred = output.argmax(2)\n        correct = ((pred[:, 1:] == trg[:, 1:]) & (trg[:, 1:] != 0)).sum().item()\n        total_correct += correct\n        total_count += (trg[:, 1:] != 0).sum().item()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader), 100.0 * total_correct / total_count\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss, total_correct, total_count = 0, 0, 0\n    with torch.no_grad():\n        for src, trg, src_lens, _, _, _ in loader:\n            src, trg = src.to(device), trg.to(device)\n            output = model((src, src_lens), trg, teacher_forcing_ratio=0)\n            loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))\n            pred = output.argmax(2)\n            correct = ((pred[:, 1:] == trg[:, 1:]) & (trg[:, 1:] != 0)).sum().item()\n            total_correct += correct\n            total_count += (trg[:, 1:] != 0).sum().item()\n            total_loss += loss.item()\n    return total_loss / len(loader), 100.0 * total_correct / total_count\n\n# =======================\n# Main\n# =======================\ndef main():\n    wandb.init(project=\"dakshina-transliteration\")\n    config = best_config\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    inp_vocab, out_vocab = Vocab(), Vocab()\n    train_data = TransliterationDataset(\"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\", inp_vocab, out_vacab)\n    dev_data = TransliterationDataset(\"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\", inp_vocab, out_vocab)\n    test_data = TransliterationDataset(\"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\", inp_vocab, out_vocab, is_test=True)\n\n    # Update DataLoader batch sizes\n    train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    # Update optimizer with learning rate\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    best_val_acc = 0.0\n    for epoch in range(config['epochs']):\n        print(f\"Epoch {epoch+1}/{config['epochs']}\")\n        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n\n        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n        print(f\"Val   Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc\n        })\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(\"Best model saved.\")\n\n    \n    # Evaluation on test set\n    print(\"\\n Evaluating on test set with best model:\")\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    model.eval()\n    total_correct, total_count = 0, 0\n\n    with open(\"predictions_vanilla/test_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n        for src, _, src_lens, _, lat, gold in test_loader:\n            src = src.to(device)\n            pred_ids = model.predict(src[0], src_lens[0].item())\n            pred = out_vocab.decode(pred_ids)\n            f.write(f\"{lat[0]}\\t{gold[0]}\\t{pred}\\n\")\n            if pred == gold[0]:\n                total_correct += 1\n            total_count += 1\n\n    test_acc = 100.0 * total_correct / total_count\n    print(f\"📊 Test Accuracy: {test_acc:.2f}%\")\n    wandb.log({\"test_acc\": test_acc})\n\n\n     # After test evaluation, add random samples display\n    print(\"\\nRandom Test Samples Predictions:\")\n    import random\n    random_indices = random.sample(range(len(test_data)), 20)\n    \n    with open(\"predictions_vanilla/test_predictions.txt\", \"a\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\\nRandom Sample Predictions:\\n\")\n        for idx in random_indices:\n            x, y, lat, dev = test_data[idx]\n            src_tensor = x.to(device)\n            pred_ids = model.predict(src_tensor, len(x))\n            pred = out_vocab.decode(pred_ids)\n            \n            print(f\"Input: {lat}\")\n            print(f\"True: {dev}\")\n            print(f\"Pred: {pred}\\n\")\n            \n            f.write(f\"Input: {lat}\\n\")\n            f.write(f\"True: {dev}\\n\")\n            f.write(f\"Pred: {pred}\\n\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:29:40.343513Z","iopub.execute_input":"2025-05-18T18:29:40.343795Z","iopub.status.idle":"2025-05-18T18:43:01.291000Z","shell.execute_reply.started":"2025-05-18T18:29:40.343774Z","shell.execute_reply":"2025-05-18T18:43:01.290192Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▆▇███</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▆▇▇██</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>83.12333</td></tr><tr><td>train_loss</td><td>0.50879</td></tr><tr><td>val_acc</td><td>72.13333</td></tr><tr><td>val_loss</td><td>0.86891</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">evaluate_test</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250518_180647-1zswwpkt/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_182940-1zswwpkt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt' target=\"_blank\">evaluate_test</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/sweeps/q6l1non5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-transliteration/runs/1zswwpkt</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.2916, Accuracy: 60.52%\nVal   Loss: 0.9746, Accuracy: 68.25%\n✅ Best model saved.\nEpoch 2/15\nTrain Loss: 0.6719, Accuracy: 78.24%\nVal   Loss: 0.8701, Accuracy: 71.92%\n✅ Best model saved.\nEpoch 3/15\nTrain Loss: 0.5746, Accuracy: 81.27%\nVal   Loss: 0.8041, Accuracy: 73.33%\n✅ Best model saved.\nEpoch 4/15\nTrain Loss: 0.5236, Accuracy: 82.72%\nVal   Loss: 0.8114, Accuracy: 73.86%\n✅ Best model saved.\nEpoch 5/15\nTrain Loss: 0.4914, Accuracy: 83.66%\nVal   Loss: 0.8050, Accuracy: 74.60%\n✅ Best model saved.\nEpoch 6/15\nTrain Loss: 0.4701, Accuracy: 84.21%\nVal   Loss: 0.8052, Accuracy: 74.57%\nEpoch 7/15\nTrain Loss: 0.4553, Accuracy: 84.49%\nVal   Loss: 0.8100, Accuracy: 74.55%\nEpoch 8/15\nTrain Loss: 0.4394, Accuracy: 84.89%\nVal   Loss: 0.7896, Accuracy: 74.93%\n✅ Best model saved.\nEpoch 9/15\nTrain Loss: 0.4285, Accuracy: 85.05%\nVal   Loss: 0.7989, Accuracy: 74.77%\nEpoch 10/15\nTrain Loss: 0.4173, Accuracy: 85.34%\nVal   Loss: 0.7853, Accuracy: 75.38%\n✅ Best model saved.\nEpoch 11/15\nTrain Loss: 0.4023, Accuracy: 85.73%\nVal   Loss: 0.7979, Accuracy: 75.08%\nEpoch 12/15\nTrain Loss: 0.4009, Accuracy: 85.66%\nVal   Loss: 0.7900, Accuracy: 75.05%\nEpoch 13/15\nTrain Loss: 0.3894, Accuracy: 85.94%\nVal   Loss: 0.7895, Accuracy: 75.08%\nEpoch 14/15\nTrain Loss: 0.3860, Accuracy: 85.99%\nVal   Loss: 0.7889, Accuracy: 75.17%\nEpoch 15/15\nTrain Loss: 0.3828, Accuracy: 86.07%\nVal   Loss: 0.8034, Accuracy: 74.93%\n\n🔍 Evaluating on test set with best model:\n📊 Test Accuracy: 35.96%\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# =======================\n# Imports and Sweep Config\n# =======================\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport wandb\nimport os\n\nsweep_config = {\n    'method': 'bayes',\n    'metric': {'name': 'val_loss', 'goal': 'minimize'},\n    'parameters': {\n        'embedding_dim': {'values': [64, 128,256]},\n        'hidden_dim': {'values': [64, 128,256]},\n        'enc_layers': {'values': [1, 2,3]},\n        'dec_layers': {'values': [1, 2,3]},\n        'cell_type': {'values': ['GRU', 'LSTM', 'RNN']},\n        'dropout': {'values': [0.0,0.2, 0.3]},\n        'epochs': {'values': [10,15]},\n        'beam_size': {'values': [1, 3, 5]}\n    }\n}\n\ndefault_config = {\n    'embedding_dim': 64,\n    'hidden_dim': 64,\n    'enc_layers': 1,\n    'dec_layers': 1,\n    'cell_type': 'LSTM',\n    'dropout': 0.2,\n    'epochs': 10,\n    'beam_size': 1\n}\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for word in text.split():\n                if word not in self.word2idx:\n                    self.word2idx[word] = self.size\n                    self.idx2word[self.size] = word\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.word2idx[w] for w in text.split()]\n\n    def decode(self, idxs):\n        return ' '.join([self.idx2word[i] for i in idxs if i > 2])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab, out_vocab):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        inp_vocab.build([p[0] for p in self.pairs])\n        out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n\n    def __len__(self): return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        y = [self.out_vocab.word2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.word2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y)\n\ndef collate_fn(batch):\n    x_batch, y_batch = zip(*batch)\n    x_lens = [len(x) for x in x_batch]\n    y_lens = [len(y) for y in y_batch]\n    x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n    y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n    return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens)\n\n# =======================\n# Encoder, Decoder\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        emb = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        _, hidden = self.rnn(packed)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, input_token, hidden):\n        emb = self.embedding(input_token).unsqueeze(1)\n        output, hidden = self.rnn(emb, hidden)\n        return self.fc(output.squeeze(1)), hidden\n\n# =======================\n# Seq2Seq Model + Beam Search\n# =======================\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, enc_layers, dec_layers, cell_type, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.enc_layers = enc_layers\n        self.dec_layers = dec_layers\n        self.cell_type = cell_type\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.fc.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n        enc_hidden = self.encoder(src[0], src[1])\n        dec_hidden = self._match_layers(enc_hidden)\n        input_token = trg[:, 0]\n\n        for t in range(1, trg_len):\n            output, dec_hidden = self.decoder(input_token, dec_hidden)\n            outputs[:, t] = output\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n        return outputs\n\n    def beam_search(self, src, src_len, sos_idx, eos_idx, beam_size, max_len=20):\n        enc_hidden = self.encoder(src, src_len)\n        dec_hidden = self._match_layers(enc_hidden)\n\n        sequences = [[sos_idx]]\n        scores = [0.0]\n\n        for _ in range(max_len):\n            all_candidates = []\n            for i, seq in enumerate(sequences):\n                input_token = torch.tensor([seq[-1]]).to(self.device)\n                output, new_hidden = self.decoder(input_token, dec_hidden)\n                probs = torch.log_softmax(output, dim=-1)\n                topk = torch.topk(probs, beam_size)\n                for j in range(beam_size):\n                    candidate = seq + [topk.indices[0][j].item()]\n                    score = scores[i] + topk.values[0][j].item()\n                    all_candidates.append((candidate, score))\n            ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n            sequences, scores = zip(*ordered[:beam_size])\n            if all(seq[-1] == eos_idx for seq in sequences):\n                break\n        return sequences[0]\n\n    def _match_layers(self, hidden):\n        if self.cell_type == 'LSTM':\n            h, c = hidden\n            return self._pad_layers(h), self._pad_layers(c)\n        return self._pad_layers(hidden)\n\n    def _pad_layers(self, h):\n        if self.enc_layers == self.dec_layers:\n            return h\n        elif self.enc_layers > self.dec_layers:\n            return h[:self.dec_layers]\n        else:\n            pad = h.new_zeros((self.dec_layers - self.enc_layers, *h.shape[1:]))\n            return torch.cat([h, pad], dim=0)\n\n# =======================\n# Train and Evaluate\n# =======================\ndef train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss, total_correct, total_count = 0, 0, 0\n    for src, trg, src_lens, _ in loader:\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model((src, src_lens), trg)\n        loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))\n        pred = output.argmax(2)\n        correct = ((pred[:, 1:] == trg[:, 1:]) & (trg[:, 1:] != 0)).sum().item()\n        total_correct += correct\n        total_count += (trg[:, 1:] != 0).sum().item()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader), 100.0 * total_correct / total_count\n\ndef evaluate(model, loader, criterion, device, out_vocab, beam_size):\n    model.eval()\n    total_loss, total_correct, total_count = 0, 0, 0\n    with torch.no_grad():\n        for src, trg, src_lens, _ in loader:\n            src, trg = src.to(device), trg.to(device)\n            if beam_size > 1:\n                for i in range(src.size(0)):\n                    pred_seq = model.beam_search(src[i].unsqueeze(0), src_lens[i:i+1], 1, 2, beam_size)\n                    gold_seq = trg[i].tolist()\n                    total_correct += sum(p == g for p, g in zip(pred_seq[1:], gold_seq[1:]) if g != 0)\n                    total_count += sum(1 for g in gold_seq[1:] if g != 0)\n                total_loss += 0  # No loss in beam search mode\n            else:\n                output = model((src, src_lens), trg, teacher_forcing_ratio=0)\n                loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))\n                pred = output.argmax(2)\n                correct = ((pred[:, 1:] == trg[:, 1:]) & (trg[:, 1:] != 0)).sum().item()\n                total_correct += correct\n                total_count += (trg[:, 1:] != 0).sum().item()\n                total_loss += loss.item()\n    return total_loss / len(loader), 100.0 * total_correct / total_count\n\n# =======================\n# Main\n# =======================\ndef main():\n    wandb.init(config=default_config, project=\"word-level-transliteration\")\n    config = wandb.config\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    inp_vocab, out_vocab = Vocab(), Vocab()\n    train_data = TransliterationDataset(\"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\", inp_vocab, out_vocab)\n    dev_data = TransliterationDataset(\"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\", inp_vocab, out_vocab)\n    train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=32, shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(inp_vocab.size, config.embedding_dim, config.hidden_dim, config.enc_layers, config.cell_type, config.dropout)\n    decoder = Decoder(out_vocab.size, config.embedding_dim, config.hidden_dim, config.dec_layers, config.cell_type, config.dropout)\n    model = Seq2Seq(encoder, decoder, config.enc_layers, config.dec_layers, config.cell_type, device).to(device)\n    optimizer = torch.optim.Adam(model.parameters())\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    for epoch in range(config.epochs):\n        print(f\"Epoch {epoch+1}/{config.epochs}\")\n        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device, out_vocab, config.beam_size)\n        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} Acc={train_acc:.4f}, Val Loss={val_loss:.4f} Acc={val_acc:.4f}\")\n\n        wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc, \"epoch\": epoch+1})\n\n# =======================\nif __name__ == '__main__':\n    sweep_id = wandb.sweep(sweep_config, project=\"word-level-transliteration\")\n    wandb.agent(sweep_id, function=main, count=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:51:59.372051Z","iopub.execute_input":"2025-05-19T13:51:59.372331Z","execution_failed":"2025-05-19T14:25:36.003Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: v8nboqsd\nSweep URL: https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/sweeps/v8nboqsd\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2eokon6t with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanglesh_dlass3\u001b[0m (\u001b[33mmanglesh_dl_ass3\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'word-level-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_135218-2eokon6t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/runs/2eokon6t' target=\"_blank\">misunderstood-sweep-1</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/sweeps/v8nboqsd' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/sweeps/v8nboqsd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/word-level-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/sweeps/v8nboqsd' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/sweeps/v8nboqsd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/runs/2eokon6t' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/runs/2eokon6t</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nEpoch 1: Train Loss=5.7940 Acc=49.5804, Val Loss=0.0000 Acc=48.1758\nEpoch 2/15\nEpoch 2: Train Loss=5.3633 Acc=50.0000, Val Loss=0.0000 Acc=43.5865\nEpoch 3/15\nEpoch 3: Train Loss=5.0938 Acc=50.0181, Val Loss=0.0000 Acc=39.4103\nEpoch 4/15\nEpoch 4: Train Loss=4.5842 Acc=50.3699, Val Loss=0.0000 Acc=39.0776\nEpoch 5/15\nEpoch 5: Train Loss=3.9932 Acc=52.3686, Val Loss=0.0000 Acc=37.5516\nEpoch 6/15\nEpoch 6: Train Loss=3.4184 Acc=57.0390, Val Loss=0.0000 Acc=38.8596\nEpoch 7/15\nEpoch 7: Train Loss=2.8836 Acc=62.4333, Val Loss=0.0000 Acc=39.7774\nEpoch 8/15\nEpoch 8: Train Loss=2.3975 Acc=66.3096, Val Loss=0.0000 Acc=40.8215\nEpoch 9/15\nEpoch 9: Train Loss=1.9742 Acc=68.3739, Val Loss=0.0000 Acc=41.7393\nEpoch 10/15\nEpoch 10: Train Loss=1.6258 Acc=69.0944, Val Loss=0.0000 Acc=42.0262\nEpoch 11/15\nEpoch 11: Train Loss=1.3579 Acc=69.3105, Val Loss=0.0000 Acc=43.0014\nEpoch 12/15\nEpoch 12: Train Loss=1.1646 Acc=69.4926, Val Loss=0.0000 Acc=43.0702\nEpoch 13/15\nEpoch 13: Train Loss=1.0331 Acc=69.5333, Val Loss=0.0000 Acc=43.4029\nEpoch 14/15\nEpoch 14: Train Loss=0.9451 Acc=69.3817, Val Loss=0.0000 Acc=43.3226\nEpoch 15/15\nEpoch 15: Train Loss=0.8863 Acc=69.4847, Val Loss=0.0000 Acc=43.6324\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▄▆▇███████</td></tr><tr><td>train_loss</td><td>█▇▇▆▅▅▄▃▃▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>█▅▂▂▁▂▂▃▄▄▅▅▅▅▅</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>69.48466</td></tr><tr><td>train_loss</td><td>0.88627</td></tr><tr><td>val_acc</td><td>43.6324</td></tr><tr><td>val_loss</td><td>0</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">misunderstood-sweep-1</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/runs/2eokon6t' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/runs/2eokon6t</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/word-level-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_135218-2eokon6t/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qagkeha0 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'word-level-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_141232-qagkeha0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/runs/qagkeha0' target=\"_blank\">radiant-sweep-2</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/sweeps/v8nboqsd' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/sweeps/v8nboqsd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/word-level-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/sweeps/v8nboqsd' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/sweeps/v8nboqsd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/runs/qagkeha0' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/word-level-transliteration/runs/qagkeha0</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nEpoch 1: Train Loss=6.1236 Acc=49.6120, Val Loss=0.0000 Acc=26.2391\nEpoch 2/15\nEpoch 2: Train Loss=5.5682 Acc=50.0011, Val Loss=0.0000 Acc=25.8720\nEpoch 3/15\nEpoch 3: Train Loss=5.4350 Acc=50.0011, Val Loss=0.0000 Acc=28.7976\nEpoch 4/15\nEpoch 4: Train Loss=5.3200 Acc=50.0034, Val Loss=0.0000 Acc=30.7480\nEpoch 5/15\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# =======================\n# Imports and Sweep Config\n# =======================\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport wandb\nimport os\nimport math\n\nsweep_config = {\n    'method': 'bayes',\n    'metric': {\n        'name': 'val_loss',\n        'goal': 'minimize'\n    },\n    'parameters': {\n        'embedding_dim': {'values': [32,64,128,256]},\n        'hidden_dim': {'values': [64, 128,256 ]},\n        'enc_layers': {'values': [1, 2,3]},\n        'dec_layers': {'values': [1, 2,3]},\n        'cell_type': {'values': ['GRU', 'LSTM', 'RNN']},\n        'dropout': {'values': [0.0,0.2, 0.3,0.5]},\n        'epochs': {'values': [10,20, 15]},\n        'beam_size': {'values': [1, 3, 5]},\n        'batch_size': {'values': [64, 128, 256]},\n        'learning_rate': {'values': [0.001, 0.0005, 0.0001]}\n    }\n}\n\ndefault_config = {\n    'embedding_dim': 32,\n    'hidden_dim': 64,\n    'enc_layers': 1,\n    'dec_layers': 1,\n    'cell_type': 'LSTM',\n    'dropout': 0.2,\n    'epochs': 10,\n    'beam_size': 1,\n    'batch_size': 64,\n    'learning_rate': 0.001\n}\n\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.char2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2char = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for char in text:\n                if char not in self.char2idx:\n                    self.char2idx[char] = self.size\n                    self.idx2char[self.size] = char\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.char2idx[c] for c in text]\n\n    def decode(self, idxs):\n        return ''.join([self.idx2char[i] for i in idxs if i > 2])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab, out_vocab):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        inp_vocab.build([p[0] for p in self.pairs])\n        out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        y = [self.out_vocab.char2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.char2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y)\n\ndef collate_fn(batch):\n    x_batch, y_batch = zip(*batch)\n    x_lens = [len(x) for x in x_batch]\n    y_lens = [len(y) for y in y_batch]\n    x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n    y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n    return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens)\n\n# =======================\n# Encoder and Decoder\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.out = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, input_token, hidden):\n        embedded = self.embedding(input_token).unsqueeze(1)\n        output, hidden = self.rnn(embedded, hidden)\n        output = self.out(output.squeeze(1))\n        return output, hidden\n\n# =======================\n# Seq2Seq Model with Beam Search\n# =======================\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, enc_layers, dec_layers, cell_type, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.cell_type = cell_type\n        self.enc_layers = enc_layers\n        self.dec_layers = dec_layers\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n        enc_hidden = self.encoder(src[0], src[1])\n\n        if self.cell_type == \"LSTM\":\n            h, c = enc_hidden\n            h = self._match_layers(h)\n            c = self._match_layers(c)\n            dec_hidden = (h, c)\n        else:\n            dec_hidden = self._match_layers(enc_hidden)\n\n        input_token = trg[:, 0]\n        for t in range(1, trg_len):\n            output, dec_hidden = self.decoder(input_token, dec_hidden)\n            outputs[:, t] = output\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n        return outputs\n\n    def _match_layers(self, hidden):\n        if self.enc_layers == self.dec_layers:\n            return hidden\n        elif self.enc_layers > self.dec_layers:\n            return hidden[:self.dec_layers]\n        else:\n            pad = hidden.new_zeros((self.dec_layers - self.enc_layers, *hidden.shape[1:]))\n            return torch.cat([hidden, pad], dim=0)\n\n# =======================\n# Train & Eval\n# =======================\ndef train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss, total_correct, total_count = 0, 0, 0\n    for src, trg, src_lens, _ in loader:\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model((src, src_lens), trg)\n        output_dim = output.shape[-1]\n        loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n        pred = output.argmax(2)\n        correct = ((pred[:, 1:] == trg[:, 1:]) & (trg[:, 1:] != 0)).sum().item()\n        total_correct += correct\n        total_count += (trg[:, 1:] != 0).sum().item()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    acc = 100.0 * total_correct / total_count\n    print(f\"Train Loss: {total_loss / len(loader):.4f}, Acc: {acc:.2f}%\")\n    return total_loss / len(loader), acc\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss, total_correct, total_count = 0, 0, 0\n    with torch.no_grad():\n        for src, trg, src_lens, _ in loader:\n            src, trg = src.to(device), trg.to(device)\n            output = model((src, src_lens), trg, teacher_forcing_ratio=0)\n            output_dim = output.shape[-1]\n            loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n            pred = output.argmax(2)\n            correct = ((pred[:, 1:] == trg[:, 1:]) & (trg[:, 1:] != 0)).sum().item()\n            total_correct += correct\n            total_count += (trg[:, 1:] != 0).sum().item()\n            total_loss += loss.item()\n    acc = 100.0 * total_correct / total_count\n    print(f\"Val Loss: {total_loss / len(loader):.4f}, Acc: {acc:.2f}%\")\n    return total_loss / len(loader), acc\n\n# =======================\n# Main\n# =======================\ndef main():\n    wandb.init(config=default_config, project=\"dakshina-transliteration\")\n    config = wandb.config\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    inp_vocab, out_vocab = Vocab(), Vocab()\n    train_path = \"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n    dev_path = \"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n    train_data = TransliterationDataset(train_path, inp_vocab, out_vocab)\n    dev_data = TransliterationDataset(dev_path, inp_vocab, out_vocab)\n    \n    # Use config.batch_size for DataLoader\n    train_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(inp_vocab.size, config.embedding_dim, config.hidden_dim, config.enc_layers, config.cell_type, config.dropout)\n    decoder = Decoder(out_vocab.size, config.embedding_dim, config.hidden_dim, config.dec_layers, config.cell_type, config.dropout)\n    model = Seq2Seq(encoder, decoder, config.enc_layers, config.dec_layers, config.cell_type, device).to(device)\n    \n    # Use config.learning_rate for optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    for epoch in range(config.epochs):\n        print(f\"Epoch {epoch+1}/{config.epochs}\")\n        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n        wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc, \"epoch\": epoch+1})\n\n# =======================\n\nif __name__ == '__main__':\n    sweep_id = wandb.sweep(sweep_config, project=\"dakshina-translit\")\n    wandb.agent(sweep_id, function=main,count=30)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:27:05.463994Z","iopub.execute_input":"2025-05-19T14:27:05.464300Z","iopub.status.idle":"2025-05-19T16:37:40.627867Z","shell.execute_reply.started":"2025-05-19T14:27:05.464275Z","shell.execute_reply":"2025-05-19T16:37:40.627370Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: 2yk21c98\nSweep URL: https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f8lsoyx5 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanglesh_dlass3\u001b[0m (\u001b[33mmanglesh_dl_ass3\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_142723-f8lsoyx5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/f8lsoyx5' target=\"_blank\">classic-sweep-1</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/f8lsoyx5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/f8lsoyx5</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.8135, Acc: 23.51%\nVal Loss: 2.5705, Acc: 28.12%\nEpoch 2/15\nTrain Loss: 2.4345, Acc: 29.73%\nVal Loss: 2.3086, Acc: 30.87%\nEpoch 3/15\nTrain Loss: 2.2041, Acc: 32.47%\nVal Loss: 2.1289, Acc: 32.56%\nEpoch 4/15\nTrain Loss: 2.0241, Acc: 35.56%\nVal Loss: 2.0125, Acc: 34.26%\nEpoch 5/15\nTrain Loss: 1.8849, Acc: 38.74%\nVal Loss: 1.9114, Acc: 36.35%\nEpoch 6/15\nTrain Loss: 1.7606, Acc: 42.41%\nVal Loss: 1.8224, Acc: 38.88%\nEpoch 7/15\nTrain Loss: 1.6551, Acc: 45.66%\nVal Loss: 1.7421, Acc: 41.13%\nEpoch 8/15\nTrain Loss: 1.5627, Acc: 48.60%\nVal Loss: 1.6578, Acc: 44.00%\nEpoch 9/15\nTrain Loss: 1.4863, Acc: 50.87%\nVal Loss: 1.5707, Acc: 47.06%\nEpoch 10/15\nTrain Loss: 1.4028, Acc: 53.94%\nVal Loss: 1.4997, Acc: 49.84%\nEpoch 11/15\nTrain Loss: 1.3305, Acc: 56.48%\nVal Loss: 1.4181, Acc: 52.45%\nEpoch 12/15\nTrain Loss: 1.2608, Acc: 58.98%\nVal Loss: 1.3476, Acc: 55.49%\nEpoch 13/15\nTrain Loss: 1.2049, Acc: 60.94%\nVal Loss: 1.2937, Acc: 56.91%\nEpoch 14/15\nTrain Loss: 1.1443, Acc: 63.20%\nVal Loss: 1.2410, Acc: 58.59%\nEpoch 15/15\nTrain Loss: 1.0974, Acc: 64.67%\nVal Loss: 1.1993, Acc: 60.21%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▂▃▃▄▄▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▆▅▄▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▂▂▃▃▄▄▅▆▆▇▇██</td></tr><tr><td>val_loss</td><td>█▇▆▅▅▄▄▃▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>64.67042</td></tr><tr><td>train_loss</td><td>1.09736</td></tr><tr><td>val_acc</td><td>60.21238</td></tr><tr><td>val_loss</td><td>1.19925</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">classic-sweep-1</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/f8lsoyx5' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/f8lsoyx5</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_142723-f8lsoyx5/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dteoe9dh with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_143055-dteoe9dh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dteoe9dh' target=\"_blank\">magic-sweep-2</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dteoe9dh' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dteoe9dh</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 2.2577, Acc: 33.81%\nVal Loss: 1.8775, Acc: 39.00%\nEpoch 2/20\nTrain Loss: 1.5278, Acc: 50.62%\nVal Loss: 1.5635, Acc: 48.27%\nEpoch 3/20\nTrain Loss: 1.2740, Acc: 58.72%\nVal Loss: 1.3934, Acc: 54.09%\nEpoch 4/20\nTrain Loss: 1.1179, Acc: 64.05%\nVal Loss: 1.2667, Acc: 58.63%\nEpoch 5/20\nTrain Loss: 1.0101, Acc: 67.65%\nVal Loss: 1.1977, Acc: 61.08%\nEpoch 6/20\nTrain Loss: 0.9265, Acc: 70.48%\nVal Loss: 1.1232, Acc: 63.40%\nEpoch 7/20\nTrain Loss: 0.8674, Acc: 72.41%\nVal Loss: 1.0720, Acc: 64.79%\nEpoch 8/20\nTrain Loss: 0.8083, Acc: 74.40%\nVal Loss: 1.0452, Acc: 65.60%\nEpoch 9/20\nTrain Loss: 0.7741, Acc: 75.33%\nVal Loss: 1.0145, Acc: 66.78%\nEpoch 10/20\nTrain Loss: 0.7444, Acc: 76.19%\nVal Loss: 0.9946, Acc: 67.19%\nEpoch 11/20\nTrain Loss: 0.7136, Acc: 77.26%\nVal Loss: 0.9856, Acc: 67.79%\nEpoch 12/20\nTrain Loss: 0.6835, Acc: 78.26%\nVal Loss: 0.9504, Acc: 68.51%\nEpoch 13/20\nTrain Loss: 0.6657, Acc: 78.76%\nVal Loss: 0.9413, Acc: 69.25%\nEpoch 14/20\nTrain Loss: 0.6476, Acc: 79.26%\nVal Loss: 0.9249, Acc: 69.53%\nEpoch 15/20\nTrain Loss: 0.6313, Acc: 79.78%\nVal Loss: 0.9320, Acc: 69.67%\nEpoch 16/20\nTrain Loss: 0.6126, Acc: 80.46%\nVal Loss: 0.9111, Acc: 70.06%\nEpoch 17/20\nTrain Loss: 0.6007, Acc: 80.76%\nVal Loss: 0.9075, Acc: 70.50%\nEpoch 18/20\nTrain Loss: 0.5896, Acc: 81.08%\nVal Loss: 0.8997, Acc: 70.69%\nEpoch 19/20\nTrain Loss: 0.5731, Acc: 81.60%\nVal Loss: 0.9063, Acc: 70.75%\nEpoch 20/20\nTrain Loss: 0.5615, Acc: 81.97%\nVal Loss: 0.8942, Acc: 70.86%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▃▅▅▆▆▇▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▄▅▆▆▇▇▇▇▇▇████████</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_acc</td><td>81.97143</td></tr><tr><td>train_loss</td><td>0.56146</td></tr><tr><td>val_acc</td><td>70.86311</td></tr><tr><td>val_loss</td><td>0.89417</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">magic-sweep-2</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dteoe9dh' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dteoe9dh</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_143055-dteoe9dh/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5fr42dqj with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_143710-5fr42dqj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/5fr42dqj' target=\"_blank\">blooming-sweep-3</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/5fr42dqj' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/5fr42dqj</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.8228, Acc: 22.32%\nVal Loss: 2.6427, Acc: 25.90%\nEpoch 2/15\nTrain Loss: 2.4641, Acc: 30.08%\nVal Loss: 2.4051, Acc: 31.34%\nEpoch 3/15\nTrain Loss: 2.2427, Acc: 35.23%\nVal Loss: 2.2665, Acc: 33.58%\nEpoch 4/15\nTrain Loss: 2.1051, Acc: 38.24%\nVal Loss: 2.1652, Acc: 35.20%\nEpoch 5/15\nTrain Loss: 1.9974, Acc: 40.49%\nVal Loss: 2.0863, Acc: 36.22%\nEpoch 6/15\nTrain Loss: 1.9087, Acc: 42.28%\nVal Loss: 2.0062, Acc: 37.97%\nEpoch 7/15\nTrain Loss: 1.8243, Acc: 44.30%\nVal Loss: 1.9386, Acc: 39.49%\nEpoch 8/15\nTrain Loss: 1.7495, Acc: 46.09%\nVal Loss: 1.8720, Acc: 41.01%\nEpoch 9/15\nTrain Loss: 1.6797, Acc: 47.84%\nVal Loss: 1.8153, Acc: 42.46%\nEpoch 10/15\nTrain Loss: 1.6159, Acc: 49.61%\nVal Loss: 1.7517, Acc: 44.19%\nEpoch 11/15\nTrain Loss: 1.5563, Acc: 51.27%\nVal Loss: 1.7019, Acc: 45.46%\nEpoch 12/15\nTrain Loss: 1.5039, Acc: 52.75%\nVal Loss: 1.6616, Acc: 46.51%\nEpoch 13/15\nTrain Loss: 1.4591, Acc: 54.10%\nVal Loss: 1.6110, Acc: 47.95%\nEpoch 14/15\nTrain Loss: 1.4088, Acc: 55.58%\nVal Loss: 1.5701, Acc: 49.38%\nEpoch 15/15\nTrain Loss: 1.3721, Acc: 56.66%\nVal Loss: 1.5386, Acc: 50.25%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▃▄▄▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▃▄▄▄▅▅▆▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▆▆▅▄▄▄▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>56.65684</td></tr><tr><td>train_loss</td><td>1.37214</td></tr><tr><td>val_acc</td><td>50.25318</td></tr><tr><td>val_loss</td><td>1.53863</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">blooming-sweep-3</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/5fr42dqj' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/5fr42dqj</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_143710-5fr42dqj/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g9wdvcj7 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_144125-g9wdvcj7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/g9wdvcj7' target=\"_blank\">earthy-sweep-4</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/g9wdvcj7' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/g9wdvcj7</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.4299, Acc: 55.76%\nVal Loss: 1.0009, Acc: 66.51%\nEpoch 2/15\nTrain Loss: 0.7540, Acc: 75.56%\nVal Loss: 0.9032, Acc: 70.34%\nEpoch 3/15\nTrain Loss: 0.6351, Acc: 79.36%\nVal Loss: 0.8520, Acc: 71.82%\nEpoch 4/15\nTrain Loss: 0.5792, Acc: 81.04%\nVal Loss: 0.8505, Acc: 72.46%\nEpoch 5/15\nTrain Loss: 0.5405, Acc: 82.22%\nVal Loss: 0.8121, Acc: 73.14%\nEpoch 6/15\nTrain Loss: 0.5166, Acc: 82.82%\nVal Loss: 0.8129, Acc: 73.83%\nEpoch 7/15\nTrain Loss: 0.4904, Acc: 83.70%\nVal Loss: 0.8076, Acc: 73.98%\nEpoch 8/15\nTrain Loss: 0.4758, Acc: 84.13%\nVal Loss: 0.8095, Acc: 73.94%\nEpoch 9/15\nTrain Loss: 0.4650, Acc: 84.25%\nVal Loss: 0.8191, Acc: 74.25%\nEpoch 10/15\nTrain Loss: 0.4528, Acc: 84.60%\nVal Loss: 0.8058, Acc: 74.45%\nEpoch 11/15\nTrain Loss: 0.4332, Acc: 85.16%\nVal Loss: 0.8232, Acc: 74.03%\nEpoch 12/15\nTrain Loss: 0.4322, Acc: 85.17%\nVal Loss: 0.8190, Acc: 74.21%\nEpoch 13/15\nTrain Loss: 0.4241, Acc: 85.40%\nVal Loss: 0.8095, Acc: 74.25%\nEpoch 14/15\nTrain Loss: 0.4123, Acc: 85.62%\nVal Loss: 0.8495, Acc: 74.69%\nEpoch 15/15\nTrain Loss: 0.4143, Acc: 85.47%\nVal Loss: 0.8087, Acc: 74.69%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▆▇▇▇▇██▇████</td></tr><tr><td>val_loss</td><td>█▄▃▃▁▁▁▁▁▁▂▁▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>85.47126</td></tr><tr><td>train_loss</td><td>0.41433</td></tr><tr><td>val_acc</td><td>74.68534</td></tr><tr><td>val_loss</td><td>0.80871</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">earthy-sweep-4</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/g9wdvcj7' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/g9wdvcj7</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_144125-g9wdvcj7/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ak8uq9uh with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_144644-ak8uq9uh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ak8uq9uh' target=\"_blank\">expert-sweep-5</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ak8uq9uh' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ak8uq9uh</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 2.7816, Acc: 24.42%\nVal Loss: 2.5395, Acc: 28.75%\nEpoch 2/20\nTrain Loss: 2.4379, Acc: 29.77%\nVal Loss: 2.3549, Acc: 30.73%\nEpoch 3/20\nTrain Loss: 2.2707, Acc: 32.13%\nVal Loss: 2.2359, Acc: 31.73%\nEpoch 4/20\nTrain Loss: 2.1306, Acc: 34.52%\nVal Loss: 2.1341, Acc: 33.17%\nEpoch 5/20\nTrain Loss: 2.0118, Acc: 36.99%\nVal Loss: 2.0551, Acc: 34.16%\nEpoch 6/20\nTrain Loss: 1.9255, Acc: 38.79%\nVal Loss: 1.9935, Acc: 34.97%\nEpoch 7/20\nTrain Loss: 1.8439, Acc: 40.88%\nVal Loss: 1.9283, Acc: 36.83%\nEpoch 8/20\nTrain Loss: 1.7730, Acc: 42.80%\nVal Loss: 1.8754, Acc: 38.09%\nEpoch 9/20\nTrain Loss: 1.7043, Acc: 44.84%\nVal Loss: 1.7996, Acc: 40.74%\nEpoch 10/20\nTrain Loss: 1.6360, Acc: 46.99%\nVal Loss: 1.7418, Acc: 42.34%\nEpoch 11/20\nTrain Loss: 1.5757, Acc: 48.87%\nVal Loss: 1.6795, Acc: 44.64%\nEpoch 12/20\nTrain Loss: 1.5212, Acc: 50.52%\nVal Loss: 1.6173, Acc: 46.73%\nEpoch 13/20\nTrain Loss: 1.4588, Acc: 52.64%\nVal Loss: 1.5578, Acc: 48.56%\nEpoch 14/20\nTrain Loss: 1.4004, Acc: 54.71%\nVal Loss: 1.4894, Acc: 50.28%\nEpoch 15/20\nTrain Loss: 1.3512, Acc: 56.10%\nVal Loss: 1.4414, Acc: 52.78%\nEpoch 16/20\nTrain Loss: 1.3007, Acc: 57.89%\nVal Loss: 1.3981, Acc: 53.86%\nEpoch 17/20\nTrain Loss: 1.2585, Acc: 59.28%\nVal Loss: 1.3483, Acc: 55.09%\nEpoch 18/20\nTrain Loss: 1.2119, Acc: 60.78%\nVal Loss: 1.3083, Acc: 56.91%\nEpoch 19/20\nTrain Loss: 1.1737, Acc: 62.16%\nVal Loss: 1.2656, Acc: 57.97%\nEpoch 20/20\nTrain Loss: 1.1473, Acc: 62.93%\nVal Loss: 1.2310, Acc: 59.49%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▆▆▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▇▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_acc</td><td>62.92534</td></tr><tr><td>train_loss</td><td>1.14731</td></tr><tr><td>val_acc</td><td>59.48902</td></tr><tr><td>val_loss</td><td>1.23102</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">expert-sweep-5</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ak8uq9uh' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ak8uq9uh</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_144644-ak8uq9uh/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kk3m58a6 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_145329-kk3m58a6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/kk3m58a6' target=\"_blank\">floral-sweep-6</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/kk3m58a6' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/kk3m58a6</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 2.6833, Acc: 24.82%\nVal Loss: 2.4851, Acc: 28.21%\nEpoch 2/20\nTrain Loss: 2.1716, Acc: 36.15%\nVal Loss: 2.1096, Acc: 36.65%\nEpoch 3/20\nTrain Loss: 1.8521, Acc: 43.91%\nVal Loss: 1.8682, Acc: 42.03%\nEpoch 4/20\nTrain Loss: 1.6465, Acc: 49.00%\nVal Loss: 1.7460, Acc: 44.78%\nEpoch 5/20\nTrain Loss: 1.5112, Acc: 52.33%\nVal Loss: 1.6029, Acc: 47.44%\nEpoch 6/20\nTrain Loss: 1.4144, Acc: 54.95%\nVal Loss: 1.5326, Acc: 50.31%\nEpoch 7/20\nTrain Loss: 1.3292, Acc: 57.53%\nVal Loss: 1.4553, Acc: 51.48%\nEpoch 8/20\nTrain Loss: 1.2563, Acc: 59.96%\nVal Loss: 1.3879, Acc: 53.88%\nEpoch 9/20\nTrain Loss: 1.2101, Acc: 61.21%\nVal Loss: 1.3446, Acc: 55.28%\nEpoch 10/20\nTrain Loss: 1.1497, Acc: 63.31%\nVal Loss: 1.3139, Acc: 56.07%\nEpoch 11/20\nTrain Loss: 1.1158, Acc: 64.27%\nVal Loss: 1.2713, Acc: 57.67%\nEpoch 12/20\nTrain Loss: 1.0862, Acc: 65.19%\nVal Loss: 1.2793, Acc: 58.15%\nEpoch 13/20\nTrain Loss: 1.0538, Acc: 66.20%\nVal Loss: 1.2270, Acc: 59.37%\nEpoch 14/20\nTrain Loss: 1.0205, Acc: 67.37%\nVal Loss: 1.1908, Acc: 60.55%\nEpoch 15/20\nTrain Loss: 1.0033, Acc: 67.82%\nVal Loss: 1.1602, Acc: 61.10%\nEpoch 16/20\nTrain Loss: 0.9773, Acc: 68.68%\nVal Loss: 1.1522, Acc: 61.39%\nEpoch 17/20\nTrain Loss: 0.9542, Acc: 69.57%\nVal Loss: 1.1304, Acc: 61.75%\nEpoch 18/20\nTrain Loss: 0.9455, Acc: 69.63%\nVal Loss: 1.1117, Acc: 62.30%\nEpoch 19/20\nTrain Loss: 0.9204, Acc: 70.59%\nVal Loss: 1.1439, Acc: 63.18%\nEpoch 20/20\nTrain Loss: 0.9133, Acc: 70.62%\nVal Loss: 1.0988, Acc: 63.25%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▃▄▅▅▆▆▆▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▄▄▅▅▆▆▆▇▇▇▇▇██████</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_acc</td><td>70.62467</td></tr><tr><td>train_loss</td><td>0.91335</td></tr><tr><td>val_acc</td><td>63.25338</td></tr><tr><td>val_loss</td><td>1.09876</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">floral-sweep-6</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/kk3m58a6' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/kk3m58a6</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_145329-kk3m58a6/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 474z9jpv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_145536-474z9jpv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/474z9jpv' target=\"_blank\">jolly-sweep-7</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/474z9jpv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/474z9jpv</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 2.2773, Acc: 34.01%\nVal Loss: 1.8953, Acc: 38.51%\nEpoch 2/20\nTrain Loss: 1.5351, Acc: 51.10%\nVal Loss: 1.5801, Acc: 48.04%\nEpoch 3/20\nTrain Loss: 1.2716, Acc: 59.28%\nVal Loss: 1.3953, Acc: 54.43%\nEpoch 4/20\nTrain Loss: 1.1060, Acc: 64.71%\nVal Loss: 1.2811, Acc: 58.22%\nEpoch 5/20\nTrain Loss: 1.0088, Acc: 67.62%\nVal Loss: 1.1764, Acc: 61.24%\nEpoch 6/20\nTrain Loss: 0.9210, Acc: 70.53%\nVal Loss: 1.1285, Acc: 62.94%\nEpoch 7/20\nTrain Loss: 0.8559, Acc: 72.60%\nVal Loss: 1.0966, Acc: 64.01%\nEpoch 8/20\nTrain Loss: 0.8151, Acc: 73.78%\nVal Loss: 1.0346, Acc: 65.69%\nEpoch 9/20\nTrain Loss: 0.7731, Acc: 75.14%\nVal Loss: 1.0130, Acc: 66.76%\nEpoch 10/20\nTrain Loss: 0.7338, Acc: 76.51%\nVal Loss: 0.9812, Acc: 67.63%\nEpoch 11/20\nTrain Loss: 0.7109, Acc: 77.15%\nVal Loss: 0.9620, Acc: 68.47%\nEpoch 12/20\nTrain Loss: 0.6836, Acc: 78.08%\nVal Loss: 0.9529, Acc: 68.59%\nEpoch 13/20\nTrain Loss: 0.6621, Acc: 78.71%\nVal Loss: 0.9454, Acc: 68.90%\nEpoch 14/20\nTrain Loss: 0.6456, Acc: 79.25%\nVal Loss: 0.9349, Acc: 69.65%\nEpoch 15/20\nTrain Loss: 0.6303, Acc: 79.69%\nVal Loss: 0.9146, Acc: 70.13%\nEpoch 16/20\nTrain Loss: 0.6196, Acc: 79.99%\nVal Loss: 0.9158, Acc: 70.11%\nEpoch 17/20\nTrain Loss: 0.5998, Acc: 80.71%\nVal Loss: 0.9043, Acc: 70.48%\nEpoch 18/20\nTrain Loss: 0.5882, Acc: 80.97%\nVal Loss: 0.9007, Acc: 70.88%\nEpoch 19/20\nTrain Loss: 0.5716, Acc: 81.58%\nVal Loss: 0.9059, Acc: 70.64%\nEpoch 20/20\nTrain Loss: 0.5633, Acc: 81.88%\nVal Loss: 0.9014, Acc: 70.90%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▃▅▅▆▆▇▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▄▅▆▆▇▇▇▇▇█████████</td></tr><tr><td>val_loss</td><td>█▆▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_acc</td><td>81.87763</td></tr><tr><td>train_loss</td><td>0.56334</td></tr><tr><td>val_acc</td><td>70.90073</td></tr><tr><td>val_loss</td><td>0.90142</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">jolly-sweep-7</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/474z9jpv' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/474z9jpv</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_145536-474z9jpv/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c1urnk2t with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_150151-c1urnk2t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/c1urnk2t' target=\"_blank\">twilight-sweep-8</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/c1urnk2t' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/c1urnk2t</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.2421, Acc: 60.84%\nVal Loss: 0.9920, Acc: 66.09%\nEpoch 2/15\nTrain Loss: 0.7083, Acc: 76.85%\nVal Loss: 0.8844, Acc: 71.10%\nEpoch 3/15\nTrain Loss: 0.6031, Acc: 80.23%\nVal Loss: 0.8312, Acc: 71.75%\nEpoch 4/15\nTrain Loss: 0.5552, Acc: 81.67%\nVal Loss: 0.8320, Acc: 72.84%\nEpoch 5/15\nTrain Loss: 0.5106, Acc: 83.11%\nVal Loss: 0.8236, Acc: 73.51%\nEpoch 6/15\nTrain Loss: 0.4871, Acc: 83.75%\nVal Loss: 0.8094, Acc: 73.92%\nEpoch 7/15\nTrain Loss: 0.4658, Acc: 84.25%\nVal Loss: 0.8225, Acc: 73.83%\nEpoch 8/15\nTrain Loss: 0.4492, Acc: 84.61%\nVal Loss: 0.8164, Acc: 73.77%\nEpoch 9/15\nTrain Loss: 0.4396, Acc: 84.90%\nVal Loss: 0.8154, Acc: 74.14%\nEpoch 10/15\nTrain Loss: 0.4275, Acc: 85.13%\nVal Loss: 0.8244, Acc: 74.05%\nEpoch 11/15\nTrain Loss: 0.4151, Acc: 85.49%\nVal Loss: 0.8501, Acc: 74.24%\nEpoch 12/15\nTrain Loss: 0.4024, Acc: 85.76%\nVal Loss: 0.8028, Acc: 74.53%\nEpoch 13/15\nTrain Loss: 0.3994, Acc: 85.86%\nVal Loss: 0.8249, Acc: 74.01%\nEpoch 14/15\nTrain Loss: 0.3877, Acc: 86.12%\nVal Loss: 0.8154, Acc: 74.56%\nEpoch 15/15\nTrain Loss: 0.3864, Acc: 86.11%\nVal Loss: 0.8308, Acc: 73.77%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▇▇▇▇▇██████▇</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▂▂▁▂▃▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>86.10721</td></tr><tr><td>train_loss</td><td>0.38636</td></tr><tr><td>val_acc</td><td>73.77101</td></tr><tr><td>val_loss</td><td>0.83076</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">twilight-sweep-8</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/c1urnk2t' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/c1urnk2t</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_150151-c1urnk2t/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n0ba6hlh with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_150716-n0ba6hlh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/n0ba6hlh' target=\"_blank\">upbeat-sweep-9</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/n0ba6hlh' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/n0ba6hlh</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.8939, Acc: 43.98%\nVal Loss: 1.1589, Acc: 61.76%\nEpoch 2/15\nTrain Loss: 0.9003, Acc: 70.89%\nVal Loss: 0.9160, Acc: 68.57%\nEpoch 3/15\nTrain Loss: 0.7167, Acc: 76.69%\nVal Loss: 0.8615, Acc: 71.31%\nEpoch 4/15\nTrain Loss: 0.6412, Acc: 79.02%\nVal Loss: 0.8244, Acc: 73.00%\nEpoch 5/15\nTrain Loss: 0.5855, Acc: 80.77%\nVal Loss: 0.8009, Acc: 73.55%\nEpoch 6/15\nTrain Loss: 0.5574, Acc: 81.52%\nVal Loss: 0.8118, Acc: 73.89%\nEpoch 7/15\nTrain Loss: 0.5282, Acc: 82.40%\nVal Loss: 0.7962, Acc: 74.36%\nEpoch 8/15\nTrain Loss: 0.5060, Acc: 83.07%\nVal Loss: 0.7950, Acc: 74.07%\nEpoch 9/15\nTrain Loss: 0.4856, Acc: 83.69%\nVal Loss: 0.7987, Acc: 74.62%\nEpoch 10/15\nTrain Loss: 0.4757, Acc: 83.93%\nVal Loss: 0.7627, Acc: 75.09%\nEpoch 11/15\nTrain Loss: 0.4615, Acc: 84.36%\nVal Loss: 0.7614, Acc: 74.96%\nEpoch 12/15\nTrain Loss: 0.4533, Acc: 84.48%\nVal Loss: 0.8043, Acc: 74.76%\nEpoch 13/15\nTrain Loss: 0.4375, Acc: 84.95%\nVal Loss: 0.7885, Acc: 74.61%\nEpoch 14/15\nTrain Loss: 0.4305, Acc: 85.12%\nVal Loss: 0.7797, Acc: 74.94%\nEpoch 15/15\nTrain Loss: 0.4250, Acc: 85.19%\nVal Loss: 0.7644, Acc: 75.09%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▇▇▇█▇███████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▂▂▂▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>85.19011</td></tr><tr><td>train_loss</td><td>0.42496</td></tr><tr><td>val_acc</td><td>75.09042</td></tr><tr><td>val_loss</td><td>0.76437</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">upbeat-sweep-9</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/n0ba6hlh' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/n0ba6hlh</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_150716-n0ba6hlh/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u3lluckt with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_151100-u3lluckt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/u3lluckt' target=\"_blank\">divine-sweep-10</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/u3lluckt' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/u3lluckt</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.9789, Acc: 41.10%\nVal Loss: 1.2306, Acc: 58.50%\nEpoch 2/15\nTrain Loss: 0.9919, Acc: 67.66%\nVal Loss: 0.9386, Acc: 68.30%\nEpoch 3/15\nTrain Loss: 0.8052, Acc: 73.55%\nVal Loss: 0.8596, Acc: 70.52%\nEpoch 4/15\nTrain Loss: 0.7135, Acc: 76.58%\nVal Loss: 0.8184, Acc: 72.35%\nEpoch 5/15\nTrain Loss: 0.6585, Acc: 78.24%\nVal Loss: 0.8232, Acc: 73.08%\nEpoch 6/15\nTrain Loss: 0.6162, Acc: 79.76%\nVal Loss: 0.7991, Acc: 73.41%\nEpoch 7/15\nTrain Loss: 0.5856, Acc: 80.73%\nVal Loss: 0.7853, Acc: 73.81%\nEpoch 8/15\nTrain Loss: 0.5660, Acc: 81.23%\nVal Loss: 0.8013, Acc: 74.04%\nEpoch 9/15\nTrain Loss: 0.5508, Acc: 81.66%\nVal Loss: 0.7773, Acc: 74.74%\nEpoch 10/15\nTrain Loss: 0.5332, Acc: 82.19%\nVal Loss: 0.7989, Acc: 74.57%\nEpoch 11/15\nTrain Loss: 0.5218, Acc: 82.58%\nVal Loss: 0.7869, Acc: 74.70%\nEpoch 12/15\nTrain Loss: 0.5189, Acc: 82.50%\nVal Loss: 0.7664, Acc: 74.47%\nEpoch 13/15\nTrain Loss: 0.5047, Acc: 82.99%\nVal Loss: 0.7696, Acc: 75.14%\nEpoch 14/15\nTrain Loss: 0.4958, Acc: 83.29%\nVal Loss: 0.7908, Acc: 74.91%\nEpoch 15/15\nTrain Loss: 0.4823, Acc: 83.74%\nVal Loss: 0.7720, Acc: 74.92%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>83.73804</td></tr><tr><td>train_loss</td><td>0.48231</td></tr><tr><td>val_acc</td><td>74.91971</td></tr><tr><td>val_loss</td><td>0.77202</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">divine-sweep-10</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/u3lluckt' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/u3lluckt</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_151100-u3lluckt/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yox3f4pr with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_151443-yox3f4pr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/yox3f4pr' target=\"_blank\">light-sweep-11</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/yox3f4pr' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/yox3f4pr</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.1898, Acc: 36.27%\nVal Loss: 2.0792, Acc: 41.18%\nEpoch 2/15\nTrain Loss: 1.8874, Acc: 44.36%\nVal Loss: 1.9383, Acc: 43.13%\nEpoch 3/15\nTrain Loss: 1.7658, Acc: 47.31%\nVal Loss: 1.7935, Acc: 45.61%\nEpoch 4/15\nTrain Loss: 1.6580, Acc: 50.30%\nVal Loss: 1.7302, Acc: 47.44%\nEpoch 5/15\nTrain Loss: 1.5867, Acc: 52.02%\nVal Loss: 1.6727, Acc: 48.92%\nEpoch 6/15\nTrain Loss: 1.5410, Acc: 53.21%\nVal Loss: 1.6465, Acc: 49.10%\nEpoch 7/15\nTrain Loss: 1.5025, Acc: 54.10%\nVal Loss: 1.6208, Acc: 49.74%\nEpoch 8/15\nTrain Loss: 1.4725, Acc: 54.99%\nVal Loss: 1.6094, Acc: 50.05%\nEpoch 9/15\nTrain Loss: 1.4404, Acc: 55.83%\nVal Loss: 1.5648, Acc: 51.22%\nEpoch 10/15\nTrain Loss: 1.4140, Acc: 56.50%\nVal Loss: 1.5393, Acc: 52.18%\nEpoch 11/15\nTrain Loss: 1.3953, Acc: 57.00%\nVal Loss: 1.5437, Acc: 52.46%\nEpoch 12/15\nTrain Loss: 1.3712, Acc: 57.69%\nVal Loss: 1.5086, Acc: 52.82%\nEpoch 13/15\nTrain Loss: 1.3512, Acc: 58.26%\nVal Loss: 1.5014, Acc: 52.79%\nEpoch 14/15\nTrain Loss: 1.3345, Acc: 58.80%\nVal Loss: 1.4902, Acc: 53.56%\nEpoch 15/15\nTrain Loss: 1.3180, Acc: 59.30%\nVal Loss: 1.4569, Acc: 54.55%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▆▆▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▃▄▅▅▅▆▆▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▃▃▂▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>59.29501</td></tr><tr><td>train_loss</td><td>1.31797</td></tr><tr><td>val_acc</td><td>54.54703</td></tr><tr><td>val_loss</td><td>1.45694</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">light-sweep-11</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/yox3f4pr' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/yox3f4pr</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_151443-yox3f4pr/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cspkb8s3 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_152001-cspkb8s3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/cspkb8s3' target=\"_blank\">skilled-sweep-12</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/cspkb8s3' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/cspkb8s3</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.9495, Acc: 42.36%\nVal Loss: 1.2082, Acc: 60.83%\nEpoch 2/15\nTrain Loss: 0.9175, Acc: 70.61%\nVal Loss: 0.9184, Acc: 69.41%\nEpoch 3/15\nTrain Loss: 0.7210, Acc: 76.76%\nVal Loss: 0.8686, Acc: 71.34%\nEpoch 4/15\nTrain Loss: 0.6349, Acc: 79.48%\nVal Loss: 0.8205, Acc: 72.65%\nEpoch 5/15\nTrain Loss: 0.5794, Acc: 81.16%\nVal Loss: 0.8144, Acc: 73.72%\nEpoch 6/15\nTrain Loss: 0.5507, Acc: 81.93%\nVal Loss: 0.7915, Acc: 74.03%\nEpoch 7/15\nTrain Loss: 0.5167, Acc: 83.00%\nVal Loss: 0.7979, Acc: 74.31%\nEpoch 8/15\nTrain Loss: 0.5009, Acc: 83.31%\nVal Loss: 0.8038, Acc: 74.81%\nEpoch 9/15\nTrain Loss: 0.4795, Acc: 84.01%\nVal Loss: 0.8039, Acc: 74.90%\nEpoch 10/15\nTrain Loss: 0.4692, Acc: 84.20%\nVal Loss: 0.7717, Acc: 75.29%\nEpoch 11/15\nTrain Loss: 0.4575, Acc: 84.46%\nVal Loss: 0.8012, Acc: 75.02%\nEpoch 12/15\nTrain Loss: 0.4391, Acc: 85.04%\nVal Loss: 0.8074, Acc: 74.86%\nEpoch 13/15\nTrain Loss: 0.4308, Acc: 85.19%\nVal Loss: 0.8024, Acc: 75.19%\nEpoch 14/15\nTrain Loss: 0.4281, Acc: 85.18%\nVal Loss: 0.8018, Acc: 75.15%\nEpoch 15/15\nTrain Loss: 0.4133, Acc: 85.64%\nVal Loss: 0.7881, Acc: 75.46%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▁▁▂▂▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>85.64176</td></tr><tr><td>train_loss</td><td>0.41331</td></tr><tr><td>val_acc</td><td>75.46078</td></tr><tr><td>val_loss</td><td>0.7881</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">skilled-sweep-12</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/cspkb8s3' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/cspkb8s3</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_152001-cspkb8s3/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ycmh85vr with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_152346-ycmh85vr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ycmh85vr' target=\"_blank\">lemon-sweep-13</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ycmh85vr' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ycmh85vr</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.4875, Acc: 54.08%\nVal Loss: 1.0604, Acc: 62.82%\nEpoch 2/15\nTrain Loss: 0.8074, Acc: 73.59%\nVal Loss: 0.9146, Acc: 69.20%\nEpoch 3/15\nTrain Loss: 0.6775, Acc: 77.77%\nVal Loss: 0.8877, Acc: 70.45%\nEpoch 4/15\nTrain Loss: 0.6076, Acc: 79.98%\nVal Loss: 0.8471, Acc: 71.66%\nEpoch 5/15\nTrain Loss: 0.5615, Acc: 81.47%\nVal Loss: 0.8351, Acc: 72.40%\nEpoch 6/15\nTrain Loss: 0.5266, Acc: 82.51%\nVal Loss: 0.8168, Acc: 73.41%\nEpoch 7/15\nTrain Loss: 0.4957, Acc: 83.51%\nVal Loss: 0.8146, Acc: 73.91%\nEpoch 8/15\nTrain Loss: 0.4860, Acc: 83.69%\nVal Loss: 0.8162, Acc: 73.74%\nEpoch 9/15\nTrain Loss: 0.4733, Acc: 83.95%\nVal Loss: 0.8253, Acc: 74.00%\nEpoch 10/15\nTrain Loss: 0.4531, Acc: 84.59%\nVal Loss: 0.7964, Acc: 73.95%\nEpoch 11/15\nTrain Loss: 0.4403, Acc: 85.01%\nVal Loss: 0.8265, Acc: 73.71%\nEpoch 12/15\nTrain Loss: 0.4328, Acc: 85.12%\nVal Loss: 0.8187, Acc: 74.16%\nEpoch 13/15\nTrain Loss: 0.4206, Acc: 85.39%\nVal Loss: 0.8213, Acc: 74.44%\nEpoch 14/15\nTrain Loss: 0.4153, Acc: 85.49%\nVal Loss: 0.8100, Acc: 74.00%\nEpoch 15/15\nTrain Loss: 0.4024, Acc: 85.85%\nVal Loss: 0.8283, Acc: 74.40%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▆▇▇█████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▂▂▁▂▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>85.84814</td></tr><tr><td>train_loss</td><td>0.40241</td></tr><tr><td>val_acc</td><td>74.40178</td></tr><tr><td>val_loss</td><td>0.82827</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lemon-sweep-13</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ycmh85vr' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ycmh85vr</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_152346-ycmh85vr/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yeey4ogj with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_152713-yeey4ogj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/yeey4ogj' target=\"_blank\">rare-sweep-14</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/yeey4ogj' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/yeey4ogj</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.4779, Acc: 28.59%\nVal Loss: 2.2635, Acc: 34.37%\nEpoch 2/15\nTrain Loss: 2.0328, Acc: 39.84%\nVal Loss: 1.9597, Acc: 41.49%\nEpoch 3/15\nTrain Loss: 1.8329, Acc: 45.07%\nVal Loss: 1.8043, Acc: 44.83%\nEpoch 4/15\nTrain Loss: 1.6957, Acc: 48.52%\nVal Loss: 1.6953, Acc: 47.42%\nEpoch 5/15\nTrain Loss: 1.5883, Acc: 51.15%\nVal Loss: 1.6138, Acc: 48.74%\nEpoch 6/15\nTrain Loss: 1.4976, Acc: 53.52%\nVal Loss: 1.4868, Acc: 52.18%\nEpoch 7/15\nTrain Loss: 1.4190, Acc: 55.69%\nVal Loss: 1.4376, Acc: 53.53%\nEpoch 8/15\nTrain Loss: 1.3523, Acc: 57.70%\nVal Loss: 1.3848, Acc: 55.01%\nEpoch 9/15\nTrain Loss: 1.2998, Acc: 59.14%\nVal Loss: 1.3185, Acc: 56.95%\nEpoch 10/15\nTrain Loss: 1.2415, Acc: 61.00%\nVal Loss: 1.3044, Acc: 57.49%\nEpoch 11/15\nTrain Loss: 1.2058, Acc: 61.92%\nVal Loss: 1.2462, Acc: 59.09%\nEpoch 12/15\nTrain Loss: 1.1639, Acc: 63.09%\nVal Loss: 1.2240, Acc: 59.90%\nEpoch 13/15\nTrain Loss: 1.1168, Acc: 64.67%\nVal Loss: 1.1982, Acc: 61.26%\nEpoch 14/15\nTrain Loss: 1.0893, Acc: 65.50%\nVal Loss: 1.1651, Acc: 62.01%\nEpoch 15/15\nTrain Loss: 1.0548, Acc: 66.56%\nVal Loss: 1.1636, Acc: 62.57%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▃▄▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▄▄▅▅▆▆▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>66.56421</td></tr><tr><td>train_loss</td><td>1.05484</td></tr><tr><td>val_acc</td><td>62.56763</td></tr><tr><td>val_loss</td><td>1.16361</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">rare-sweep-14</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/yeey4ogj' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/yeey4ogj</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_152713-yeey4ogj/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dsh73upq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_153011-dsh73upq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dsh73upq' target=\"_blank\">lyric-sweep-15</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dsh73upq' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dsh73upq</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.4359, Acc: 29.42%\nVal Loss: 2.2767, Acc: 34.49%\nEpoch 2/15\nTrain Loss: 2.0975, Acc: 38.72%\nVal Loss: 2.0787, Acc: 39.70%\nEpoch 3/15\nTrain Loss: 1.9221, Acc: 43.28%\nVal Loss: 1.8858, Acc: 43.93%\nEpoch 4/15\nTrain Loss: 1.7799, Acc: 46.72%\nVal Loss: 1.7879, Acc: 45.67%\nEpoch 5/15\nTrain Loss: 1.6906, Acc: 48.56%\nVal Loss: 1.6876, Acc: 47.50%\nEpoch 6/15\nTrain Loss: 1.6022, Acc: 50.70%\nVal Loss: 1.6231, Acc: 49.06%\nEpoch 7/15\nTrain Loss: 1.5171, Acc: 53.24%\nVal Loss: 1.5824, Acc: 50.01%\nEpoch 8/15\nTrain Loss: 1.4696, Acc: 54.33%\nVal Loss: 1.5066, Acc: 52.16%\nEpoch 9/15\nTrain Loss: 1.4006, Acc: 56.40%\nVal Loss: 1.4793, Acc: 53.58%\nEpoch 10/15\nTrain Loss: 1.3501, Acc: 57.94%\nVal Loss: 1.4142, Acc: 55.08%\nEpoch 11/15\nTrain Loss: 1.3097, Acc: 58.95%\nVal Loss: 1.3537, Acc: 56.68%\nEpoch 12/15\nTrain Loss: 1.2714, Acc: 60.05%\nVal Loss: 1.3411, Acc: 57.67%\nEpoch 13/15\nTrain Loss: 1.2300, Acc: 61.30%\nVal Loss: 1.3058, Acc: 58.10%\nEpoch 14/15\nTrain Loss: 1.1969, Acc: 62.28%\nVal Loss: 1.2900, Acc: 58.88%\nEpoch 15/15\nTrain Loss: 1.1649, Acc: 63.26%\nVal Loss: 1.2508, Acc: 59.68%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▃▄▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▄▄▅▅▅▆▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▅▅▄▄▃▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>63.25697</td></tr><tr><td>train_loss</td><td>1.16489</td></tr><tr><td>val_acc</td><td>59.67709</td></tr><tr><td>val_loss</td><td>1.2508</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lyric-sweep-15</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dsh73upq' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dsh73upq</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_153011-dsh73upq/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t3d7y2bh with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_153309-t3d7y2bh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/t3d7y2bh' target=\"_blank\">prime-sweep-16</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/t3d7y2bh' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/t3d7y2bh</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.2682, Acc: 35.28%\nVal Loss: 2.0886, Acc: 41.21%\nEpoch 2/15\nTrain Loss: 1.8362, Acc: 46.21%\nVal Loss: 1.7592, Acc: 47.63%\nEpoch 3/15\nTrain Loss: 1.5636, Acc: 52.58%\nVal Loss: 1.6207, Acc: 49.82%\nEpoch 4/15\nTrain Loss: 1.4038, Acc: 56.76%\nVal Loss: 1.4162, Acc: 55.28%\nEpoch 5/15\nTrain Loss: 1.2600, Acc: 61.05%\nVal Loss: 1.3002, Acc: 58.55%\nEpoch 6/15\nTrain Loss: 1.1544, Acc: 64.00%\nVal Loss: 1.2392, Acc: 59.92%\nEpoch 7/15\nTrain Loss: 1.0710, Acc: 66.37%\nVal Loss: 1.1272, Acc: 63.07%\nEpoch 8/15\nTrain Loss: 1.0126, Acc: 67.90%\nVal Loss: 1.0986, Acc: 64.59%\nEpoch 9/15\nTrain Loss: 0.9639, Acc: 69.29%\nVal Loss: 1.0859, Acc: 64.73%\nEpoch 10/15\nTrain Loss: 0.9125, Acc: 70.92%\nVal Loss: 1.0558, Acc: 65.45%\nEpoch 11/15\nTrain Loss: 0.8940, Acc: 71.35%\nVal Loss: 1.0086, Acc: 66.97%\nEpoch 12/15\nTrain Loss: 0.8599, Acc: 72.42%\nVal Loss: 1.0068, Acc: 66.37%\nEpoch 13/15\nTrain Loss: 0.8387, Acc: 73.03%\nVal Loss: 1.0050, Acc: 67.10%\nEpoch 14/15\nTrain Loss: 0.8202, Acc: 73.55%\nVal Loss: 1.0223, Acc: 67.01%\nEpoch 15/15\nTrain Loss: 0.7934, Acc: 74.51%\nVal Loss: 0.9720, Acc: 67.68%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▃▅▆▆▇▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>74.51434</td></tr><tr><td>train_loss</td><td>0.79342</td></tr><tr><td>val_acc</td><td>67.67744</td></tr><tr><td>val_loss</td><td>0.97203</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">prime-sweep-16</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/t3d7y2bh' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/t3d7y2bh</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_153309-t3d7y2bh/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t7d1otzw with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_153612-t7d1otzw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/t7d1otzw' target=\"_blank\">stellar-sweep-17</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/t7d1otzw' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/t7d1otzw</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.2499, Acc: 35.41%\nVal Loss: 2.0772, Acc: 40.73%\nEpoch 2/15\nTrain Loss: 1.7658, Acc: 47.55%\nVal Loss: 1.7414, Acc: 47.10%\nEpoch 3/15\nTrain Loss: 1.5230, Acc: 53.58%\nVal Loss: 1.5572, Acc: 51.54%\nEpoch 4/15\nTrain Loss: 1.3745, Acc: 57.71%\nVal Loss: 1.4855, Acc: 53.90%\nEpoch 5/15\nTrain Loss: 1.2518, Acc: 61.20%\nVal Loss: 1.3554, Acc: 56.93%\nEpoch 6/15\nTrain Loss: 1.1693, Acc: 63.33%\nVal Loss: 1.2624, Acc: 59.49%\nEpoch 7/15\nTrain Loss: 1.0902, Acc: 65.75%\nVal Loss: 1.2501, Acc: 60.09%\nEpoch 8/15\nTrain Loss: 1.0221, Acc: 67.88%\nVal Loss: 1.1752, Acc: 61.73%\nEpoch 9/15\nTrain Loss: 0.9713, Acc: 69.32%\nVal Loss: 1.1364, Acc: 63.13%\nEpoch 10/15\nTrain Loss: 0.9362, Acc: 70.18%\nVal Loss: 1.1187, Acc: 63.73%\nEpoch 11/15\nTrain Loss: 0.8912, Acc: 71.77%\nVal Loss: 1.1154, Acc: 64.02%\nEpoch 12/15\nTrain Loss: 0.8784, Acc: 71.87%\nVal Loss: 1.1177, Acc: 64.27%\nEpoch 13/15\nTrain Loss: 0.8613, Acc: 72.31%\nVal Loss: 1.0788, Acc: 65.21%\nEpoch 14/15\nTrain Loss: 0.8284, Acc: 73.43%\nVal Loss: 1.0688, Acc: 65.76%\nEpoch 15/15\nTrain Loss: 0.8098, Acc: 73.90%\nVal Loss: 1.0696, Acc: 65.15%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▆▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▄▅▆▆▆▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▆▄▄▃▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>73.90101</td></tr><tr><td>train_loss</td><td>0.80975</td></tr><tr><td>val_acc</td><td>65.14858</td></tr><tr><td>val_loss</td><td>1.06963</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">stellar-sweep-17</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/t7d1otzw' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/t7d1otzw</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_153612-t7d1otzw/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ndzqxg9t with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_153834-ndzqxg9t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ndzqxg9t' target=\"_blank\">zany-sweep-18</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ndzqxg9t' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ndzqxg9t</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.5748, Acc: 52.53%\nVal Loss: 0.9994, Acc: 66.27%\nEpoch 2/15\nTrain Loss: 0.7779, Acc: 74.87%\nVal Loss: 0.8744, Acc: 70.71%\nEpoch 3/15\nTrain Loss: 0.6548, Acc: 78.67%\nVal Loss: 0.8564, Acc: 72.56%\nEpoch 4/15\nTrain Loss: 0.5948, Acc: 80.51%\nVal Loss: 0.8241, Acc: 73.05%\nEpoch 5/15\nTrain Loss: 0.5535, Acc: 81.73%\nVal Loss: 0.7969, Acc: 73.72%\nEpoch 6/15\nTrain Loss: 0.5156, Acc: 82.92%\nVal Loss: 0.8008, Acc: 74.03%\nEpoch 7/15\nTrain Loss: 0.5019, Acc: 83.15%\nVal Loss: 0.7917, Acc: 74.47%\nEpoch 8/15\nTrain Loss: 0.4846, Acc: 83.70%\nVal Loss: 0.8085, Acc: 74.74%\nEpoch 9/15\nTrain Loss: 0.4669, Acc: 84.23%\nVal Loss: 0.7874, Acc: 74.38%\nEpoch 10/15\nTrain Loss: 0.4588, Acc: 84.34%\nVal Loss: 0.7936, Acc: 74.73%\nEpoch 11/15\nTrain Loss: 0.4516, Acc: 84.43%\nVal Loss: 0.7677, Acc: 75.24%\nEpoch 12/15\nTrain Loss: 0.4460, Acc: 84.57%\nVal Loss: 0.7663, Acc: 74.80%\nEpoch 13/15\nTrain Loss: 0.4296, Acc: 85.05%\nVal Loss: 0.8121, Acc: 75.10%\nEpoch 14/15\nTrain Loss: 0.4257, Acc: 85.14%\nVal Loss: 0.7634, Acc: 75.30%\nEpoch 15/15\nTrain Loss: 0.4182, Acc: 85.30%\nVal Loss: 0.7975, Acc: 75.18%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▆▇▇▇█▇██████</td></tr><tr><td>val_loss</td><td>█▄▄▃▂▂▂▂▂▂▁▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>85.29854</td></tr><tr><td>train_loss</td><td>0.41819</td></tr><tr><td>val_acc</td><td>75.18012</td></tr><tr><td>val_loss</td><td>0.79751</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">zany-sweep-18</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ndzqxg9t' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/ndzqxg9t</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_153834-ndzqxg9t/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n91f3nbw with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_154505-n91f3nbw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/n91f3nbw' target=\"_blank\">proud-sweep-19</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/n91f3nbw' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/n91f3nbw</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.0697, Acc: 38.83%\nVal Loss: 1.3035, Acc: 56.78%\nEpoch 2/15\nTrain Loss: 1.0324, Acc: 66.50%\nVal Loss: 0.9715, Acc: 67.27%\nEpoch 3/15\nTrain Loss: 0.8065, Acc: 73.86%\nVal Loss: 0.8763, Acc: 70.66%\nEpoch 4/15\nTrain Loss: 0.7194, Acc: 76.54%\nVal Loss: 0.8536, Acc: 71.96%\nEpoch 5/15\nTrain Loss: 0.6619, Acc: 78.35%\nVal Loss: 0.8173, Acc: 72.66%\nEpoch 6/15\nTrain Loss: 0.6304, Acc: 79.19%\nVal Loss: 0.8055, Acc: 73.59%\nEpoch 7/15\nTrain Loss: 0.5896, Acc: 80.63%\nVal Loss: 0.7730, Acc: 73.73%\nEpoch 8/15\nTrain Loss: 0.5714, Acc: 81.17%\nVal Loss: 0.7883, Acc: 74.25%\nEpoch 9/15\nTrain Loss: 0.5473, Acc: 81.93%\nVal Loss: 0.7717, Acc: 74.20%\nEpoch 10/15\nTrain Loss: 0.5459, Acc: 81.75%\nVal Loss: 0.7695, Acc: 74.62%\nEpoch 11/15\nTrain Loss: 0.5261, Acc: 82.39%\nVal Loss: 0.7846, Acc: 74.80%\nEpoch 12/15\nTrain Loss: 0.5136, Acc: 82.77%\nVal Loss: 0.7499, Acc: 75.06%\nEpoch 13/15\nTrain Loss: 0.5041, Acc: 83.03%\nVal Loss: 0.7636, Acc: 74.85%\nEpoch 14/15\nTrain Loss: 0.4955, Acc: 83.27%\nVal Loss: 0.7580, Acc: 74.99%\nEpoch 15/15\nTrain Loss: 0.4846, Acc: 83.65%\nVal Loss: 0.7586, Acc: 75.16%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>83.64947</td></tr><tr><td>train_loss</td><td>0.48463</td></tr><tr><td>val_acc</td><td>75.16276</td></tr><tr><td>val_loss</td><td>0.7586</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">proud-sweep-19</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/n91f3nbw' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/n91f3nbw</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_154505-n91f3nbw/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wiunnrml with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_154857-wiunnrml</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/wiunnrml' target=\"_blank\">fragrant-sweep-20</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/wiunnrml' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/wiunnrml</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.4075, Acc: 57.00%\nVal Loss: 0.9825, Acc: 66.98%\nEpoch 2/15\nTrain Loss: 0.7508, Acc: 75.43%\nVal Loss: 0.8848, Acc: 71.24%\nEpoch 3/15\nTrain Loss: 0.6288, Acc: 79.42%\nVal Loss: 0.8568, Acc: 72.12%\nEpoch 4/15\nTrain Loss: 0.5769, Acc: 80.99%\nVal Loss: 0.8112, Acc: 73.55%\nEpoch 5/15\nTrain Loss: 0.5293, Acc: 82.42%\nVal Loss: 0.8164, Acc: 74.09%\nEpoch 6/15\nTrain Loss: 0.5102, Acc: 82.94%\nVal Loss: 0.8174, Acc: 74.16%\nEpoch 7/15\nTrain Loss: 0.4872, Acc: 83.66%\nVal Loss: 0.7903, Acc: 74.25%\nEpoch 8/15\nTrain Loss: 0.4709, Acc: 84.06%\nVal Loss: 0.7975, Acc: 74.18%\nEpoch 9/15\nTrain Loss: 0.4517, Acc: 84.61%\nVal Loss: 0.7870, Acc: 74.74%\nEpoch 10/15\nTrain Loss: 0.4503, Acc: 84.45%\nVal Loss: 0.8142, Acc: 74.53%\nEpoch 11/15\nTrain Loss: 0.4344, Acc: 85.00%\nVal Loss: 0.7999, Acc: 74.61%\nEpoch 12/15\nTrain Loss: 0.4257, Acc: 85.15%\nVal Loss: 0.7921, Acc: 75.08%\nEpoch 13/15\nTrain Loss: 0.4199, Acc: 85.28%\nVal Loss: 0.7777, Acc: 75.06%\nEpoch 14/15\nTrain Loss: 0.4130, Acc: 85.40%\nVal Loss: 0.8005, Acc: 74.57%\nEpoch 15/15\nTrain Loss: 0.3999, Acc: 85.78%\nVal Loss: 0.8030, Acc: 74.99%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▅▇▇▇▇▇███████</td></tr><tr><td>val_loss</td><td>█▅▄▂▂▂▁▂▁▂▂▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>85.78054</td></tr><tr><td>train_loss</td><td>0.39988</td></tr><tr><td>val_acc</td><td>74.99204</td></tr><tr><td>val_loss</td><td>0.80303</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fragrant-sweep-20</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/wiunnrml' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/wiunnrml</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_154857-wiunnrml/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dq5dejol with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_155539-dq5dejol</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dq5dejol' target=\"_blank\">fanciful-sweep-21</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dq5dejol' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dq5dejol</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.2723, Acc: 33.76%\nVal Loss: 1.6611, Acc: 46.52%\nEpoch 2/15\nTrain Loss: 1.1243, Acc: 64.51%\nVal Loss: 0.9851, Acc: 66.76%\nEpoch 3/15\nTrain Loss: 0.8078, Acc: 74.15%\nVal Loss: 0.8910, Acc: 69.73%\nEpoch 4/15\nTrain Loss: 0.7063, Acc: 77.24%\nVal Loss: 0.8634, Acc: 71.87%\nEpoch 5/15\nTrain Loss: 0.6480, Acc: 78.97%\nVal Loss: 0.8635, Acc: 72.42%\nEpoch 6/15\nTrain Loss: 0.6060, Acc: 80.26%\nVal Loss: 0.8092, Acc: 73.38%\nEpoch 7/15\nTrain Loss: 0.5741, Acc: 81.26%\nVal Loss: 0.8110, Acc: 73.33%\nEpoch 8/15\nTrain Loss: 0.5546, Acc: 81.69%\nVal Loss: 0.7988, Acc: 73.51%\nEpoch 9/15\nTrain Loss: 0.5321, Acc: 82.50%\nVal Loss: 0.8005, Acc: 73.80%\nEpoch 10/15\nTrain Loss: 0.5160, Acc: 82.83%\nVal Loss: 0.8004, Acc: 73.99%\nEpoch 11/15\nTrain Loss: 0.5000, Acc: 83.30%\nVal Loss: 0.7949, Acc: 74.60%\nEpoch 12/15\nTrain Loss: 0.4797, Acc: 83.99%\nVal Loss: 0.7820, Acc: 74.35%\nEpoch 13/15\nTrain Loss: 0.4780, Acc: 83.89%\nVal Loss: 0.7930, Acc: 74.81%\nEpoch 14/15\nTrain Loss: 0.4650, Acc: 84.30%\nVal Loss: 0.8157, Acc: 74.50%\nEpoch 15/15\nTrain Loss: 0.4605, Acc: 84.39%\nVal Loss: 0.7754, Acc: 74.78%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▄▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▇▇▇██████████</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>84.39192</td></tr><tr><td>train_loss</td><td>0.46049</td></tr><tr><td>val_acc</td><td>74.78082</td></tr><tr><td>val_loss</td><td>0.77535</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fanciful-sweep-21</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dq5dejol' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/dq5dejol</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_155539-dq5dejol/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bq87zo6n with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_155918-bq87zo6n</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/bq87zo6n' target=\"_blank\">confused-sweep-22</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/bq87zo6n' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/bq87zo6n</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.8840, Acc: 42.05%\nVal Loss: 1.2589, Acc: 57.22%\nEpoch 2/15\nTrain Loss: 0.9581, Acc: 68.99%\nVal Loss: 0.9719, Acc: 67.92%\nEpoch 3/15\nTrain Loss: 0.7767, Acc: 74.65%\nVal Loss: 0.8680, Acc: 70.16%\nEpoch 4/15\nTrain Loss: 0.6934, Acc: 77.27%\nVal Loss: 0.8606, Acc: 71.62%\nEpoch 5/15\nTrain Loss: 0.6335, Acc: 79.23%\nVal Loss: 0.8462, Acc: 72.15%\nEpoch 6/15\nTrain Loss: 0.5985, Acc: 80.32%\nVal Loss: 0.8459, Acc: 72.48%\nEpoch 7/15\nTrain Loss: 0.5783, Acc: 80.78%\nVal Loss: 0.8358, Acc: 73.12%\nEpoch 8/15\nTrain Loss: 0.5341, Acc: 82.42%\nVal Loss: 0.8179, Acc: 73.58%\nEpoch 9/15\nTrain Loss: 0.5213, Acc: 82.73%\nVal Loss: 0.8088, Acc: 73.93%\nEpoch 10/15\nTrain Loss: 0.5090, Acc: 82.92%\nVal Loss: 0.7994, Acc: 74.42%\nEpoch 11/15\nTrain Loss: 0.4976, Acc: 83.31%\nVal Loss: 0.8001, Acc: 74.23%\nEpoch 12/15\nTrain Loss: 0.4858, Acc: 83.69%\nVal Loss: 0.7979, Acc: 74.61%\nEpoch 13/15\nTrain Loss: 0.4718, Acc: 84.07%\nVal Loss: 0.7996, Acc: 74.22%\nEpoch 14/15\nTrain Loss: 0.4697, Acc: 84.05%\nVal Loss: 0.7948, Acc: 74.95%\nEpoch 15/15\nTrain Loss: 0.4638, Acc: 84.25%\nVal Loss: 0.8014, Acc: 74.23%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▇▇▇▇▇███████</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>84.25177</td></tr><tr><td>train_loss</td><td>0.46381</td></tr><tr><td>val_acc</td><td>74.22818</td></tr><tr><td>val_loss</td><td>0.80138</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">confused-sweep-22</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/bq87zo6n' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/bq87zo6n</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_155918-bq87zo6n/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 20c74d9l with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_160125-20c74d9l</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/20c74d9l' target=\"_blank\">efficient-sweep-23</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/20c74d9l' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/20c74d9l</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.6042, Acc: 49.49%\nVal Loss: 1.1109, Acc: 63.48%\nEpoch 2/15\nTrain Loss: 0.9266, Acc: 69.35%\nVal Loss: 0.9430, Acc: 69.15%\nEpoch 3/15\nTrain Loss: 0.7767, Acc: 74.51%\nVal Loss: 0.8828, Acc: 71.03%\nEpoch 4/15\nTrain Loss: 0.7042, Acc: 76.92%\nVal Loss: 0.8755, Acc: 71.69%\nEpoch 5/15\nTrain Loss: 0.6747, Acc: 77.82%\nVal Loss: 0.8389, Acc: 72.47%\nEpoch 6/15\nTrain Loss: 0.6330, Acc: 79.22%\nVal Loss: 0.8136, Acc: 73.18%\nEpoch 7/15\nTrain Loss: 0.6083, Acc: 79.97%\nVal Loss: 0.8472, Acc: 73.56%\nEpoch 8/15\nTrain Loss: 0.5917, Acc: 80.47%\nVal Loss: 0.8117, Acc: 74.25%\nEpoch 9/15\nTrain Loss: 0.5757, Acc: 80.97%\nVal Loss: 0.8260, Acc: 74.11%\nEpoch 10/15\nTrain Loss: 0.5689, Acc: 81.19%\nVal Loss: 0.8286, Acc: 74.31%\nEpoch 11/15\nTrain Loss: 0.5596, Acc: 81.36%\nVal Loss: 0.8347, Acc: 73.96%\nEpoch 12/15\nTrain Loss: 0.5463, Acc: 81.84%\nVal Loss: 0.7965, Acc: 74.44%\nEpoch 13/15\nTrain Loss: 0.5382, Acc: 82.09%\nVal Loss: 0.8068, Acc: 74.00%\nEpoch 14/15\nTrain Loss: 0.5279, Acc: 82.34%\nVal Loss: 0.8228, Acc: 74.44%\nEpoch 15/15\nTrain Loss: 0.5304, Acc: 82.21%\nVal Loss: 0.8148, Acc: 74.60%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▆▇▇▇████████</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▁▂▁▂▂▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>82.20705</td></tr><tr><td>train_loss</td><td>0.5304</td></tr><tr><td>val_acc</td><td>74.59564</td></tr><tr><td>val_loss</td><td>0.81478</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">efficient-sweep-23</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/20c74d9l' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/20c74d9l</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_160125-20c74d9l/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zp3gbqsj with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_160453-zp3gbqsj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/zp3gbqsj' target=\"_blank\">elated-sweep-24</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/zp3gbqsj' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/zp3gbqsj</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 2.0434, Acc: 39.54%\nVal Loss: 1.3052, Acc: 56.15%\nEpoch 2/15\nTrain Loss: 1.0172, Acc: 66.97%\nVal Loss: 1.0009, Acc: 66.29%\nEpoch 3/15\nTrain Loss: 0.8097, Acc: 73.46%\nVal Loss: 0.9110, Acc: 68.83%\nEpoch 4/15\nTrain Loss: 0.6948, Acc: 77.32%\nVal Loss: 0.8604, Acc: 71.12%\nEpoch 5/15\nTrain Loss: 0.6502, Acc: 78.61%\nVal Loss: 0.8340, Acc: 72.05%\nEpoch 6/15\nTrain Loss: 0.6044, Acc: 80.07%\nVal Loss: 0.8069, Acc: 72.46%\nEpoch 7/15\nTrain Loss: 0.5601, Acc: 81.53%\nVal Loss: 0.8260, Acc: 73.23%\nEpoch 8/15\nTrain Loss: 0.5446, Acc: 81.86%\nVal Loss: 0.7847, Acc: 73.90%\nEpoch 9/15\nTrain Loss: 0.5268, Acc: 82.45%\nVal Loss: 0.7909, Acc: 73.78%\nEpoch 10/15\nTrain Loss: 0.5058, Acc: 83.10%\nVal Loss: 0.7896, Acc: 74.02%\nEpoch 11/15\nTrain Loss: 0.4900, Acc: 83.55%\nVal Loss: 0.7946, Acc: 73.81%\nEpoch 12/15\nTrain Loss: 0.4710, Acc: 84.08%\nVal Loss: 0.7975, Acc: 74.31%\nEpoch 13/15\nTrain Loss: 0.4646, Acc: 84.18%\nVal Loss: 0.8092, Acc: 73.77%\nEpoch 14/15\nTrain Loss: 0.4536, Acc: 84.57%\nVal Loss: 0.8075, Acc: 74.36%\nEpoch 15/15\nTrain Loss: 0.4405, Acc: 84.87%\nVal Loss: 0.7891, Acc: 74.95%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>84.86813</td></tr><tr><td>train_loss</td><td>0.44054</td></tr><tr><td>val_acc</td><td>74.95443</td></tr><tr><td>val_loss</td><td>0.78912</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">elated-sweep-24</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/zp3gbqsj' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/zp3gbqsj</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_160453-zp3gbqsj/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4n3kcgza with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_160851-4n3kcgza</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/4n3kcgza' target=\"_blank\">hopeful-sweep-25</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/4n3kcgza' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/4n3kcgza</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.4188, Acc: 55.46%\nVal Loss: 1.0102, Acc: 65.15%\nEpoch 2/15\nTrain Loss: 0.8032, Acc: 73.79%\nVal Loss: 0.8963, Acc: 70.61%\nEpoch 3/15\nTrain Loss: 0.6785, Acc: 77.84%\nVal Loss: 0.8566, Acc: 71.85%\nEpoch 4/15\nTrain Loss: 0.6161, Acc: 79.84%\nVal Loss: 0.8493, Acc: 72.79%\nEpoch 5/15\nTrain Loss: 0.5730, Acc: 81.26%\nVal Loss: 0.8139, Acc: 73.80%\nEpoch 6/15\nTrain Loss: 0.5433, Acc: 82.16%\nVal Loss: 0.8070, Acc: 74.17%\nEpoch 7/15\nTrain Loss: 0.5213, Acc: 82.79%\nVal Loss: 0.7850, Acc: 74.55%\nEpoch 8/15\nTrain Loss: 0.5079, Acc: 83.05%\nVal Loss: 0.8056, Acc: 74.43%\nEpoch 9/15\nTrain Loss: 0.4917, Acc: 83.53%\nVal Loss: 0.7877, Acc: 74.95%\nEpoch 10/15\nTrain Loss: 0.4772, Acc: 83.91%\nVal Loss: 0.7868, Acc: 74.96%\nEpoch 11/15\nTrain Loss: 0.4645, Acc: 84.26%\nVal Loss: 0.7927, Acc: 74.79%\nEpoch 12/15\nTrain Loss: 0.4558, Acc: 84.49%\nVal Loss: 0.8180, Acc: 74.81%\nEpoch 13/15\nTrain Loss: 0.4469, Acc: 84.70%\nVal Loss: 0.7861, Acc: 75.18%\nEpoch 14/15\nTrain Loss: 0.4359, Acc: 85.00%\nVal Loss: 0.7863, Acc: 75.37%\nEpoch 15/15\nTrain Loss: 0.4297, Acc: 85.17%\nVal Loss: 0.8259, Acc: 74.94%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▆▇▇▇▇███████</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▂▁▂▁▁▁▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>85.16693</td></tr><tr><td>train_loss</td><td>0.42968</td></tr><tr><td>val_acc</td><td>74.94285</td></tr><tr><td>val_loss</td><td>0.82594</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">hopeful-sweep-25</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/4n3kcgza' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/4n3kcgza</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_160851-4n3kcgza/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3z4tr0a9 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_161457-3z4tr0a9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/3z4tr0a9' target=\"_blank\">frosty-sweep-26</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/3z4tr0a9' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/3z4tr0a9</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.8365, Acc: 42.66%\nVal Loss: 1.2305, Acc: 59.14%\nEpoch 2/15\nTrain Loss: 1.0175, Acc: 66.46%\nVal Loss: 0.9715, Acc: 67.22%\nEpoch 3/15\nTrain Loss: 0.8351, Acc: 72.66%\nVal Loss: 0.9157, Acc: 69.43%\nEpoch 4/15\nTrain Loss: 0.7555, Acc: 75.28%\nVal Loss: 0.8797, Acc: 71.08%\nEpoch 5/15\nTrain Loss: 0.7061, Acc: 76.91%\nVal Loss: 0.8834, Acc: 71.80%\nEpoch 6/15\nTrain Loss: 0.6676, Acc: 78.18%\nVal Loss: 0.8530, Acc: 71.61%\nEpoch 7/15\nTrain Loss: 0.6433, Acc: 78.99%\nVal Loss: 0.8187, Acc: 72.71%\nEpoch 8/15\nTrain Loss: 0.6262, Acc: 79.53%\nVal Loss: 0.8503, Acc: 72.69%\nEpoch 9/15\nTrain Loss: 0.6034, Acc: 80.33%\nVal Loss: 0.8206, Acc: 73.31%\nEpoch 10/15\nTrain Loss: 0.5906, Acc: 80.66%\nVal Loss: 0.8420, Acc: 73.24%\nEpoch 11/15\nTrain Loss: 0.5805, Acc: 81.02%\nVal Loss: 0.8375, Acc: 73.66%\nEpoch 12/15\nTrain Loss: 0.5621, Acc: 81.64%\nVal Loss: 0.8375, Acc: 73.56%\nEpoch 13/15\nTrain Loss: 0.5625, Acc: 81.58%\nVal Loss: 0.8530, Acc: 73.62%\nEpoch 14/15\nTrain Loss: 0.5560, Acc: 81.62%\nVal Loss: 0.8196, Acc: 73.96%\nEpoch 15/15\nTrain Loss: 0.5467, Acc: 81.92%\nVal Loss: 0.8213, Acc: 74.05%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▇▇▇▇▇███████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▂▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>81.92094</td></tr><tr><td>train_loss</td><td>0.54673</td></tr><tr><td>val_acc</td><td>74.05457</td></tr><tr><td>val_loss</td><td>0.82128</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">frosty-sweep-26</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/3z4tr0a9' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/3z4tr0a9</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_161457-3z4tr0a9/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 16kiufuf with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_161820-16kiufuf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/16kiufuf' target=\"_blank\">unique-sweep-27</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/16kiufuf' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/16kiufuf</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.8954, Acc: 43.58%\nVal Loss: 1.1600, Acc: 61.84%\nEpoch 2/15\nTrain Loss: 0.8947, Acc: 71.12%\nVal Loss: 0.9200, Acc: 68.91%\nEpoch 3/15\nTrain Loss: 0.7209, Acc: 76.52%\nVal Loss: 0.8705, Acc: 71.29%\nEpoch 4/15\nTrain Loss: 0.6402, Acc: 79.06%\nVal Loss: 0.8347, Acc: 72.30%\nEpoch 5/15\nTrain Loss: 0.5912, Acc: 80.64%\nVal Loss: 0.8118, Acc: 73.12%\nEpoch 6/15\nTrain Loss: 0.5526, Acc: 81.83%\nVal Loss: 0.8015, Acc: 73.71%\nEpoch 7/15\nTrain Loss: 0.5269, Acc: 82.56%\nVal Loss: 0.7727, Acc: 74.23%\nEpoch 8/15\nTrain Loss: 0.5038, Acc: 83.22%\nVal Loss: 0.7811, Acc: 74.63%\nEpoch 9/15\nTrain Loss: 0.4896, Acc: 83.55%\nVal Loss: 0.8056, Acc: 74.51%\nEpoch 10/15\nTrain Loss: 0.4752, Acc: 83.93%\nVal Loss: 0.7956, Acc: 74.51%\nEpoch 11/15\nTrain Loss: 0.4635, Acc: 84.25%\nVal Loss: 0.7699, Acc: 74.82%\nEpoch 12/15\nTrain Loss: 0.4494, Acc: 84.60%\nVal Loss: 0.7782, Acc: 74.72%\nEpoch 13/15\nTrain Loss: 0.4445, Acc: 84.76%\nVal Loss: 0.7772, Acc: 75.35%\nEpoch 14/15\nTrain Loss: 0.4325, Acc: 85.08%\nVal Loss: 0.7924, Acc: 74.92%\nEpoch 15/15\nTrain Loss: 0.4289, Acc: 85.08%\nVal Loss: 0.7732, Acc: 75.25%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▆▇▇▇████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>85.07589</td></tr><tr><td>train_loss</td><td>0.42885</td></tr><tr><td>val_acc</td><td>75.24667</td></tr><tr><td>val_loss</td><td>0.7732</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">unique-sweep-27</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/16kiufuf' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/16kiufuf</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_161820-16kiufuf/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sm12c672 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_162204-sm12c672</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/sm12c672' target=\"_blank\">pretty-sweep-28</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/sm12c672' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/sm12c672</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 1.6876, Acc: 47.55%\nVal Loss: 1.0979, Acc: 62.50%\nEpoch 2/20\nTrain Loss: 0.8810, Acc: 71.40%\nVal Loss: 0.9687, Acc: 68.77%\nEpoch 3/20\nTrain Loss: 0.7360, Acc: 76.11%\nVal Loss: 0.9171, Acc: 70.65%\nEpoch 4/20\nTrain Loss: 0.6704, Acc: 78.17%\nVal Loss: 0.8979, Acc: 71.54%\nEpoch 5/20\nTrain Loss: 0.6235, Acc: 79.65%\nVal Loss: 0.8618, Acc: 72.51%\nEpoch 6/20\nTrain Loss: 0.5931, Acc: 80.61%\nVal Loss: 0.8318, Acc: 73.12%\nEpoch 7/20\nTrain Loss: 0.5717, Acc: 81.16%\nVal Loss: 0.8411, Acc: 73.42%\nEpoch 8/20\nTrain Loss: 0.5481, Acc: 81.93%\nVal Loss: 0.8301, Acc: 73.59%\nEpoch 9/20\nTrain Loss: 0.5387, Acc: 82.10%\nVal Loss: 0.8412, Acc: 73.18%\nEpoch 10/20\nTrain Loss: 0.5175, Acc: 82.80%\nVal Loss: 0.8556, Acc: 73.80%\nEpoch 11/20\nTrain Loss: 0.5038, Acc: 83.23%\nVal Loss: 0.8368, Acc: 74.17%\nEpoch 12/20\nTrain Loss: 0.5102, Acc: 82.92%\nVal Loss: 0.7940, Acc: 74.34%\nEpoch 13/20\nTrain Loss: 0.4984, Acc: 83.26%\nVal Loss: 0.8230, Acc: 74.26%\nEpoch 14/20\nTrain Loss: 0.4841, Acc: 83.71%\nVal Loss: 0.8401, Acc: 74.07%\nEpoch 15/20\nTrain Loss: 0.4808, Acc: 83.74%\nVal Loss: 0.8632, Acc: 73.96%\nEpoch 16/20\nTrain Loss: 0.4795, Acc: 83.72%\nVal Loss: 0.8068, Acc: 74.73%\nEpoch 17/20\nTrain Loss: 0.4745, Acc: 83.81%\nVal Loss: 0.8290, Acc: 74.25%\nEpoch 18/20\nTrain Loss: 0.4568, Acc: 84.44%\nVal Loss: 0.8334, Acc: 74.25%\nEpoch 19/20\nTrain Loss: 0.4548, Acc: 84.46%\nVal Loss: 0.8153, Acc: 74.29%\nEpoch 20/20\nTrain Loss: 0.4533, Acc: 84.53%\nVal Loss: 0.7942, Acc: 74.41%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▆▆▇▇▇▇█████████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▆▇▇▇▇▇▇██████████</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▁▂▂▃▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_acc</td><td>84.53319</td></tr><tr><td>train_loss</td><td>0.45333</td></tr><tr><td>val_acc</td><td>74.41046</td></tr><tr><td>val_loss</td><td>0.79422</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">pretty-sweep-28</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/sm12c672' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/sm12c672</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_162204-sm12c672/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a6fsw9dg with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_162632-a6fsw9dg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/a6fsw9dg' target=\"_blank\">avid-sweep-29</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/a6fsw9dg' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/a6fsw9dg</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.2661, Acc: 59.65%\nVal Loss: 1.0002, Acc: 66.62%\nEpoch 2/15\nTrain Loss: 0.7886, Acc: 74.16%\nVal Loss: 0.8904, Acc: 70.81%\nEpoch 3/15\nTrain Loss: 0.6842, Acc: 77.68%\nVal Loss: 0.8809, Acc: 71.32%\nEpoch 4/15\nTrain Loss: 0.6292, Acc: 79.39%\nVal Loss: 0.8741, Acc: 72.36%\nEpoch 5/15\nTrain Loss: 0.5966, Acc: 80.38%\nVal Loss: 0.8798, Acc: 73.05%\nEpoch 6/15\nTrain Loss: 0.5733, Acc: 81.13%\nVal Loss: 0.8426, Acc: 72.72%\nEpoch 7/15\nTrain Loss: 0.5643, Acc: 81.36%\nVal Loss: 0.8669, Acc: 73.15%\nEpoch 8/15\nTrain Loss: 0.5520, Acc: 81.72%\nVal Loss: 0.8347, Acc: 73.32%\nEpoch 9/15\nTrain Loss: 0.5412, Acc: 82.00%\nVal Loss: 0.8197, Acc: 73.34%\nEpoch 10/15\nTrain Loss: 0.5219, Acc: 82.68%\nVal Loss: 0.8537, Acc: 73.57%\nEpoch 11/15\nTrain Loss: 0.5224, Acc: 82.59%\nVal Loss: 0.8548, Acc: 73.11%\nEpoch 12/15\nTrain Loss: 0.5123, Acc: 82.92%\nVal Loss: 0.8518, Acc: 73.37%\nEpoch 13/15\nTrain Loss: 0.5099, Acc: 82.93%\nVal Loss: 0.8633, Acc: 73.76%\nEpoch 14/15\nTrain Loss: 0.4981, Acc: 83.23%\nVal Loss: 0.8565, Acc: 73.78%\nEpoch 15/15\nTrain Loss: 0.5026, Acc: 83.12%\nVal Loss: 0.8191, Acc: 74.09%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▅▆▇▇▇▇▇█▇▇███</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▂▃▂▁▂▂▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>83.12333</td></tr><tr><td>train_loss</td><td>0.5026</td></tr><tr><td>val_acc</td><td>74.08929</td></tr><tr><td>val_loss</td><td>0.81907</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">avid-sweep-29</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/a6fsw9dg' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/a6fsw9dg</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_162632-a6fsw9dg/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tg4tt48f with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_163152-tg4tt48f</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/tg4tt48f' target=\"_blank\">kind-sweep-30</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/tg4tt48f' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/tg4tt48f</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.4077, Acc: 54.87%\nVal Loss: 1.0090, Acc: 66.14%\nEpoch 2/15\nTrain Loss: 0.8640, Acc: 71.54%\nVal Loss: 0.9221, Acc: 69.50%\nEpoch 3/15\nTrain Loss: 0.7513, Acc: 75.29%\nVal Loss: 0.8711, Acc: 71.03%\nEpoch 4/15\nTrain Loss: 0.6924, Acc: 77.31%\nVal Loss: 0.8669, Acc: 71.88%\nEpoch 5/15\nTrain Loss: 0.6581, Acc: 78.29%\nVal Loss: 0.8526, Acc: 72.66%\nEpoch 6/15\nTrain Loss: 0.6410, Acc: 78.81%\nVal Loss: 0.8505, Acc: 73.46%\nEpoch 7/15\nTrain Loss: 0.6177, Acc: 79.61%\nVal Loss: 0.8647, Acc: 73.26%\nEpoch 8/15\nTrain Loss: 0.6004, Acc: 80.14%\nVal Loss: 0.8810, Acc: 73.43%\nEpoch 9/15\nTrain Loss: 0.5909, Acc: 80.56%\nVal Loss: 0.8552, Acc: 73.15%\nEpoch 10/15\nTrain Loss: 0.5896, Acc: 80.41%\nVal Loss: 0.8119, Acc: 73.61%\nEpoch 11/15\nTrain Loss: 0.5795, Acc: 80.69%\nVal Loss: 0.8255, Acc: 73.75%\nEpoch 12/15\nTrain Loss: 0.5674, Acc: 81.18%\nVal Loss: 0.8329, Acc: 73.74%\nEpoch 13/15\nTrain Loss: 0.5686, Acc: 81.12%\nVal Loss: 0.8025, Acc: 74.07%\nEpoch 14/15\nTrain Loss: 0.5660, Acc: 81.03%\nVal Loss: 0.8296, Acc: 74.36%\nEpoch 15/15\nTrain Loss: 0.5495, Acc: 81.72%\nVal Loss: 0.8199, Acc: 74.38%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▃▃▄▃▁▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>81.72257</td></tr><tr><td>train_loss</td><td>0.54951</td></tr><tr><td>val_acc</td><td>74.37863</td></tr><tr><td>val_loss</td><td>0.81987</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">kind-sweep-30</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/tg4tt48f' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/tg4tt48f</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_163152-tg4tt48f/logs</code>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# =======================\n# Imports and Sweep Config\n# =======================\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport wandb\nimport os\nimport math\nfrom tqdm import tqdm\n\nos.makedirs(\"predictions_vanilla\", exist_ok=True)\n\nbest_config = {\n    'embedding_dim': 128,\n    'hidden_dim': 256,\n    'enc_layers': 2,\n    'dec_layers': 3,\n    'cell_type': 'LSTM',\n    'dropout': 0.5,\n    'epochs': 15,\n    'beam_size': 3,\n    'batch_size': 64,  # Added\n    'learning_rate': 0.0005  # Added\n}\n\n# =======================\n# Vocabulary\n# =======================\nclass Vocab:\n    def __init__(self):\n        self.char2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n        self.idx2char = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\"}\n        self.size = 3\n\n    def build(self, texts):\n        for text in texts:\n            for char in text:\n                if char not in self.char2idx:\n                    self.char2idx[char] = self.size\n                    self.idx2char[self.size] = char\n                    self.size += 1\n\n    def encode(self, text):\n        return [self.char2idx[c] for c in text]\n\n    def decode(self, idxs):\n        return ''.join([self.idx2char[i] for i in idxs if i > 2])\n\n# =======================\n# Dataset\n# =======================\nclass TransliterationDataset(Dataset):\n    def __init__(self, filepath, inp_vocab, out_vocab, is_test=False):\n        self.pairs = []\n        with open(filepath, encoding='utf-8') as f:\n            for line in f:\n                fields = line.strip().split('\\t')\n                if len(fields) < 2:\n                    continue\n                lat, dev = fields[0], fields[1]\n                self.pairs.append((lat, dev))\n        if not is_test:\n            inp_vocab.build([p[0] for p in self.pairs])\n            out_vocab.build([p[1] for p in self.pairs])\n        self.inp_vocab = inp_vocab\n        self.out_vocab = out_vocab\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        lat, dev = self.pairs[idx]\n        x = self.inp_vocab.encode(lat)\n        y = [self.out_vocab.char2idx[\"<sos>\"]] + self.out_vocab.encode(dev) + [self.out_vocab.char2idx[\"<eos>\"]]\n        return torch.tensor(x), torch.tensor(y), lat, dev\n\ndef collate_fn(batch):\n    x_batch, y_batch, lat, dev = zip(*batch)\n    x_lens = [len(x) for x in x_batch]\n    y_lens = [len(y) for y in y_batch]\n    x_pad = nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=0)\n    y_pad = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=0)\n    return x_pad, y_pad, torch.tensor(x_lens), torch.tensor(y_lens), lat, dev\n\n# =======================\n# Encoder, Decoder, Seq2Seq\n# =======================\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n        super().__init__()\n        self.cell_type = cell_type\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        rnn_class = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM, \"RNN\": nn.RNN}[cell_type]\n        self.rnn = rnn_class(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.out = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, input_token, hidden):\n        embedded = self.embedding(input_token).unsqueeze(1)\n        output, hidden = self.rnn(embedded, hidden)\n        output = self.out(output.squeeze(1))\n        return output, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, enc_layers, dec_layers, cell_type, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.cell_type = cell_type\n        self.enc_layers = enc_layers\n        self.dec_layers = dec_layers\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size, trg_len = trg.size()\n        vocab_size = self.decoder.out.out_features\n        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n        enc_hidden = self.encoder(src[0], src[1])\n        if self.cell_type == \"LSTM\":\n            h, c = enc_hidden\n            h = self._match_layers(h)\n            c = self._match_layers(c)\n            dec_hidden = (h, c)\n        else:\n            dec_hidden = self._match_layers(enc_hidden)\n        input_token = trg[:, 0]\n        for t in range(1, trg_len):\n            output, dec_hidden = self.decoder(input_token, dec_hidden)\n            outputs[:, t] = output\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n        return outputs\n\n    def _match_layers(self, hidden):\n        if self.enc_layers == self.dec_layers:\n            return hidden\n        elif self.enc_layers > self.dec_layers:\n            return hidden[:self.dec_layers]\n        else:\n            pad = hidden.new_zeros((self.dec_layers - self.enc_layers, *hidden.shape[1:]))\n            return torch.cat([hidden, pad], dim=0)\n\n    def predict(self, src_tensor, src_len, max_len=30):\n        self.eval()\n        with torch.no_grad():\n            enc_hidden = self.encoder(src_tensor.unsqueeze(0), torch.tensor([src_len]))\n            if self.cell_type == \"LSTM\":\n                h, c = enc_hidden\n                h = self._match_layers(h)\n                c = self._match_layers(c)\n                dec_hidden = (h, c)\n            else:\n                dec_hidden = self._match_layers(enc_hidden)\n            input_token = torch.tensor([2]).to(self.device)  # <sos>\n            output_seq = []\n            for _ in range(max_len):\n                output, dec_hidden = self.decoder(input_token, dec_hidden)\n                top1 = output.argmax(1)\n                if top1.item() == 2: break  # <eos>\n                output_seq.append(top1.item())\n                input_token = top1\n        return output_seq\n\n# =======================\n# Train & Eval\n# =======================\ndef train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss, total_correct, total_count = 0, 0, 0\n    for src, trg, src_lens, _, _, _ in loader:\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model((src, src_lens), trg)\n        loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))\n        pred = output.argmax(2)\n        correct = ((pred[:, 1:] == trg[:, 1:]) & (trg[:, 1:] != 0)).sum().item()\n        total_correct += correct\n        total_count += (trg[:, 1:] != 0).sum().item()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader), 100.0 * total_correct / total_count\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss, total_correct, total_count = 0, 0, 0\n    with torch.no_grad():\n        for src, trg, src_lens, _, _, _ in loader:\n            src, trg = src.to(device), trg.to(device)\n            output = model((src, src_lens), trg, teacher_forcing_ratio=0)\n            loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))\n            pred = output.argmax(2)\n            correct = ((pred[:, 1:] == trg[:, 1:]) & (trg[:, 1:] != 0)).sum().item()\n            total_correct += correct\n            total_count += (trg[:, 1:] != 0).sum().item()\n            total_loss += loss.item()\n    return total_loss / len(loader), 100.0 * total_correct / total_count\n\n# =======================\n# Main\n# =======================\ndef main():\n    wandb.init(project=\"dakshina-translit-test\")\n    config = best_config\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    inp_vocab, out_vocab = Vocab(), Vocab()\n    train_data = TransliterationDataset(\"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\", inp_vocab, out_vocab)\n    dev_data = TransliterationDataset(\"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\", inp_vocab, out_vocab)\n    test_data = TransliterationDataset(\"/kaggle/input/dataset-01/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\", inp_vocab, out_vocab, is_test=True)\n\n    # Update DataLoader batch sizes\n    train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_fn)\n     # First create model\n    encoder = Encoder(inp_vocab.size, config['embedding_dim'], config['hidden_dim'],\n                      config['enc_layers'], config['cell_type'], config['dropout'])\n    decoder = Decoder(out_vocab.size, config['embedding_dim'], config['hidden_dim'],\n                      config['dec_layers'], config['cell_type'], config['dropout'])\n    model = Seq2Seq(encoder, decoder, config['enc_layers'], config['dec_layers'], config['cell_type'], device).to(device)\n    # Update optimizer with learning rate\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    best_val_acc = 0.0\n    for epoch in range(config['epochs']):\n        print(f\"Epoch {epoch+1}/{config['epochs']}\")\n        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n\n        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n        print(f\"Val   Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc\n        })\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(\"Best model saved.\")\n\n    \n    # Evaluation on test set\n    print(\"\\n Evaluating on test set with best model:\")\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    model.eval()\n    total_correct, total_count = 0, 0\n\n    with open(\"predictions_vanilla/test_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n        for src, _, src_lens, _, lat, gold in test_loader:\n            src = src.to(device)\n            pred_ids = model.predict(src[0], src_lens[0].item())\n            pred = out_vocab.decode(pred_ids)\n            f.write(f\"{lat[0]}\\t{gold[0]}\\t{pred}\\n\")\n            if pred == gold[0]:\n                total_correct += 1\n            total_count += 1\n\n    test_acc = 100.0 * total_correct / total_count\n    print(f\"📊 Test Accuracy: {test_acc:.2f}%\")\n    wandb.log({\"test_acc\": test_acc})\n\n\n     # After test evaluation, add random samples display\n    print(\"\\nRandom Test Samples Predictions:\")\n    import random\n    random_indices = random.sample(range(len(test_data)), 30)\n    \n    with open(\"predictions_vanilla/test_predictions.txt\", \"a\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\\nRandom Sample Predictions:\\n\")\n        for idx in random_indices:\n            x, y, lat, dev = test_data[idx]\n            src_tensor = x.to(device)\n            pred_ids = model.predict(src_tensor, len(x))\n            pred = out_vocab.decode(pred_ids)\n            \n            print(f\"Input: {lat}\")\n            print(f\"True: {dev}\")\n            print(f\"Pred: {pred}\\n\")\n            \n            f.write(f\"Input: {lat}\\n\")\n            f.write(f\"True: {dev}\\n\")\n            f.write(f\"Pred: {pred}\\n\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T17:17:32.504544Z","iopub.execute_input":"2025-05-19T17:17:32.505290Z","iopub.status.idle":"2025-05-19T17:21:33.160309Z","shell.execute_reply.started":"2025-05-19T17:17:32.505265Z","shell.execute_reply":"2025-05-19T17:21:33.159681Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-translit' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">kind-sweep-30</strong> at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/tg4tt48f' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/tg4tt48f</a><br> View project at: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_171410-tg4tt48f/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_171732-tg4tt48f</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/tg4tt48f' target=\"_blank\">kind-sweep-30</a></strong> to <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/sweeps/2yk21c98</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/tg4tt48f' target=\"_blank\">https://wandb.ai/manglesh_dl_ass3/dakshina-translit/runs/tg4tt48f</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 1.7807, Accuracy: 47.10%\nVal   Loss: 1.0987, Accuracy: 62.92%\nBest model saved.\nEpoch 2/15\nTrain Loss: 0.8313, Accuracy: 73.17%\nVal   Loss: 0.8883, Accuracy: 69.75%\nBest model saved.\nEpoch 3/15\nTrain Loss: 0.6634, Accuracy: 78.55%\nVal   Loss: 0.8675, Accuracy: 71.87%\nBest model saved.\nEpoch 4/15\nTrain Loss: 0.5937, Accuracy: 80.57%\nVal   Loss: 0.8368, Accuracy: 72.91%\nBest model saved.\nEpoch 5/15\nTrain Loss: 0.5427, Accuracy: 82.14%\nVal   Loss: 0.8239, Accuracy: 73.44%\nBest model saved.\nEpoch 6/15\nTrain Loss: 0.5105, Accuracy: 83.07%\nVal   Loss: 0.8186, Accuracy: 73.39%\nEpoch 7/15\nTrain Loss: 0.4891, Accuracy: 83.70%\nVal   Loss: 0.7836, Accuracy: 74.55%\nBest model saved.\nEpoch 8/15\nTrain Loss: 0.4709, Accuracy: 84.08%\nVal   Loss: 0.7878, Accuracy: 74.21%\nEpoch 9/15\nTrain Loss: 0.4516, Accuracy: 84.61%\nVal   Loss: 0.7778, Accuracy: 74.79%\nBest model saved.\nEpoch 10/15\nTrain Loss: 0.4297, Accuracy: 85.32%\nVal   Loss: 0.8025, Accuracy: 74.56%\nEpoch 11/15\nTrain Loss: 0.4135, Accuracy: 85.74%\nVal   Loss: 0.8168, Accuracy: 74.68%\nEpoch 12/15\nTrain Loss: 0.4114, Accuracy: 85.62%\nVal   Loss: 0.8018, Accuracy: 75.00%\nBest model saved.\nEpoch 13/15\nTrain Loss: 0.4060, Accuracy: 85.71%\nVal   Loss: 0.7642, Accuracy: 74.93%\nEpoch 14/15\nTrain Loss: 0.3965, Accuracy: 85.88%\nVal   Loss: 0.8109, Accuracy: 75.12%\nBest model saved.\nEpoch 15/15\nTrain Loss: 0.3922, Accuracy: 85.89%\nVal   Loss: 0.7969, Accuracy: 75.05%\n\n Evaluating on test set with best model:\n📊 Test Accuracy: 37.45%\n\nRandom Test Samples Predictions:\nInput: इकाईयों\nTrue: ekaeyon\nPred: ikaiyon\n\nInput: फेफड़ा\nTrue: fefada\nPred: fefada\n\nInput: ढूंढ\nTrue: dhundh\nPred: dhundh\n\nInput: भूलों\nTrue: bhulon\nPred: bhulon\n\nInput: अधिवास\nTrue: adhivas\nPred: adhivas\n\nInput: वर्साचे\nTrue: varsaache\nPred: varsache\n\nInput: लंबी\nTrue: lambi\nPred: lambi\n\nInput: उपलक्ष्य\nTrue: uplakshay\nPred: upalkshy\n\nInput: दिखला\nTrue: dikhlaa\nPred: dikhla\n\nInput: ग्रंथियों\nTrue: granthiyon\nPred: granthiyon\n\nInput: मैरेज\nTrue: merraige\nPred: marrage\n\nInput: अर्थ\nTrue: arth\nPred: arth\n\nInput: स्वा\nTrue: swa\nPred: swa\n\nInput: वेस्टिंग\nTrue: wasting\nPred: westing\n\nInput: सिनेप्स\nTrue: synapse\nPred: cinpes\n\nInput: पहला\nTrue: pehla\nPred: pahla\n\nInput: अंकोर\nTrue: ankor\nPred: ankor\n\nInput: फिजिशियन\nTrue: physician\nPred: phisision\n\nInput: वंशवाद\nTrue: vanshvad\nPred: vanshwaad\n\nInput: बिखेरती\nTrue: bikherti\nPred: bikherati\n\n","output_type":"stream"}],"execution_count":10}]}